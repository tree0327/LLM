{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain ì‹œì‘í•˜ê¸° - Model I/O (ëª¨ë¸ ì…ì¶œë ¥)\n",
    "\n",
    "LangChainì˜ ê°€ì¥ ê¸°ì´ˆì ì¸ êµ¬ì„± ìš”ì†Œì¸ **Model I/O**ë¥¼ ì‹¤ìŠµí•©ë‹ˆë‹¤.\n",
    "LLMì„ í˜¸ì¶œí•˜ê³ , ì…ë ¥ì„ í…œí”Œë¦¿í™”í•˜ë©°, ì¶œë ¥ì„ ì›í•˜ëŠ” í˜•íƒœë¡œ íŒŒì‹±í•˜ëŠ” ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ ìµí™ë‹ˆë‹¤.\n",
    "\n",
    "**í•™ìŠµ ëª©í‘œ:**\n",
    "1. **Model (ëª¨ë¸):** OpenAI, HuggingFace ë“± ë‹¤ì–‘í•œ LLMì„ LangChainìœ¼ë¡œ í˜¸ì¶œí•˜ëŠ” ë°©ë²• ìµíˆê¸°\n",
    "2. **Prompt (í”„ë¡¬í”„íŠ¸):** `PromptTemplate`ì„ ì‚¬ìš©í•˜ì—¬ ë™ì ì´ê³  ì¬ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë¡¬í”„íŠ¸ ë§Œë“¤ê¸°\n",
    "3. **Output Parser (ì¶œë ¥ íŒŒì„œ):** LLMì˜ í…ìŠ¤íŠ¸ ì‘ë‹µì„ ë¦¬ìŠ¤íŠ¸, JSON, Pydantic ê°ì²´ ë“± êµ¬ì¡°í™”ëœ ë°ì´í„°ë¡œ ë³€í™˜í•˜ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. í™˜ê²½ ì„¤ì • (Environment Setup)\n",
    "\n",
    "ì‹¤ìŠµì— í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•˜ê³ , API í‚¤ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "- `langchain-openai`: OpenAI ëª¨ë¸ ì—°ë™\n",
    "- `python-dotenv`: í™˜ê²½ë³€ìˆ˜(.env) ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain langchain-community langchain-openai langchain-huggingface python-dotenv -qqq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. LangSmith ì„¤ì • (ì„ íƒ ì‚¬í•­)\n",
    "**LangSmith**ëŠ” LangChain ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ì‹¤í–‰ ê³¼ì •ì„ ì¶”ì (Tracing)í•˜ê³  ë””ë²„ê¹…í•  ìˆ˜ ìˆëŠ” ë„êµ¬ì…ë‹ˆë‹¤.\n",
    "ë³µì¡í•œ ì²´ì¸(Chain)ì´ ì–´ë–»ê²Œ ë™ì‘í•˜ëŠ”ì§€ ì‹œê°ì ìœ¼ë¡œ í™•ì¸í•  ìˆ˜ ìˆì–´ ë§¤ìš° ìœ ìš©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv  # .env íŒŒì¼ì—ì„œ í™˜ê²½ë³€ìˆ˜ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "import os                       # ìš´ì˜ì²´ì œ í™˜ê²½ë³€ìˆ˜ ì œì–´ìš©\n",
    "\n",
    "# 1. .env íŒŒì¼ ë¡œë“œ (API Key ë³´ì•ˆ ê´€ë¦¬)\n",
    "load_dotenv()\n",
    "\n",
    "# 2. OpenAI API Key ì„¤ì •\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"openai_key\")\n",
    "\n",
    "# 3. LangSmith Tracing ì„¤ì • (ì‹¤í–‰ ë¡œê·¸ ì¶”ì )\n",
    "os.environ[\"LANGSMITH_TRACING\"] = 'true'\n",
    "os.environ[\"LANGSMITH_ENDPOINT\"] = 'https://api.smith.langchain.com'\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = 'skn23-langchain'         # í”„ë¡œì íŠ¸ëª… (ë³¸ì¸ í”„ë¡œì íŠ¸ë¡œ ë³€ê²½ ê°€ëŠ¥)\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = os.getenv(\"langsmith_key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "### 3. Model (ëª¨ë¸)\n",
    "\n",
    "LangChainì—ì„œëŠ” ë‹¤ì–‘í•œ LLM(OpenAI, Anthropic, HuggingFace ë“±)ì„ **ê³µí†µëœ ì¸í„°í˜ì´ìŠ¤**ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "ê°€ì¥ ë§ì´ ì‚¬ìš©ë˜ëŠ” `ChatOpenAI`ë¥¼ ì‹¤ìŠµí•´ ë´…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI  # OpenAI ì±„íŒ… ëª¨ë¸ í´ë˜ìŠ¤\n",
    "\n",
    "# 1. ëª¨ë¸ ê°ì²´ ìƒì„± (gpt-4o-mini ëª¨ë¸ ì‚¬ìš©)\n",
    "# temperature=0 : ë§¤ë²ˆ ê°™ì€ ë‹µë³€ì„ í•˜ë„ë¡ ì„¤ì • (ì¼ê´€ì„±)\n",
    "# temperature=1 : ì°½ì˜ì ì´ê³  ë‹¤ì–‘í•œ ë‹µë³€ ìœ ë„\n",
    "llm = ChatOpenAI(model='gpt-4o-mini', temperature=0.7)\n",
    "\n",
    "# 2. invoke() ë©”ì„œë“œë¡œ LLM í˜¸ì¶œ\n",
    "# invoke ë©”ì„œë“œëŠ” ì§ˆë¬¸(ë¬¸ìì—´)ì„ ì…ë ¥ë°›ì•„ AI ì‘ë‹µ(AIMessage)ì„ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "response = llm.invoke('í”„ë‘ìŠ¤ì˜ ì—¬í–‰ì§€ ì¶”ì²œí•´ì¤˜.')\n",
    "\n",
    "# 3. ê²°ê³¼ í™•ì¸ (content ì†ì„±ì— ì‹¤ì œ í…ìŠ¤íŠ¸ê°€ ë“¤ì–´ìˆìŠµë‹ˆë‹¤)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ğŸ“˜ ëª¨ë¸ ì¶”ìƒí™” (`init_chat_model`)\n",
    "íŠ¹ì • í´ë˜ìŠ¤(`ChatOpenAI`)ì— ì˜ì¡´í•˜ì§€ ì•Šê³ , ëª¨ë¸ ì´ë¦„ë§Œìœ¼ë¡œ ê°ì²´ë¥¼ ìƒì„±í•  ìˆ˜ ìˆëŠ” íŒ©í† ë¦¬ í•¨ìˆ˜ì…ë‹ˆë‹¤.\n",
    "ë‚˜ì¤‘ì— ëª¨ë¸ì„ GPTì—ì„œ Claudeë‚˜ Llamaë¡œ ë°”ê¿€ ë•Œ ì½”ë“œ ìˆ˜ì •ì´ ìµœì†Œí™”ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# 'provider:model_name' í˜•ì‹ìœ¼ë¡œ ì§€ì •í•˜ì—¬ ëª¨ë¸ ìƒì„±\n",
    "llm_gpt = init_chat_model('openai:gpt-4o-mini', temperature=1)\n",
    "\n",
    "result = llm_gpt.invoke('í•€ë€ë“œì˜ ì—¬í–‰ì§€ ì¶”ì²œí•´ì¤˜.')\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "### 4. Prompt Template (í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿)\n",
    "\n",
    "LLMì—ê²Œ ë³´ë‚¼ ì§ˆë¬¸ì„ ë§¤ë²ˆ ì²˜ìŒë¶€í„° ì‘ì„±í•˜ëŠ” ê²ƒì€ ë¹„íš¨ìœ¨ì ì…ë‹ˆë‹¤.\n",
    "**PromptTemplate**ì„ ì‚¬ìš©í•˜ë©´ êµ¬ë© ëš«ë¦° í‹€(Template)ì„ ë§Œë“¤ì–´ ë‘ê³ , ìƒí™©ì— ë”°ë¼ ë‚´ìš©ë§Œ ë°”ê¿”ë¼ìš¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "- `PromptTemplate`: ë¬¸ìì—´ ì™„ì„±í˜• í…œí”Œë¦¿\n",
    "- `ChatPromptTemplate`: ëŒ€í™”í˜•(System, User) ë©”ì‹œì§€ í…œí”Œë¦¿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# 1. í…œí”Œë¦¿ ì •ì˜ ({product} ë¶€ë¶„ì´ ë³€ìˆ˜ì…ë‹ˆë‹¤)\n",
    "template_str = '{product}ë¥¼ í™ë³´í•˜ê¸° ìœ„í•œ ì°¸ì‹ í•œ ë¬¸êµ¬ë¥¼ ì‘ì„±í•´ ì¤˜.'\n",
    "prompt = PromptTemplate.from_template(template_str)\n",
    "\n",
    "# 2. format() ë©”ì„œë“œë¡œ ë³€ìˆ˜ ì±„ìš°ê¸°\n",
    "filled_prompt_1 = prompt.format(product='ì „ê¸°ì°¨')\n",
    "filled_prompt_2 = prompt.format(product='ì¹´ë©”ë¼')\n",
    "\n",
    "print(f\"ì™„ì„±ëœ í”„ë¡¬í”„íŠ¸ 1: {filled_prompt_1}\")\n",
    "print(f\"ì™„ì„±ëœ í”„ë¡¬í”„íŠ¸ 2: {filled_prompt_2}\")\n",
    "\n",
    "# 3. LLMì— ì „ë‹¬í•˜ê¸°\n",
    "# invoke()ì— ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë³€ìˆ˜ë¥¼ ë°”ë¡œ ì „ë‹¬í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.\n",
    "# (ë‚´ë¶€ì ìœ¼ë¡œ formatì´ ì‹¤í–‰ë˜ê³  ëª¨ë¸ë¡œ ì „ë‹¬ë©ë‹ˆë‹¤ - Chain ì‚¬ìš© ì‹œ)\n",
    "ai_msg = llm.invoke(filled_prompt_1)\n",
    "print(\"\\n[AI ì‘ë‹µ]:\\n\", ai_msg.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "### 5. Output Parser (ì¶œë ¥ íŒŒì„œ)\n",
    "\n",
    "LLMì€ ê¸°ë³¸ì ìœ¼ë¡œ ì¥ë¬¸ì˜ ì¤„ê¸€(String)ì„ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "í•˜ì§€ë§Œ ê°œë°œ ê³¼ì •ì—ì„œëŠ” **ë¦¬ìŠ¤íŠ¸(List)**ë‚˜ **JSON** ê°™ì€ êµ¬ì¡°í™”ëœ ë°ì´í„°ê°€ í•„ìš”í•  ë•Œê°€ ë§ìŠµë‹ˆë‹¤.\n",
    "\n",
    "**ì£¼ìš” íŒŒì„œ:**\n",
    "- `CommaSeparatedListOutputParser`: ì½¤ë§ˆ(,)ë¡œ êµ¬ë¶„ëœ í…ìŠ¤íŠ¸ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜\n",
    "- `JsonOutputParser`: JSON í˜•íƒœì˜ í…ìŠ¤íŠ¸ë¥¼ íŒŒì´ì¬ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜\n",
    "- `PydanticOutputParser`: Pydantic ìŠ¤í‚¤ë§ˆì— ë§ì¶° ì—„ê²©í•˜ê²Œ ê²€ì¦ëœ ê°ì²´ë¡œ ë³€í™˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "# 1. íŒŒì„œ ê°ì²´ ìƒì„±\n",
    "list_parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "# 2. ì§€ì‹œì‚¬í•­(Instruction) í™•ì¸\n",
    "# íŒŒì„œê°€ LLMì—ê²Œ \"ì´ë ‡ê²Œ ë‹µí•´ë¼\"ë¼ê³  ì§€ì‹œí•  ë¬¸êµ¬ë¥¼ ìë™ìœ¼ë¡œ ë§Œë“¤ì–´ì¤ë‹ˆë‹¤.\n",
    "format_instruction = list_parser.get_format_instructions()\n",
    "print(\"[íŒŒì„œ ì§€ì‹œì‚¬í•­]:\", format_instruction)\n",
    "\n",
    "# 3. í”„ë¡¬í”„íŠ¸ì— ì§€ì‹œì‚¬í•­ í¬í•¨ì‹œí‚¤ê¸°\n",
    "prompt = PromptTemplate(\n",
    "    template='{subject} íŒ€ {n}ê°œë¥¼ ë‚˜ì—´í•´ì£¼ì„¸ìš”.\\n{format_instruction}',\n",
    "    input_variables=['subject', 'n'],\n",
    "    partial_variables={'format_instruction': format_instruction} # í•­ìƒ í¬í•¨ë  ë³€ìˆ˜\n",
    ")\n",
    "\n",
    "# 4. ì²´ì¸ ì‹¤í–‰ (Prompt -> LLM -> Parser)\n",
    "# | (Pipe) ì—°ì‚°ìë¡œ íë¦„ì„ ì—°ê²°í•©ë‹ˆë‹¤ (LCEL)\n",
    "chain = prompt | llm | list_parser\n",
    "\n",
    "result = chain.invoke({'subject': 'í•œêµ­ í”„ë¡œì•¼êµ¬', 'n': 5})\n",
    "\n",
    "print(f\"\\n[ê²°ê³¼ íƒ€ì…]: {type(result)}\")\n",
    "print(f\"[ê²°ê³¼ ê°’]: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ğŸ“˜ JSON íŒŒì„œ ì‹¤ìŠµ\n",
    "ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•˜ê±°ë‚˜ APIë¡œ ì „ë‹¬í•˜ê¸° ìœ„í•´ JSON í˜•ì‹ì´ í•„ìš”í•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# 1. ê²°ê³¼ê°’ì˜ êµ¬ì¡°(Schema)ë¥¼ Pydanticìœ¼ë¡œ ì •ì˜ (ê¶Œì¥ ì‚¬í•­)\n",
    "class Book(BaseModel):\n",
    "    title: str = Field(description=\"ì±… ì œëª©\")\n",
    "    author: str = Field(description=\"ì €ì\")\n",
    "    year: int = Field(description=\"ì¶œíŒ ì—°ë„\")\n",
    "\n",
    "# 2. íŒŒì„œ ìƒì„±\n",
    "json_parser = JsonOutputParser(pydantic_object=Book)\n",
    "\n",
    "# 3. í”„ë¡¬í”„íŠ¸ êµ¬ì„±\n",
    "prompt = PromptTemplate(\n",
    "    template=\"'{subject}' ì£¼ì œì˜ ë² ìŠ¤íŠ¸ì…€ëŸ¬ ì±… í•œ ê¶Œì„ ì¶”ì²œí•´ì¤˜.\\n{format_instruction}\",\n",
    "    input_variables=[\"subject\"],\n",
    "    partial_variables={\"format_instruction\": json_parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "# 4. ì²´ì¸ ì‹¤í–‰\n",
    "chain = prompt | llm | json_parser\n",
    "\n",
    "result = chain.invoke({\"subject\": \"ì¸ê³µì§€ëŠ¥\"})\n",
    "\n",
    "print(f\"[ê²°ê³¼ íƒ€ì…]: {type(result)}\")  # dict íƒ€ì…ì´ì–´ì•¼ í•¨\n",
    "print(json.dumps(result, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "### 6. ì •ë¦¬ (Summary)\n",
    "\n",
    "- **Model**: `ChatOpenAI` ë“±ì„ í†µí•´ LLMì„ í˜¸ì¶œí•©ë‹ˆë‹¤.\n",
    "- **PromptTemplate**: ì…ë ¥ê°’ì„ ë™ì ìœ¼ë¡œ ì±„ìš¸ ìˆ˜ ìˆëŠ” ì¬ì‚¬ìš© ê°€ëŠ¥í•œ í‹€ì…ë‹ˆë‹¤.\n",
    "- **OutputParser**: LLMì˜ ì¤„ê¸€ ì¶œë ¥ì„ ë¦¬ìŠ¤íŠ¸ë‚˜ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜í•˜ì—¬ í”„ë¡œê·¸ë¨ì—ì„œ ì“°ê¸° ì¢‹ê²Œ ë§Œë“­ë‹ˆë‹¤.\n",
    "- **LCEL (`|`)**: `Prompt | Model | Parser` í˜•íƒœë¡œ ìš°ì•„í•˜ê²Œ íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í•©ë‹ˆë‹¤."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}