{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG의 핵심 - Retrieval (정보 검색)\n",
    "\n",
    "LLM이 학습하지 않은 외부 데이터를 참고하여 답변하도록 만드는 **RAG(Retrieval-Augmented Generation)**의 핵심, **Retrieval** 과정을 실습합니다.\n",
    "\n",
    "**학습 목표:**\n",
    "1. **Document Loader:** 웹페이지, PDF, 텍스트 파일 등 외부 데이터를 LangChain Document 형식으로 불러오기\n",
    "2. **Text Splitter:** 긴 문서를 LLM이 처리하기 좋은 크기의 청크(Chunk)로 나누기\n",
    "3. **Embedding & Vector Store:** 텍스트를 의미 기반 벡터로 변환하고 검색 가능한 저장소에 저장하기\n",
    "4. **Retriever:** 사용자의 질문과 관련된 최적의 문서를 검색하여 LLM에 전달하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 환경 설정 (Environment Setup)\n",
    "\n",
    "필요한 패키지를 설치하고 환경 변수를 로드합니다.\n",
    "- `tiktoken`: 텍스트 분할 시 토큰 수를 계산하기 위해 필요\n",
    "- `faiss-cpu`: 벡터 저장소(Vector Store)로 사용\n",
    "- `pypdf`: PDF 파일 로딩용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain langchain-community langchain-openai langchain-huggingface wikipedia pypdf tavily-python tiktoken faiss-cpu sentence-transformers -Uqqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# 1. API Key 로드\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"openai_key\")\n",
    "os.environ[\"TAVILY_API_KEY\"] = os.getenv(\"tavily_key\")\n",
    "\n",
    "# 2. LangSmith Tracing 설정\n",
    "os.environ[\"LANGSMITH_TRACING\"] = 'true'\n",
    "os.environ[\"LANGSMITH_ENDPOINT\"] = 'https://api.smith.langchain.com'\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = 'skn23-langchain'\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = os.getenv(\"langsmith_key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "### 2. Document (문서 객체)\n",
    "\n",
    "LangChain에서 모든 텍스트 데이터는 `Document`라는 표준 객체로 변환되어 처리됩니다.\n",
    "두 가지 핵심 속성을 가집니다:\n",
    "1. `page_content`: 실제 텍스트 내용\n",
    "2. `metadata`: 출처(source), 제목, 페이지 번호 등 부가 정보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "# 임의의 텍스트로 Document 객체 생성 예시\n",
    "doc = Document(\n",
    "    page_content='LangChain은 LLM 애플리케이션 개발을 위한 프레임워크입니다.',\n",
    "    metadata={\n",
    "        'source': 'manual.txt',  # 출처\n",
    "        'page': 1,               # 페이지 번호\n",
    "        'author': 'gimdabin'     # 작성자 등 다양한 정보 추가 가능\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"--- 본문 ---\")\n",
    "print(doc.page_content)\n",
    "print(\"\\n--- 메타데이터 ---\")\n",
    "print(doc.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "### 3. Document Loader (문서 로더)\n",
    "\n",
    "외부의 다양한 데이터 소스(Web, PDF, CSV 등)를 `Document` 객체 리스트로 불러옵니다.\n",
    "\n",
    "#### 3-1. WebBaseLoader\n",
    "특정 URL의 웹페이지 내용을 크롤링하여 가져옵니다. 네이버 뉴스 등의 본문을 가져올 때 유용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "url = \"https://n.news.naver.com/mnews/article/005/0001830299\"\n",
    "\n",
    "# 일부 웹사이트는 봇 접근을 차단하므로, 브라우저처럼 보이게 헤더(User-Agent)를 설정합니다.\n",
    "header = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}\n",
    "\n",
    "loader = WebBaseLoader(url, header_template=header)\n",
    "docs = loader.load()\n",
    "\n",
    "# 결과 확인 (너무 길어서 앞부분 500자만 출력)\n",
    "print(f\"문서 개수: {len(docs)}\")\n",
    "print(f\"제목: {docs[0].metadata.get('title')}\")\n",
    "print(f\"\\n본문(일부):\\n{docs[0].page_content[:500]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3-2. PyPDFLoader\n",
    "PDF 파일을 페이지 단위로 읽어옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# 현재 폴더에 'The_Adventures_of_Tom_Sawyer.pdf' 파일이 있다고 가정\n",
    "if os.path.exists('The_Adventures_of_Tom_Sawyer.pdf'):\n",
    "    loader = PyPDFLoader('The_Adventures_of_Tom_Sawyer.pdf')\n",
    "    pdf_docs = loader.load()\n",
    "\n",
    "    print(f\"총 페이지 수: {len(pdf_docs)}\")\n",
    "    print(f\"15페이지 내용 일부:\\n{pdf_docs[14].page_content[:300]}...\")\n",
    "else:\n",
    "    print(\"PDF 파일이 없습니다. 실습을 위해 파일을 준비해주세요.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "### 4. Text Splitter (텍스트 분할)\n",
    "\n",
    "긴 문서를 통째로 LLM에 넣으면 **토큰 제한(Token Limit)**에 걸리거나, 검색 정확도가 떨어질 수 있습니다.\n",
    "따라서 문서를 의미 있는 단위(Chunk)로 잘개 쪼개는 과정이 필요합니다.\n",
    "\n",
    "- `RecursiveCharacterTextSplitter`: 문단 -> 문장 -> 단어 순으로 자연스럽게 자르는 가장 일반적인 분할기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 1. 분할기 생성\n",
    "# chunk_size: 각 청크의 최대 글자 수\n",
    "# chunk_overlap: 청크 간 겹치는 글자 수 (문맥 안 끊기게)\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "# 2. 문서 분할 수행 (PDF 문서 사용)\n",
    "if 'pdf_docs' in locals():\n",
    "    split_docs = text_splitter.split_documents(pdf_docs[:5])  # 앞 5페이지만 실습\n",
    "    print(f\"분할 전 문서 수: 5개\")\n",
    "    print(f\"분할 후 청크 수: {len(split_docs)}개\")\n",
    "    print(f\"\\n첫 번째 청크 내용:\\n{split_docs[0].page_content}\")\n",
    "else:\n",
    "    # PDF가 없을 경우 임시 텍스트로 실습\n",
    "    txt = \"LangChain은 아주 유용한 도구입니다. \" * 50\n",
    "    split_docs = text_splitter.create_documents([txt])\n",
    "    print(f\"분할 후 청크 수: {len(split_docs)}개\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "### 5. Embedding (임베딩)\n",
    "\n",
    "텍스트를 컴퓨터가 이해할 수 있는 **숫자 벡터(Vector)**로 변환하는 과정입니다.\n",
    "유사한 의미를 가진 텍스트는 벡터 공간에서 서로 가깝게 위치하게 됩니다.\n",
    "\n",
    "- `OpenAIEmbeddings`: 성능이 좋지만 유료 (API 호출)\n",
    "- `HuggingFaceEmbeddings`: 로컬에서 무료로 실행 가능 (성능 준수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import numpy as np\n",
    "\n",
    "# 1. OpenAI 임베딩 모델 (3-small)\n",
    "embeddings_openai = OpenAIEmbeddings(model='text-embedding-3-small')\n",
    "\n",
    "text = \"강아지는 귀엽다.\"\n",
    "vector = embeddings_openai.embed_query(text)\n",
    "print(f\"OpenAI 벡터 차원수: {len(vector)} (보통 1536차원)\")\n",
    "print(f\"앞 5개 값: {vector[:5]}\")\n",
    "\n",
    "# 2. HuggingFace 로컬 임베딩 모델 (sentence-transformers)\n",
    "# 무료이고 로컬에서 돌아가지만, 처음 다운로드 시 시간이 걸림\n",
    "# embeddings_hf = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "### 6. Vector Store & Retriever (벡터 저장소와 검색기)\n",
    "\n",
    "임베딩된 벡터들을 저장하고, 질문(Query)과 가장 유사한 문서를 찾아주는 검색 엔진입니다.\n",
    "여기서는 가볍고 빠른 **FAISS**를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# 1. 벡터 저장소 생성 (문서들을 벡터화해서 저장)\n",
    "# split_docs: 위에서 분할한 문서 청크들\n",
    "# embeddings_openai: 벡터화에 사용할 임베딩 모델\n",
    "if 'split_docs' in locals():\n",
    "    vector_db = FAISS.from_documents(split_docs, embeddings_openai)\n",
    "    print(\"FAISS 인덱스 생성 완료\")\n",
    "\n",
    "    # 2. 로컬에 저장 (선택)\n",
    "    vector_db.save_local(\"./db/faiss_index\")\n",
    "\n",
    "    # 3. 검색 (Similarity Search)\n",
    "    query = \"Tom은 누구입니까?\"  # PDF 내용 관련 질문\n",
    "    results = vector_db.similarity_search(query, k=3)  # 상위 3개 검색\n",
    "\n",
    "    print(f\"\\n['{query}'] 검색 결과:\")\n",
    "    for i, res in enumerate(results, 1):\n",
    "        print(f\"{i}. {res.page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Retriever Interface\n",
    "VectorStore를 **Retriever** 인터페이스로 변환하면, Chain(`|`)에 바로 연결하여 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retriever로 변환\n",
    "retriever = vector_db.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 3}  # 검색 옵션 설정\n",
    ")\n",
    "\n",
    "# invoke로 바로 검색 가능\n",
    "docs = retriever.invoke(\"Huck Finn은 누구인가?\")\n",
    "print(f\"검색된 문서 수: {len(docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "### 8. RAG Chain 구성해보기 (종합 예제)\n",
    "검색된 문서를 프롬프트에 넣고 LLM에게 답변하게 만드는 최종 단계입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# 1. 프롬프트 정의\n",
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "당신은 친절한 AI 어시스턴트입니다. 아래 Context를 바탕으로 사용자의 질문에 답하세요.\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\")\n",
    "\n",
    "# 2. 모델 정의\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# 3. 체인 구성 (LCEL)\n",
    "# 질문 -> Retriever가 문서 검색 -> context에 주입\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# 4. 실행\n",
    "print(chain.invoke(\"Tom Sawyer는 어떤 인물인가요?\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}