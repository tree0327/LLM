{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c6880a5",
   "metadata": {},
   "source": [
    "# Finetuning - QLoRA\n",
    "\n",
    "https://huggingface.co/docs/peft/en/developer_guides/quantization\n",
    "\n",
    "\n",
    "## Quantization\n",
    "4비트 양자화(4-bit Quantization)는 모델의 가중치를 정밀도가 낮은 4비트 데이터 형식으로 변환하여 메모리 사용량을 획기적으로 줄이는 기술이다. 지적한 대로 모든 파라미터가 양자화 대상이 되는 것은 아니며, 성능 유지를 위해 전략적으로 적용된다.\n",
    "4비트 양자화는 **\"대세인 가중치는 작게 줄이고, 민감한 레이어와 통계 정보는 원본을 유지\"**하는 전략이다. 이를 통해 일반 소비자용 GPU(예: RTX 3090/4090 24GB)에서도 30B급 대형 모델을 구동할 수 있게 된다.\n",
    "\n",
    "\n",
    "**1. 4비트 양자화 시 메모리 변화**\n",
    "\n",
    "30B 파라미터 모델을 기준으로 계산하면 다음과 같은 변화가 발생한다.\n",
    "\n",
    "- **BF16 (기본):** 파라미터당 2바이트 $\\rightarrow$ 약 **60GB** 필요\n",
    "- **4-bit (양자화):** 파라미터당 0.5바이트 $\\rightarrow$ 약 **15GB** 필요 (이론상 1/4 수준)\n",
    "\n",
    "실제로는 양자화 과정에서 발생하는 스케일링 계수(Scaling Factor)와 메타데이터 때문에 약 **17~18GB** 정도의 VRAM을 사용하게 된다.\n",
    "\n",
    "**2. 왜 모든 파라미터를 양자화하지 않는가?**\n",
    "\n",
    "모델의 성능(Perplexity) 저하를 최소화하기 위해 **혼합 정밀도(Mixed Precision)** 방식을 사용한다.\n",
    "\n",
    "- **양자화 대상 (Linear Layers):** 모델의 대부분을 차지하는 행렬 연산 가중치(Attention, MLP 레이어 등)는 4비트로 변환하여 용량을 줄인다.\n",
    "- **양자화 제외 (Sensitive Layers):**\n",
    "    - **Normalization 레이어:** LayerNorm 등은 수치 민감도가 매우 높아 원본 정밀도(FP32/BF16)를 유지한다.\n",
    "    - **Embedding 레이어:** 텍스트를 벡터로 변환하는 첫 단계이므로 정밀도가 중요하다.\n",
    "    - **LM Head:** 최종 출력층은 예측 정확도를 위해 보통 양자화하지 않는다.\n",
    "\n",
    "**3. 주요 양자화 기법 (NF4)**\n",
    "\n",
    "단순히 소수점을 자르는 것이 아니라, 데이터의 분포를 고려한 알고리즘을 사용한다. 가장 대표적인 것이 **NF4(NormalFloat 4)**이다.\n",
    "\n",
    "- **특징:** 가중치가 정규분포를 따른다는 가정하에, 값이 몰려 있는 구간에는 촘촘하게, 값이 적은 구간에는 넓게 비트를 할당한다.\n",
    "- **장점:** 일반적인 4비트 정수형(Int4)보다 정보 손실이 훨씬 적어 모델의 추론 능력을 잘 보존한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24e6a806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -Uq transformers datasets accelerate trl peft hf_transfer pydantic langchain-huggingface bitsandbytes wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79786a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Feb 12 03:18:20 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 580.126.09             Driver Version: 580.126.09     CUDA Version: 13.0     |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 5090        On  |   00000000:A1:00.0 Off |                  N/A |\n",
      "|  0%   34C    P8             13W /  575W |       0MiB /  32607MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi  # NVIDIA GPU 모델명 / 드라이브 버전 / 메모리 사용량 등 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3f3d0e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dotenv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Local 실행인 경우 아래 코드로 키를 설정\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdotenv\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      5\u001b[39m load_dotenv()\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'dotenv'"
     ]
    }
   ],
   "source": [
    "# Local 실행인 경우 아래 코드로 키를 설정\n",
    "# from dotenv import load_dotenv\n",
    "# import os\n",
    "\n",
    "# load_dotenv()\n",
    "# HF_TOKEN = os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c89abc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runpod 실행인 경우 아래 코드로 키를 설정\n",
    "import os\n",
    "\n",
    "HF_TOKEN = os.environ[\"HF_TOKEN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a378dc98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "Dataset({\n",
      "    features: ['system', 'user', 'assistant'],\n",
      "    num_rows: 1000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset  # HuggingFace 데이터셋 로더\n",
    "\n",
    "# Hub에서 split이 train인 데이터 로드\n",
    "dataset = load_dataset('capybaraOh/naver-economy-news2stock', split='train')\n",
    "print(len(dataset))  # 샘플 개수\n",
    "print(dataset)       # 객체 정보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6560ecd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'system': \"\\n당신은 금융/경제 뉴스의 핵심내용을 요약해 설명하고, \\n특정 상장 종목에 미치는 긍정/부정 영향여부, 이유, 근거를 분석하는 금융/경제 분석 전문가입니다.\\n\\n다음 출력지시사항을 지켜주세요.\\n1. 뉴스와 종목간의 연관성을 발견할 수 없다면:\\n    - stock_related를 False로 작성하세요.\\n    - summary에 뉴스의 요약을 작성하세요.\\n2. 뉴스와 종목간의 연관성을 발견했다면:\\n    - stock_related를 True로 작성하세요.\\n    - summary에 뉴스의 요약을 작성하세요.\\n    - 긍정영향이 예상되는 종목이 있다면, positive_stocks, positive_keywords, positive_reasons를 작성하세요.\\n    - 부정영향이 예상되는 종목이 있다면, negative_stocks, negative_keywords, negative_reasons를 작성하세요.\\n    - 값이 없는 경우 빈 문자열(''), 빈 리스트([])로 작성하세요.\\n\",\n",
       " 'user': '추경호 중기 수출지원 총력 무역금융 40조 확대\\n앵커 정부가 올해 하반기 우리 경제의 버팀목인 수출 확대를 위해 총력을 기울이기로 했습니다. 특히 수출 중소기업의 물류난 해소를 위해 무역금융 규모를 40조 원 이상 확대하고 물류비 지원과 임시선박 투입 등을 추진하기로 했습니다. 류환홍 기자가 보도합니다. 기자 수출은 최고의 실적을 보였지만 수입액이 급증하면서 올해 상반기 우리나라 무역수지는 역대 최악인 103억 달러 적자를 기록했습니다. 정부가 수출확대에 총력을 기울이기로 한 것은 원자재 가격 상승 등 대외 리스크가 가중되는 상황에서 수출 증가세 지속이야말로 한국경제의 회복을 위한 열쇠라고 본 것입니다. 추경호 경제부총리 겸 기획재정부 장관 정부는 우리 경제의 성장엔진인 수출이 높은 증가세를 지속할 수 있도록 총력을 다하겠습니다. 우선 물류 부담 증가 원자재 가격 상승 등 가중되고 있는 대외 리스크에 대해 적극 대응하겠습니다. 특히 중소기업과 중견기업 수출 지원을 위해 무역금융 규모를 연초 목표보다 40조 원 늘린 301조 원까지 확대하고 물류비 부담을 줄이기 위한 대책도 마련했습니다. 이창양 산업통상자원부 장관 국제 해상운임이 안정될 때까지 월 4척 이상의 임시선박을 지속 투입하는 한편 중소기업 전용 선복 적재 용량 도 현재보다 주당 50TEU 늘려 공급하겠습니다. 하반기에 우리 기업들의 수출 기회를 늘리기 위해 2 500여 개 수출기업을 대상으로 해외 전시회 참가를 지원하는 등 마케팅 지원도 벌이기로 했습니다. 정부는 또 이달 중으로 반도체를 비롯한 첨단 산업 육성 전략을 마련해 수출 증가세를 뒷받침하고 에너지 소비를 줄이기 위한 효율화 방안을 마련해 무역수지 개선에 나서기로 했습니다. YTN 류환홍입니다.',\n",
       " 'assistant': '{\"stock_related\":true,\"summary\":\"정부가 중소기업, 중견기업의 수출 확대를 지원하기 위해 무역금융을 40조원 이상 추가 확대(총 301조원)하고, 물류비 지원·임시선박 투입 등 실질 지원책을 추진한다. 또한 반도체 등 첨단산업 육성, 에너지 효율화 방안 등 종합적 무역수지 개선책을 병행할 예정이다.\",\"positive_stocks\":[\"한국항공우주\",\"CJ대한통운\",\"현대글로비스\",\"삼성전자\",\"SK하이닉스\"],\"positive_keywords\":[\"무역금융 확대\",\"수출 지원\",\"물류비 지원\",\"임시선박 투입\",\"반도체 육성\",\"수출 증가세\"],\"positive_reasons\":\"수출기업·물류기업·중소/중견 제조업체의 자금난 및 물류난 해소로 직접적 매출 확대가 기대되며, 특히 반도체 등 첨단산업은 정부의 집중 지원으로 실적 개선 및 성장 모멘텀 확보가 예상됨.\",\"negative_stocks\":[],\"negative_keywords\":[],\"negative_reasons\":\"\"}'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]  # 1번째 샘플을 확인 (딕셔너리)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958f9e41",
   "metadata": {},
   "source": [
    "## 데이터셋 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2508d69b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "test_ratio = 0.2  # 평가셋 비율\n",
    "\n",
    "train_data = []   # 학습 데이터 리스트\n",
    "test_data = []    # 테스트 데이터 리스트\n",
    "\n",
    "data_indices = list(range(len(dataset)))       # 전체 인덱스\n",
    "test_size = int(len(dataset) * test_ratio)     # 테스트 셋 크기\n",
    "\n",
    "test_data_indices = data_indices[:test_size]   # 앞부분은 평가셋으로 사용\n",
    "train_data_indices = data_indices[test_size:]  # 나머지는 학습셋으로 사용\n",
    "\n",
    "# 학습/평가 데이터셋 형식을 지정하는 함수\n",
    "def format_data(data):\n",
    "    # OpenAI / Chat 학습용 messages 포맷 반환\n",
    "    return {\n",
    "        'messages': [\n",
    "            {\n",
    "                'role': 'system',\n",
    "                'content': data['system']\n",
    "            },\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': data['user']\n",
    "            },\n",
    "            {\n",
    "                'role': 'assistant',\n",
    "                'content': data['assistant']\n",
    "            },\n",
    "        ]\n",
    "    }\n",
    "\n",
    "# 학습 / 평가 인덱스를 변환해 리스트 생성\n",
    "train_data = [format_data(dataset[i]) for i in train_data_indices]\n",
    "test_data = [format_data(dataset[i]) for i in test_data_indices]\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0f805fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'role': 'system', 'content': \"\\n당신은 금융/경제 뉴스의 핵심내용을 요약해 설명하고, \\n특정 상장 종목에 미치는 긍정/부정 영향여부, 이유, 근거를 분석하는 금융/경제 분석 전문가입니다.\\n\\n다음 출력지시사항을 지켜주세요.\\n1. 뉴스와 종목간의 연관성을 발견할 수 없다면:\\n    - stock_related를 False로 작성하세요.\\n    - summary에 뉴스의 요약을 작성하세요.\\n2. 뉴스와 종목간의 연관성을 발견했다면:\\n    - stock_related를 True로 작성하세요.\\n    - summary에 뉴스의 요약을 작성하세요.\\n    - 긍정영향이 예상되는 종목이 있다면, positive_stocks, positive_keywords, positive_reasons를 작성하세요.\\n    - 부정영향이 예상되는 종목이 있다면, negative_stocks, negative_keywords, negative_reasons를 작성하세요.\\n    - 값이 없는 경우 빈 문자열(''), 빈 리스트([])로 작성하세요.\\n\"}, {'role': 'user', 'content': '송옥렬 공정위원장 후보자 첫 출근길\\n서울 연합뉴스 임화영 기자 송옥렬 공정거래위원장 후보자가 5일 오전 서울 중구 한국공정거래조정원에 마련된 인사청문회 사무실로 첫 출근을 하며 취재진의 질문에 답하고 있다.'}, {'role': 'assistant', 'content': '{\"stock_related\":false,\"summary\":\"송옥렬 공정거래위원장 후보자가 인사청문회 사무실로 첫 출근했다는 소식으로, 인사와 관련된 기본적인 출근 동정 뉴스다.\",\"positive_stocks\":[],\"positive_keywords\":[],\"positive_reasons\":\"\",\"negative_stocks\":[],\"negative_keywords\":[],\"negative_reasons\":\"\"}'}]}\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(train_data[100])  # 학습 데이터의 101번째 샘플 확인\n",
    "print(type(train_data))  # train_data는 list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eadc2aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'content': \"\\n당신은 금융/경제 뉴스의 핵심내용을 요약해 설명하고, \\n특정 상장 종목에 미치는 긍정/부정 영향여부, 이유, 근거를 분석하는 금융/경제 분석 전문가입니다.\\n\\n다음 출력지시사항을 지켜주세요.\\n1. 뉴스와 종목간의 연관성을 발견할 수 없다면:\\n    - stock_related를 False로 작성하세요.\\n    - summary에 뉴스의 요약을 작성하세요.\\n2. 뉴스와 종목간의 연관성을 발견했다면:\\n    - stock_related를 True로 작성하세요.\\n    - summary에 뉴스의 요약을 작성하세요.\\n    - 긍정영향이 예상되는 종목이 있다면, positive_stocks, positive_keywords, positive_reasons를 작성하세요.\\n    - 부정영향이 예상되는 종목이 있다면, negative_stocks, negative_keywords, negative_reasons를 작성하세요.\\n    - 값이 없는 경우 빈 문자열(''), 빈 리스트([])로 작성하세요.\\n\", 'role': 'system'}, {'content': '송옥렬 공정위원장 후보자 첫 출근길\\n서울 연합뉴스 임화영 기자 송옥렬 공정거래위원장 후보자가 5일 오전 서울 중구 한국공정거래조정원에 마련된 인사청문회 사무실로 첫 출근을 하며 취재진의 질문에 답하고 있다.', 'role': 'user'}, {'content': '{\"stock_related\":false,\"summary\":\"송옥렬 공정거래위원장 후보자가 인사청문회 사무실로 첫 출근했다는 소식으로, 인사와 관련된 기본적인 출근 동정 뉴스다.\",\"positive_stocks\":[],\"positive_keywords\":[],\"positive_reasons\":\"\",\"negative_stocks\":[],\"negative_keywords\":[],\"negative_reasons\":\"\"}', 'role': 'assistant'}]}\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset  # Huggfing Face Dataset 컨테이너\n",
    "\n",
    "train_dataset = Dataset.from_list(train_data)  # train_data(list) -> Dataset 변환\n",
    "test_dataset = Dataset.from_list(test_data)    # test_data(list) -> Dataset 변환\n",
    "\n",
    "print(train_dataset[100])\n",
    "print(type(train_dataset))  # train_dataset은 Dataset형"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd90114",
   "metadata": {},
   "source": [
    "## BaseModel + Quantizaton-Config\n",
    "\n",
    "`BitsAndBytesConfig`는 Hugging Face Transformers에서 대형 모델을 8비트 또는 4비트로 양자화(quantization)하여 메모리 사용량을 줄이고, 저사양 환경에서도 대형 모델을 사용할 수 있게 도와주는 설정 클래스이다.\n",
    "\n",
    "**주요 파라미터 목록**\n",
    "\n",
    "| 파라미터명                  | 설명                                                                                          | 예시 값            |\n",
    "|-----------------------------|----------------------------------------------------------------------------------------------|-------------------|\n",
    "| `load_in_8bit`              | 8비트 양자화 활성화 여부. True로 설정 시 8비트로 모델 로드.                                    | True, False       |\n",
    "| `load_in_4bit`              | 4비트 양자화 활성화 여부. True로 설정 시 4비트로 모델 로드.                                    | True, False       |\n",
    "| `bnb_4bit_quant_type`       | 4비트 양자화 타입. `nf4`(NormalFloat4, 기본값), `fp4` 중 선택.                                 | \"nf4\", \"fp4\"      |\n",
    "| `bnb_4bit_compute_dtype`    | 연산에 사용할 데이터 타입. 보통 `torch.float16`, `torch.bfloat16`, `torch.float32` 중 선택.     | torch.bfloat16    |\n",
    "| `bnb_4bit_use_double_quant` | 이중 양자화 사용 여부. True로 설정 시 추가 양자화로 메모리 절감 가능.                           | True, False       |\n",
    "| `llm_int8_threshold`        | 8비트 양자화 시 threshold 지정. 값이 낮을수록 더 많은 파라미터가 8비트로 변환됨.                | 0.0 ~ 6.0         |\n",
    "| `llm_int8_skip_modules`     | 양자화에서 제외할 모듈 리스트.                                                                | [\"lm_head\"]       |\n",
    "| `bnb_4bit_quant_storage`    | 4비트 파라미터 저장에 사용할 타입. 기본값은 `torch.uint8`.                                    | torch.uint8       |\n",
    "\n",
    "\n",
    "- **load_in_8bit**  \n",
    "  8비트 양자화를 활성화하는 플래그이다. True로 설정 시 모델 파라미터를 8비트 정수로 변환하여 메모리 사용량을 약 75%까지 줄일 수 있다.\n",
    "\n",
    "- **load_in_4bit**  \n",
    "  4비트 양자화를 활성화하는 플래그이다. True로 설정 시 더욱 극적인 메모리 절감 효과를 볼 수 있다. 4비트 양자화는 QLoRA 등 최신 연구에서 자주 사용된다.\n",
    "\n",
    "- **bnb_4bit_quant_type**  \n",
    "  4비트 양자화 시 사용할 데이터 타입을 지정한다.  \n",
    "  - `nf4`: NormalFloat4 (기본값, QLoRA에서 주로 사용)  \n",
    "  - `fp4`: FP4 타입.\n",
    "\n",
    "- **bnb_4bit_compute_dtype**  \n",
    "  연산(Forward/Backward) 시 사용할 데이터 타입을 지정한다.  \n",
    "  - `torch.float16`, `torch.bfloat16`, `torch.float32` 등이 있다.  \n",
    "  - 16비트 타입을 사용하면 연산 속도가 빨라지고, 메모리 사용량도 줄일 수 있다.\n",
    "\n",
    "- **bnb_4bit_use_double_quant**  \n",
    "  이중 양자화(nested quantization)를 활성화하는 옵션이다. True로 설정 시 한 번 더 양자화를 적용하여 메모리 사용량을 추가로 절감할 수 있다. 메모리 부족 시 유용하다.\n",
    "\n",
    "- **llm_int8_threshold**  \n",
    "  8비트 양자화 시 threshold 값을 조정하여, threshold 이하의 weight만 8비트로 변환한다. 값이 낮을수록 더 많은 파라미터가 8비트로 변환된다.\n",
    "\n",
    "- **llm_int8_skip_modules**  \n",
    "  양자화에서 제외할 모듈(레이어) 리스트를 지정한다. 예를 들어, 출력 레이어(`lm_head`) 등은 양자화에서 제외할 수 있다.\n",
    "\n",
    "- **bnb_4bit_quant_storage**  \n",
    "  4비트 파라미터 저장에 사용할 데이터 타입을 지정한다. 기본값은 `torch.uint8`이다.\n",
    "\n",
    "\n",
    "**활용 팁**\n",
    "\n",
    "- **메모리가 부족하다면**: `bnb_4bit_use_double_quant=True`로 설정.\n",
    "- **정밀도가 중요하다면**: `bnb_4bit_quant_type=\"nf4\"`로 설정.\n",
    "- **학습 속도가 중요하다면**: `bnb_4bit_compute_dtype`를 16비트(float16, bfloat16)로 설정.\n",
    "\n",
    "- `BitsAndBytesConfig`는 4비트/8비트 양자화 옵션을 통합 관리하며, 파라미터 조합을 통해 다양한 하드웨어 환경에 맞는 최적화가 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1ae8ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4bit 양자화 설정(BitsAndBytesConfig) 구성\n",
    "from transformers import BitsAndBytesConfig  # 양자화 설정\n",
    "import torch\n",
    "\n",
    "# 4bit 양자화 객체 설정\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,                    # 모델 가중치를 4bit로 로드\n",
    "    bnb_4bit_quant_type = 'nf4',            # 4bit 양자화 방식\n",
    "    bnb_4bit_use_double_quant = True,       # 더블 양자화(메모리/정확도 균형 개선)\n",
    "    bnb_4bit_compute_dtype= torch.bfloat16  # 연산 dtype (추론/학습시 연산은 bfloat16)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0488fed8",
   "metadata": {},
   "source": [
    "## NCSOFT/Llama-VARCO-8B-Instruct란?\n",
    "https://huggingface.co/NCSOFT/Llama-VARCO-8B-Instruct\n",
    "\n",
    "\n",
    "* **기반 모델:** Meta의 Llama-3.1-8B 모델을 기반으로 한다.\n",
    "* **개발 목적:** 한국어 능력을 극대화하는 동시에 영어 구사 능력도 유지하도록 설계되었다.\n",
    "* **학습 방법:** 한국어와 영어 데이터셋을 활용한 지속 사전 학습(Continual Pre-training)을 거쳤으며, 이후 지도 미세 조정(SFT)과 직접 선호도 최적화(DPO)를 통해 인간의 선호도에 맞게 정렬되었다.\n",
    "\n",
    "\n",
    "**SFT에서 한국어능력향상과 동시에 영어능력유지란:**\n",
    "\n",
    "일반적으로 한국어 데이터를 대량으로 추가 학습시키면 기존에 모델이 가지고 있던 영어 지식이 손상되는 '파괴적 망각(Catastrophic Forgetting)' 현상이 발생한다. 엔씨소프트는 이를 방지하기 위해 **지속 사전 학습(Continual Pre-training)**을 적용했다.\n",
    "\n",
    "**_1. 데이터 믹스(Data Mixing) 전략:_**\n",
    "\n",
    "단순히 한국어 데이터만 밀어 넣는 것이 아니라, 모델이 이미 학습했던 영어 데이터와 고품질의 한국어 데이터를 특정 비율로 섞어 학습한다. 이를 통해 기존의 영어 추론 능력을 '복습'하면서 새로운 언어 체계를 '습득'하게 된다.\n",
    "\n",
    "**_2. 토크나이저 효율화와 임베딩 확장:_**\n",
    "\n",
    "기존 Llama-3.1의 토크나이저 성능을 유지하면서 한국어 표현력을 높이기 위해 어휘 사전(Vocabulary)을 최적화한다. 영어 토큰 정보는 건드리지 않고 한국어 토큰의 밀도를 높여 두 언어 간의 연결 고리를 강화하는 방식이다.\n",
    "\n",
    "**_3. 지식 전이(Knowledge Transfer):_**\n",
    "\n",
    "영어 데이터로 학습된 모델의 강력한 논리적 사고 능력을 한국어로 전이시키는 과정을 거친다.\n",
    "\n",
    "* **추론 능력 유지:** 수학이나 코딩 같은 논리적 작업은 영어 데이터에서 배운 구조를 그대로 활용한다.\n",
    "* **언어 정렬:** SFT(지도 미세 조정) 단계에서 동일한 질문을 한국어와 영어로 번급하며 학습시켜, 언어에 상관없이 일관된 답변을 내놓도록 유도한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "606622b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77ce2c77d682425d96cc4ca569dd40ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/291 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM  # 토크나이저 / 생성형 모델 로더\n",
    "import torch\n",
    "\n",
    "pretrained_model_name = 'NCSOFT/Llama-VARCO-8B-Instruct'  # 사용할 사전학습 모델 ID\n",
    "\n",
    "# 사전학습 CausalLM 모델 로드\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name,   # 모델 이름\n",
    "    dtype = torch.bfloat16,  # 가중치 로딩 dtype(bf16)\n",
    "    device_map = 'auto',     # 환경에 맞게 CPU/GPU 자동 배치\n",
    "    quantization_config = quant_config  # 4bit 양자화 설정 적용\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)  # 해당 모델의 토크나이저 로드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1116a99f",
   "metadata": {},
   "source": [
    "## llama-3 chat template 변환\n",
    "\n",
    "Llama3 모델은 특정 chat template 형식으로 학습되어, 그 형식을 사용해야 최적 성능을 낼 수 있다.\n",
    "Chat template을 사용하지 않으면 모델이 대화 구조를 제대로 인식하지 못할 수 있다.\n",
    "opean_ai 형식의 데이터를 llama-3 형식으로 변환한다.\n",
    "\n",
    "\n",
    "**LLaMA-3 채팅 포맷**\n",
    "LLaMA-3 채팅 포맷은 LLaMA-3 계열 챗봇 모델이 대화 내용을 이해하고 답변할 수 있도록 만들어진 입력 데이터 구조입니다.\n",
    "여러 역할(시스템, 유저, 어시스턴트)의 메시지를 특별한 토큰과 구조로 묶어서 하나의 프롬프트로 합치는 방식입니다.\n",
    "구조 예시\n",
    "아래와 같이 대화 흐름을 명확히 구분하는 토큰들이 사용됩니다:\n",
    "\n",
    "```\n",
    "<|begin_of_text|>\n",
    "<|start_header_id|>system<|end_header_id|>\n",
    "[시스템 역할 지침]<|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "[유저 질문]<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "[모델의 답변]<|eot_id|>\n",
    "```\n",
    "* <|begin_of_text|> : 전체 프롬프트의 시작을 알리는 토큰\n",
    "* <|start_header_id|>role<|end_header_id|> : 각 메시지의 역할 구분(시스템, 유저, 어시스턴트 등)\n",
    "* 각 메시지 끝에 <|eot_id|> : 하나의 메시지 블록이 끝났음을 알림\n",
    "* 마지막 assistant 블럭은 응답 생성 위치를 가리킨다. apply_chat_template(add_generation_prompt=False)로 설정했더라도 내부 템플릿에는 응답을 받을 자리 표시자로 <|assistant|> 토큰이 남아 있어, \"여기서부터 어시스턴트가 답변을 생성해야 한다\"는 신호를 제공하는 것임.\n",
    "\n",
    "**왜 이 포맷이 필요할까?**\n",
    "\n",
    "* 모델이 **“어디까지가 시스템 안내, 어디서부터가 유저 질문, 어디서부터가 답변인지”** 정확하게 파악할 수 있다.\n",
    "* 여러 턴(turn)의 대화가 이어질 때도 메시지 경계를 명확히 구분해 혼동 없이 맥락을 유지할 수 있다.\n",
    "* LLaMA-3 계열 모델은 이런 포맷으로 학습되어 있기 때문에 **실전 파인튜닝/추론 시에도 반드시 이 구조로 입력해야** 기대하는 챗봇 성능을 발휘할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3126ee9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [128000, 128006, 9125, 128007, 271, 65895, 83628, 34804, 104193, 123061, 14, 127463, 111068, 120226, 125959, 102612, 96318, 27797, 18359, 87097, 103168, 34983, 114942, 101360, 11, 720, 108159, 30381, 59134, 41953, 99458, 88708, 19954, 101412, 116129, 41871, 235, 30381, 14, 64189, 30381, 115754, 58126, 64189, 11, 111436, 11, 106589, 93292, 18918, 109862, 44005, 104193, 123061, 14, 127463, 109862, 116425, 20565, 80052, 382, 13447, 49531, 62226, 22035, 30426, 115790, 18359, 67890, 115061, 92769, 627, 16, 13, 111068, 25941, 81673, 99458, 88708, 63375, 21028, 78453, 101106, 111490, 121712, 48936, 29833, 47782, 115300, 512, 262, 482, 5708, 54356, 18918, 3641, 17835, 114839, 92245, 627, 262, 482, 12399, 19954, 111068, 120226, 87097, 103168, 18359, 114839, 92245, 627, 17, 13, 111068, 25941, 81673, 99458, 88708, 63375, 21028, 78453, 101106, 111490, 121712, 101528, 33390, 512, 262, 482, 5708, 54356, 18918, 3082, 17835, 114839, 92245, 627, 262, 482, 12399, 19954, 111068, 120226, 87097, 103168, 18359, 114839, 92245, 627, 262, 482, 41871, 235, 30381, 101090, 104762, 13094, 96717, 57002, 107205, 99458, 88708, 13094, 91786, 33390, 11, 6928, 1284, 26246, 11, 6928, 52454, 11, 6928, 39329, 82, 18918, 114839, 92245, 627, 262, 482, 86503, 30381, 101090, 104762, 13094, 96717, 57002, 107205, 99458, 88708, 13094, 91786, 33390, 11, 8389, 1284, 26246, 11, 8389, 52454, 11, 8389, 39329, 82, 18918, 114839, 92245, 627, 262, 482, 46663, 13094, 108838, 50152, 122292, 81021, 55055, 493, 4670, 122292, 84734, 10779, 8, 17835, 114839, 92245, 13, 128009, 128006, 882, 128007, 271, 102937, 115526, 127139, 100994, 30381, 110903, 41953, 124999, 26799, 111915, 102722, 104152, 106103, 198, 115978, 78453, 100660, 121096, 105813, 57390, 101090, 126887, 112822, 115526, 127139, 100994, 30381, 105247, 110903, 41953, 124999, 108181, 220, 20, 33177, 74177, 66965, 106010, 72043, 89359, 104008, 79225, 30381, 105247, 93917, 30381, 55421, 19954, 96677, 103304, 53400, 59777, 56154, 102039, 52688, 62841, 124058, 101272, 17835, 111915, 102722, 104152, 18359, 55000, 101203, 107545, 58232, 86351, 21028, 109760, 19954, 108386, 101360, 91786, 13, 128009, 128006, 78191, 128007, 271, 5018, 13787, 54356, 794, 3934, 1359, 1743, 3332, 102937, 115526, 127139, 100994, 30381, 105247, 110903, 41953, 124999, 108181, 59777, 56154, 102039, 52688, 62841, 124058, 101272, 17835, 111915, 102722, 104152, 101528, 16969, 101228, 77437, 43139, 11, 59777, 56154, 81673, 106434, 53400, 114213, 103684, 102722, 104152, 101604, 30381, 111068, 25941, 13447, 48991, 31587, 1284, 26246, 9075, 29603, 31587, 52454, 9075, 29603, 31587, 39329, 82, 61867, 43324, 1284, 26246, 9075, 29603, 43324, 52454, 9075, 29603, 43324, 39329, 82, 63466, 92, 128009, 128006, 78191, 128007, 271], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "# apply_chat_template 함수\n",
    "# - openai 방식의 메시지를 llama3 방식으로 변환\n",
    "text = tokenizer.apply_chat_template(train_dataset[100]['messages'], tokinize=False)  # messages -> 채팅 프롬프트 문자열 변환\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1c4956",
   "metadata": {},
   "source": [
    "### data_collator 함수\n",
    "\n",
    "* 미니배치(batch) 데이터를 모델이 바로 학습할 수 있는 형태(토큰·마스크·정답)로 변환합니다.\n",
    "* 특히 아래와 같은 LLaMA-3 채팅 포맷을 쓸 때,\n",
    "  “어디까지가 질문/어디서부터가 답변(assistant)인지”를 정확히 구분해서\n",
    "  모델이 정답(답변 부분)만 학습하도록 레이블을 지정합니다.\n",
    "\n",
    "#### 함수 설명\n",
    "\n",
    "**1. 프롬프트 생성 (Prompt Construction)**\n",
    "\n",
    "입력받은 `batch` 데이터는 리스트 내에 여러 메시지(`system`, `user`, `assistant`)를 포함하는 사전(dict) 구조이다.\n",
    "\n",
    "* Llama 3의 특수 토큰(` <|begin_of_text|>`, `<|start_header_id|>`, `<|eot_id|>`)을 사용하여 모든 대화 내용을 하나의 긴 문자열로 병합한다.\n",
    "* 각 역할(role)의 시작과 끝을 명확히 구분하여 모델이 대화 맥락을 이해할 수 있도록 구성한다.\n",
    "\n",
    "**2. 토크나이즈 및 패딩 (Tokenization)**\n",
    "\n",
    "병합된 문자열 리스트를 `tokenizer`를 통해 숫자 ID(`input_ids`)로 변환한다.\n",
    "\n",
    "* `padding=True`: 배치 내의 문장들 중 가장 긴 문장을 기준으로 길이를 맞춘다.\n",
    "* `truncation=True`: `max_length`를 초과하는 데이터는 절단한다.\n",
    "* `return_tensors=\"pt\"`: PyTorch 텐서 형식으로 결과를 반환한다.\n",
    "\n",
    "**3. 레이블 생성 및 Loss Masking**\n",
    "\n",
    "이 함수의 핵심 부분이다. 모델이 '사용자의 질문'이 아닌 **'모델의 답변(assistant)'** 부분에 대해서만 학습하도록 설정한다.\n",
    "\n",
    "* **-100 값의 의미**: PyTorch의 `CrossEntropyLoss`는 레이블 값이 `-100`인 경우 손실(Loss) 계산에서 제외한다. 이를 통해 모델은 질문 부분을 예측하려고 노력하지 않고, 답변 부분의 정확도에만 집중하게 된다.\n",
    "* **구간 탐색**: `assistant_tokens`를 기점으로 답변이 시작되는 위치를 찾고, `<|eot_id|>` 토큰이 나오는 지점까지의 인덱스를 추출한다.\n",
    "* **값 복사**: 해당 구간의 `labels`에만 실제 `input_ids` 값을 복사하여 넣는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb8a3c9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[128000, 128006,   9125,  ...,   1210,     92, 128009],\n",
       "         [128000, 128006,   9125,  ...,      0,      0,      0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'labels': tensor([[  -100,   -100,   -100,  ...,   1210,     92, 128009],\n",
       "         [  -100,   -100,   -100,  ...,   -100,   -100,   -100]])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chat 모델 학습용 데이터 콜레이터: 프롬프트 생성 -> 토크나이저/패딩 -> assistant 구간만 라벨링\n",
    "\n",
    "# 배치(messages)를 모델 학습 텐서로 변환하는 함수\n",
    "def data_collator(batch, tokenizer=tokenizer, max_length=8192):\n",
    "    # 1. 프롬프트 생성\n",
    "    prompts = []  # 배치프롬프트 문자열을 담을 리스트\n",
    "    for example in batch:\n",
    "        prompt = '<|begin_of_text|>'  # 프롬프트 시작\n",
    "        for msg in example['messages']:  # system/user/assistant 순회\n",
    "            role = msg['role']\n",
    "            content = msg['content'].strip()\n",
    "            # role+content를 템플릿으로 누적\n",
    "            prompt += f\"<|start_header_id|>{role}<|end_header_id|>\\n{content}<|eot_id|>\"\n",
    "        prompts.append(prompt)  # 완성된 프롬프트를 배치 리스트에 추가\n",
    "    \n",
    "    # 2. 토큰처리/패딩/텐서변환\n",
    "    tokenized = tokenizer(        # 프롬프트를 토큰화해서 텐서로 변환\n",
    "        prompts,                  # 배치 프롬프트 목록\n",
    "        truncation = True,        # 최대길이 초과시 자름\n",
    "        max_length = max_length,  # 최대 토큰 길이\n",
    "        padding = True,           # 배치 내 최장 길이 기준 패딩\n",
    "        return_tensors = \"pt\"     # Pytorch 텐서 반환\n",
    "    )\n",
    "    input_ids = tokenized['input_ids']  # 토큰 id 텐서\n",
    "    attention_mask = tokenized['attention_mask']  # 패딩 마스크 텐서\n",
    "\n",
    "    # 3. 라벨 생성\n",
    "    labels = torch.full_like(input_ids, fill_value=-100)  # 기본은 -100(손실 계산에서 제외)\n",
    "\n",
    "    assistant_header = '<|start_header_id|>assistant<|end_header_id|>'\n",
    "    assistant_token_id = tokenizer.encode(assistant_header, add_special_tokens=False)  # 헤더의 토큰 패턴\n",
    "    eot_token = '<|eot_id|>'\n",
    "    eot_token_id = tokenizer.encode(eot_token, add_special_tokens=False)  # 종료 토큰의 토큰 패턴\n",
    "\n",
    "    for i, ids in enumerate(input_ids):  # 배치 내 각 샘플별로 라벨 구간 설정\n",
    "        ids_list = ids.tolist()          # 슬라이싱 비교를 위한 리스트 변환\n",
    "        # assistant 답변 시작위치 찾기\n",
    "        # - <|start_header_id|>assistant<|end_header_id|>\\n 다음 인덱스부터 답변 시작\n",
    "        start = None  # 답변 시작 인덱스 초기화\n",
    "        for idx in range(len(ids_list) - len(assistant_token_id) + 1):  # 헤더 길이만큼 슬라이딩 탐색\n",
    "            if ids_list[idx: idx + len(assistant_token_id)] == assistant_token_id:  # 헤더 패턴 매칭\n",
    "                start = idx + len(assistant_token_id)  # 헤더 다음 토큰부터 라벨 시작\n",
    "                break  # 첫 번째 assistant 구간만 탐색\n",
    "        # 답변 끝 위치 찾기\n",
    "        # - <|eot_id|> 까지\n",
    "        if start is not None:  # assistant 헤더를 찾은 경우\n",
    "            end = None  # 답변 종료 인덱스 초기화\n",
    "            # start 이후부터 <|eot_id|> 패턴이 나오는 곳까지를 탐색\n",
    "            for idx in range(start, len(ids_list) - len(eot_token_id) + 1):\n",
    "                if ids_list[idx: idx + len(eot_token_id)] == eot_token_id:  # 종료 패턴 매칭\n",
    "                    end = idx + len(eot_token_id)  # eot까지 포함한 구간 설정\n",
    "                    break  # 첫 번째 eot 탐색 완료시 종료\n",
    "        \n",
    "        labels[i, start:end] = input_ids[i, start:end]  # assistant 답변 구간을 정답 라벨로 복사\n",
    "    \n",
    "    return {\n",
    "        'input_ids': input_ids,            # 모델 입력\n",
    "        'attention_mask': attention_mask,  # 패딩 마스크\n",
    "        'labels': labels                   # 손실 계산용 라벨(assistant 구간)\n",
    "    }\n",
    "\n",
    "data_collator([train_dataset[0], train_dataset[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c6ed3d",
   "metadata": {},
   "source": [
    "### Causal Language Model 파인튜닝: input_ids와 labels 구조 이해\n",
    "\n",
    "**데이터 구조**\n",
    "```\n",
    "input_ids:  [system_tokens..., user_tokens..., assistant_tokens...]  # 전체 시퀀스\n",
    "labels:     [-100, -100, ..., -100, assistant_tokens...]          # assistant만 학습 대상\n",
    "```\n",
    "\n",
    "| 항목 | 내용 |\n",
    "| --- | --- |\n",
    "| **Input IDs** | 프롬프트 + 정답 (전체 시퀀스) |\n",
    "| **Labels** | `-100` (프롬프트 구간) + 정답 토큰 (답변 구간) |\n",
    "| **결과** | 모델은 입력을 다 보지만, 오직 답변을 맞히는 과정에서만 학습이 일어남 |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "> **질문에 해당하는 input_ids에 이미 답이 포함되어 있다!**\n",
    ">\n",
    "> **\"답이 이미 있는데 어떻게 학습하는가?\"**\n",
    ">\n",
    "> 모델은 정답을 \"보면서\" 각 위치에서 올바른 다음 토큰을 예측하는 법을 배운다. 마치 학생이 모범답안을 보며 \"이 상황에서는 이렇게 답해야 한다\"를 학습하는 것과 같다. 이것이 현대 LLM 파인튜닝의 핵심 메커니즘이다!\n",
    "\n",
    "\n",
    "**_1. 인과적 언어 모델링 (Causal Language Modeling):_**\n",
    "\n",
    "LLM(Llama, GPT 등)은 **이전 토큰들을 보고 다음 토큰을 예측**하는 방식으로 학습한다. 따라서 학습 데이터에는 프롬프트와 정답이 모두 포함된 전체 문장이 들어가야 한다.\n",
    "\n",
    "* **학습 원리:** 모델은 번째 토큰까지를 입력으로 받아 번째 토큰을 예측한다.\n",
    "* **구조:** `input_ids`가 `[A, B, C, D]`라면, 모델은 내부적으로 `A`를 보고 `B`를, `A, B`를 보고 `C`를 예측하는 과정을 동시에 수행한다.\n",
    "\n",
    "**_2. Teacher Forcing 기법:_**\n",
    "```\n",
    "Position:   [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
    "input_ids:  [A, B, C, D, E, F, G, H, I]\n",
    "labels:     [-100, -100, -100, -100, E, F, G, H, I]\n",
    "```\n",
    "\n",
    "학습 과정:\n",
    "- Position 4: A,B,C,D를 보고 → E 예측\n",
    "- Position 5: A,B,C,D,E를 보고 → F 예측  \n",
    "- Position 6: A,B,C,D,E,F를 보고 → G 예측\n",
    "\n",
    "**_3. Labels와 Loss 계산의 역할:_**\n",
    "\n",
    "`input_ids`에 정답이 포함되어 있더라도, 모델이 모든 구간에 대해 학습(손실 계산)을 수행하는 것은 아니다. 이때 중요한 역할을 하는 것이 바로 코드에 작성된 **`labels`**이다.\n",
    "\n",
    "* **-100의 의미:** PyTorch의 `CrossEntropyLoss`는 기본적으로 레이블 값이 `-100`인 위치를 무시(ignore)한다.\n",
    "* **학습 차단:** 코드에서 프롬프트(User 질문 등) 구간의 레이블을 `-100`으로 설정했기 때문에, 모델이 프롬프트 내용을 예측하며 발생하는 오차는 학습에 반영되지 않는다.\n",
    "* **학습 집중:** 오직 `assistant`의 답변 구간에 해당하는 `labels`만 실제 `input_ids` 값을 가지므로, 모델은 **\"프롬프트가 주어졌을 때 정답을 생성하는 방법\"**에 대해서만 가중치를 업데이트한다.\n",
    "\n",
    "\n",
    "**학습 vs 추론의 차이**\n",
    "\n",
    "**_학습 시:_**\n",
    "```\n",
    "input_ids: <system>당신은 금융분석가</system><user>뉴스내용</user><assistant>분석결과</assistant>\n",
    "labels:    [-100, -100, ..., -100, 분석결과_토큰들]\n",
    "```\n",
    "\n",
    "**_추론 시:_**\n",
    "```\n",
    "input:  <system>당신은 금융분석가</system><user>뉴스내용</user><assistant>\n",
    "output: 분석결과 (모델이 한 토큰씩 생성)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3db23bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 916])\n",
      "torch.Size([1, 916])\n",
      "torch.Size([1, 916])\n"
     ]
    }
   ],
   "source": [
    "example = train_dataset[128]               # 129번째 샘플\n",
    "batch = data_collator([example])           # 배치 1개로 콜레이터 적용\n",
    "\n",
    "print(f'{batch['input_ids'].shape}')       # input_ids 텐서 shape 확인 (batch, seq_len)\n",
    "print(f'{batch['attention_mask'].shape}')  # attention_mask 텐서 shape 확인 (batch, seq_len)\n",
    "print(f'{batch['labels'].shape}')          # labels 텐서 shape 확인 (batch, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d0b13e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[128000, 128006, 9125, 128007, 198, 65895, 83628, 34804, 104193, 123061, 14, 127463, 111068, 120226, 125959, 102612, 96318, 27797, 18359, 87097, 103168, 34983, 114942, 101360, 11, 720, 108159, 30381, 59134, 41953, 99458, 88708, 19954, 101412, 116129, 41871, 235, 30381, 14, 64189, 30381, 115754, 58126, 64189, 11, 111436, 11, 106589, 93292, 18918, 109862, 44005, 104193, 123061, 14, 127463, 109862, 116425, 20565, 80052, 382, 13447, 49531, 62226, 22035, 30426, 115790, 18359, 67890, 115061, 92769, 627, 16, 13, 111068, 25941, 81673, 99458, 88708, 63375, 21028, 78453, 101106, 111490, 121712, 48936, 29833, 47782, 115300, 512, 262, 482, 5708, 54356, 18918, 3641, 17835, 114839, 92245, 627, 262, 482, 12399, 19954, 111068, 120226, 87097, 103168, 18359, 114839, 92245, 627, 17, 13, 111068, 25941, 81673, 99458, 88708, 63375, 21028, 78453, 101106, 111490, 121712, 101528, 33390, 512, 262, 482, 5708, 54356, 18918, 3082, 17835, 114839, 92245, 627, 262, 482, 12399, 19954, 111068, 120226, 87097, 103168, 18359, 114839, 92245, 627, 262, 482, 41871, 235, 30381, 101090, 104762, 13094, 96717, 57002, 107205, 99458, 88708, 13094, 91786, 33390, 11, 6928, 1284, 26246, 11, 6928, 52454, 11, 6928, 39329, 82, 18918, 114839, 92245, 627, 262, 482, 86503, 30381, 101090, 104762, 13094, 96717, 57002, 107205, 99458, 88708, 13094, 91786, 33390, 11, 8389, 1284, 26246, 11, 8389, 52454, 11, 8389, 39329, 82, 18918, 114839, 92245, 627, 262, 482, 46663, 13094, 108838, 50152, 122292, 81021, 55055, 493, 4670, 122292, 84734, 10779, 8, 17835, 114839, 92245, 13, 128009, 128006, 882, 128007, 198, 19954, 32179, 64189, 86157, 111535, 103272, 101974, 101665, 100968, 14260, 58368, 56154, 101436, 102058, 101151, 102888, 60861, 198, 19954, 32179, 64189, 86157, 13094, 102155, 34983, 100654, 38187, 79225, 103866, 57575, 102722, 102133, 44005, 127385, 112542, 111535, 103272, 101974, 101665, 100968, 81673, 107715, 74177, 56154, 101436, 102058, 101151, 103678, 103866, 18359, 127141, 56773, 220, 17, 62841, 125274, 43139, 124141, 777, 124460, 100933, 50273, 117, 33229, 87472, 111323, 220, 1591, 125085, 63207, 19954, 102888, 60861, 52976, 35495, 220, 16, 33177, 116283, 35859, 104828, 13, 118089, 111535, 103272, 101974, 101665, 100968, 102058, 101151, 34804, 102155, 34983, 100654, 38187, 79225, 103866, 57575, 74177, 66965, 220, 23, 30426, 220, 914, 80816, 19954, 102722, 102133, 34983, 103055, 22035, 100994, 103866, 19954, 74177, 66965, 220, 806, 30426, 220, 1272, 80816, 101703, 111283, 101360, 110946, 100654, 104790, 34804, 124467, 220, 16, 118472, 102722, 102133, 34983, 102155, 34983, 79225, 103866, 19954, 124467, 220, 20, 30426, 220, 966, 80816, 101703, 111283, 44005, 125274, 43139, 56773, 220, 17, 62841, 103678, 103866, 52976, 13, 127385, 112542, 34804, 39250, 100654, 45618, 124141, 777, 86422, 56154, 81673, 107696, 83628, 108712, 102757, 84618, 64189, 18918, 74959, 88525, 51796, 54059, 117542, 120591, 58901, 121528, 13094, 125502, 109916, 13447, 13, 118089, 74177, 56154, 101436, 102058, 101151, 34804, 102155, 34983, 79225, 103866, 57575, 74177, 66965, 220, 23, 30426, 220, 1758, 80816, 19954, 102722, 102133, 34983, 105131, 125166, 79225, 103866, 19954, 74177, 66965, 220, 605, 30426, 101703, 111283, 110946, 100654, 104790, 34804, 105131, 125166, 79225, 103866, 57575, 120889, 220, 717, 118472, 102722, 102133, 34983, 102155, 34983, 79225, 103866, 19954, 124467, 220, 16, 30426, 220, 966, 80816, 101703, 111283, 44005, 125274, 43139, 56773, 220, 17, 62841, 103678, 103866, 52976, 13, 91586, 32179, 64189, 86157, 116680, 112953, 117717, 107034, 104790, 34804, 101480, 71682, 26799, 39250, 100654, 13094, 107067, 55421, 33943, 238, 18359, 54718, 115062, 102133, 44005, 121528, 29833, 36811, 18918, 101585, 101838, 67525, 107472, 101585, 38187, 82068, 66610, 60798, 103959, 35495, 116283, 35859, 104828, 13, 23955, 39277, 244, 109018, 91586, 32179, 64189, 86157, 34804, 23955, 104684, 72043, 118089, 57575, 102722, 102133, 44005, 103651, 101109, 102474, 61415, 102133, 102268, 74618, 29726, 102581, 101852, 64189, 102058, 101151, 120908, 103678, 103866, 48936, 126088, 101568, 13, 121772, 100654, 38187, 79225, 103866, 107031, 23955, 104684, 220, 975, 33177, 103551, 106248, 101532, 104182, 50467, 40275, 255, 75908, 100966, 243, 95415, 107872, 58368, 101436, 78102, 102058, 126712, 126902, 107545, 103866, 52976, 13, 128009, 128006, 78191, 128007, 198, 5018, 13787, 54356, 794, 1904, 1359, 1743, 3332, 19954, 32179, 64189, 86157, 13094, 124141, 777, 124460, 100933, 50273, 117, 111323, 220, 1591, 125085, 63207, 19954, 102155, 34983, 100654, 38187, 79225, 103866, 57575, 127385, 112542, 111535, 103272, 101974, 101665, 100968, 81673, 107715, 74177, 56154, 101436, 102058, 101151, 103678, 103866, 18359, 56773, 220, 17, 62841, 102888, 60861, 108859, 11, 124389, 103651, 101109, 102474, 61415, 102133, 102268, 11, 74618, 29726, 102581, 11, 101852, 64189, 78102, 101604, 101963, 54059, 102058, 101151, 49085, 103678, 103866, 18359, 94821, 52976, 13, 121772, 79225, 103866, 102133, 115878, 102058, 101151, 103686, 67945, 49085, 96717, 35495, 101528, 13, 127385, 112542, 21028, 101480, 102079, 29102, 14260, 100981, 106113, 83628, 39250, 100654, 126950, 11, 107715, 74177, 56154, 101436, 102058, 101151, 78102, 123102, 58126, 101066, 29833, 36811, 107034, 124784, 101585, 38187, 104182, 122358, 112012, 24486, 66610, 60798, 101568, 48991, 31587, 1284, 26246, 37899, 19954, 32179, 64189, 86157, 69982, 31587, 52454, 37899, 100654, 38187, 101151, 103678, 103866, 102888, 60861, 2247, 34983, 104065, 58126, 101066, 29833, 36811, 103686, 67945, 2247, 58189, 101963, 54059, 14260, 123256, 102058, 101151, 103686, 67945, 2247, 102525, 117465, 111323, 102058, 101151, 107067, 55421, 69982, 31587, 39329, 82, 3332, 67218, 105, 100933, 50273, 117, 111323, 80402, 113, 105940, 234, 105543, 101954, 123102, 58126, 101066, 29833, 36811, 117208, 102249, 55216, 67945, 11, 115878, 101151, 102058, 101151, 102888, 60861, 101824, 107034, 104790, 43139, 91586, 32179, 64189, 86157, 21028, 107744, 79225, 102058, 101151, 29833, 108964, 103686, 67945, 20565, 96717, 57002, 106910, 41871, 235, 30381, 82068, 126652, 109720, 29833, 127406, 13, 125578, 107715, 11, 101604, 101963, 54059, 11, 127385, 112542, 78102, 115613, 109299, 102058, 101151, 21028, 66610, 21121, 107067, 55421, 34804, 102293, 71023, 122862, 17835, 105164, 89881, 113191, 96451, 115602, 108499, 49531, 48991, 43324, 1284, 26246, 9075, 29603, 43324, 52454, 9075, 29603, 43324, 39329, 82, 63466, 92, 128009]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 198, 5018, 13787, 54356, 794, 1904, 1359, 1743, 3332, 19954, 32179, 64189, 86157, 13094, 124141, 777, 124460, 100933, 50273, 117, 111323, 220, 1591, 125085, 63207, 19954, 102155, 34983, 100654, 38187, 79225, 103866, 57575, 127385, 112542, 111535, 103272, 101974, 101665, 100968, 81673, 107715, 74177, 56154, 101436, 102058, 101151, 103678, 103866, 18359, 56773, 220, 17, 62841, 102888, 60861, 108859, 11, 124389, 103651, 101109, 102474, 61415, 102133, 102268, 11, 74618, 29726, 102581, 11, 101852, 64189, 78102, 101604, 101963, 54059, 102058, 101151, 49085, 103678, 103866, 18359, 94821, 52976, 13, 121772, 79225, 103866, 102133, 115878, 102058, 101151, 103686, 67945, 49085, 96717, 35495, 101528, 13, 127385, 112542, 21028, 101480, 102079, 29102, 14260, 100981, 106113, 83628, 39250, 100654, 126950, 11, 107715, 74177, 56154, 101436, 102058, 101151, 78102, 123102, 58126, 101066, 29833, 36811, 107034, 124784, 101585, 38187, 104182, 122358, 112012, 24486, 66610, 60798, 101568, 48991, 31587, 1284, 26246, 37899, 19954, 32179, 64189, 86157, 69982, 31587, 52454, 37899, 100654, 38187, 101151, 103678, 103866, 102888, 60861, 2247, 34983, 104065, 58126, 101066, 29833, 36811, 103686, 67945, 2247, 58189, 101963, 54059, 14260, 123256, 102058, 101151, 103686, 67945, 2247, 102525, 117465, 111323, 102058, 101151, 107067, 55421, 69982, 31587, 39329, 82, 3332, 67218, 105, 100933, 50273, 117, 111323, 80402, 113, 105940, 234, 105543, 101954, 123102, 58126, 101066, 29833, 36811, 117208, 102249, 55216, 67945, 11, 115878, 101151, 102058, 101151, 102888, 60861, 101824, 107034, 104790, 43139, 91586, 32179, 64189, 86157, 21028, 107744, 79225, 102058, 101151, 29833, 108964, 103686, 67945, 20565, 96717, 57002, 106910, 41871, 235, 30381, 82068, 126652, 109720, 29833, 127406, 13, 125578, 107715, 11, 101604, 101963, 54059, 11, 127385, 112542, 78102, 115613, 109299, 102058, 101151, 21028, 66610, 21121, 107067, 55421, 34804, 102293, 71023, 122862, 17835, 105164, 89881, 113191, 96451, 115602, 108499, 49531, 48991, 43324, 1284, 26246, 9075, 29603, 43324, 52454, 9075, 29603, 43324, 39329, 82, 63466, 92, 128009]\n"
     ]
    }
   ],
   "source": [
    "print(batch['input_ids'][0].tolist())       # 샘플의 input_ids를 리스트로 확인\n",
    "print(batch['attention_mask'][0].tolist())  # 샘플의 attention_mask를 리스트로 확인\n",
    "print(batch['labels'][0].tolist())          # 샘플의 labels를 리스트로 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "007ca0f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n{\"stock_related\":true,\"summary\":\"에어부산이 코로나19 팬데믹 이후 28개월 만에 김해국제공항에서 몽골 울란바토르와 일본 오사카 노선 운항을 주 2회 재개하며, 곧 코타키나발루, 나트랑, 세부 등 동남아 노선도 운항을 시작한다. 인천공항발 국제 노선 확대도 예고했다. 몽골의 무격리·무백신 입국 정책, 일본 오사카 노선 등 해외여행 수요 증대를 선제적으로 겨냥한 조치이다.\",\"positive_stocks\":[\"에어부산\"],\"positive_keywords\":[\"국제선 운항 재개\",\"해외여행 수요 확대\",\"동남아·일본 노선 확대\",\"코로나 이후 노선 복원\"],\"positive_reasons\":\"팬데믹 이후 억눌렸던 해외여행 수요 급증 기대, 국제선 노선 재개 및 증편으로 에어부산의 항공 노선 수익 확대가 예상되어 긍정적 영향을 줄 수 있음. 특히 일본, 동남아, 몽골 등 인기 지역 노선의 조기 복원은 매출 증가로 직결될 가능성이 높음.\",\"negative_stocks\":[],\"negative_keywords\":[],\"negative_reasons\":\"\"}<|eot_id|>'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_ids = [token_id for token_id in batch['labels'][0].tolist() if token_id != -100]  # 정답 토큰(-100 제외)만 추출\n",
    "text = tokenizer.decode(label_ids)  # 정답 토큰을 문장열로 디코딩\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ecd38771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|begin_of_text|>', '<|start_header_id|>', 'system', '<|end_header_id|>', '\\n', '당', '신', '은', ' 금', '융', '/', '경제', ' 뉴', '스의', ' 핵', '심', '내', '용', '을', ' 요', '약', '해', ' 설명', '하고', ',', ' \\n', '특', '정', ' 상', '장', ' 종', '목', '에', ' 미', '치는', ' �', '�', '정', '/', '부', '정', ' 영향', '여', '부', ',', ' 이유', ',', ' 근', '거', '를', ' 분석', '하는', ' 금', '융', '/', '경제', ' 분석', ' 전문', '가', '입니다', '.\\n\\n', '다', '음', ' 출력', '지', '시', '사항', '을', ' 지', '켜', '주세요', '.\\n', '1', '.', ' 뉴', '스', '와', ' 종', '목', '간', '의', ' 연', '관', '성을', ' 발견', '할', ' 수', ' 없', '다면', ':\\n', '   ', ' -', ' stock', '_related', '를', ' False', '로', ' 작성', '하세요', '.\\n', '   ', ' -', ' summary', '에', ' 뉴', '스의', ' 요', '약', '을', ' 작성', '하세요', '.\\n', '2', '.', ' 뉴', '스', '와', ' 종', '목', '간', '의', ' 연', '관', '성을', ' 발견', '했다', '면', ':\\n', '   ', ' -', ' stock', '_related', '를', ' True', '로', ' 작성', '하세요', '.\\n', '   ', ' -', ' summary', '에', ' 뉴', '스의', ' 요', '약', '을', ' 작성', '하세요', '.\\n', '   ', ' -', ' �', '�', '정', '영', '향', '이', ' 예', '상', '되는', ' 종', '목', '이', ' 있다', '면', ',', ' positive', '_st', 'ocks', ',', ' positive', '_keywords', ',', ' positive', '_reason', 's', '를', ' 작성', '하세요', '.\\n', '   ', ' -', ' 부', '정', '영', '향', '이', ' 예', '상', '되는', ' 종', '목', '이', ' 있다', '면', ',', ' negative', '_st', 'ocks', ',', ' negative', '_keywords', ',', ' negative', '_reason', 's', '를', ' 작성', '하세요', '.\\n', '   ', ' -', ' 값', '이', ' 없는', ' 경우', ' 빈', ' 문자', '열', \"('\", \"'),\", ' 빈', ' 리스트', '([]', ')', '로', ' 작성', '하세요', '.', '<|eot_id|>', '<|start_header_id|>', 'user', '<|end_header_id|>', '\\n', '에', '어', '부', '산', ' 울', '란', '바', '토', '르', '·', '오', '사', '카', ' 노', '선', ' 재', '개', '\\n', '에', '어', '부', '산', '이', ' 김', '해', '국', '제', '공', '항', '에서', ' 출', '발', '하는', ' 몽', '골', ' 울', '란', '바', '토', '르', '와', ' 일본', ' 오', '사', '카', ' 노', '선', ' 운', '항', '을', ' 각각', ' 주', ' ', '2', '회', ' 일정', '으로', ' 코로나', '19', ' 팬', '데', '�', '�', ' 사', '태', ' 이후', ' ', '28', '개월', ' 만', '에', ' 재', '개', '한다', '고', ' ', '1', '일', ' 밝', '�', '�다', '.', ' 부산', ' 울', '란', '바', '토', '르', ' 노', '선', '은', ' 김', '해', '국', '제', '공', '항', '에서', ' 오', '전', ' ', '8', '시', ' ', '25', '분', '에', ' 출', '발', '해', ' 현', '지', ' 공', '항', '에', ' 오', '전', ' ', '11', '시', ' ', '40', '분', ' 도', '착', '하고', ' 귀', '국', '편', '은', ' 오후', ' ', '1', '시에', ' 출', '발', '해', ' 김', '해', '공', '항', '에', ' 오후', ' ', '5', '시', ' ', '30', '분', ' 도', '착', '하는', ' 일정', '으로', ' 주', ' ', '2', '회', ' 운', '항', '한다', '.', ' 몽', '골', '은', ' 입', '국', ' 시', ' 코로나', '19', ' 검', '사', '와', ' 백', '신', ' 접', '종', ' 여', '부', '를', ' 확인', '하지', ' 않', '아', ' 자유', '롭', '게', ' 여행', '이', ' 가능한', ' 국가', '다', '.', ' 부산', ' 오', '사', '카', ' 노', '선', '은', ' 김', '해', '공', '항', '에서', ' 오', '전', ' ', '8', '시', ' ', '35', '분', '에', ' 출', '발', '해', ' 간', '사이', '공', '항', '에', ' 오', '전', ' ', '10', '시', ' 도', '착', ' 귀', '국', '편', '은', ' 간', '사이', '공', '항', '에서', ' 낮', ' ', '12', '시에', ' 출', '발', '해', ' 김', '해', '공', '항', '에', ' 오후', ' ', '1', '시', ' ', '30', '분', ' 도', '착', '하는', ' 일정', '으로', ' 주', ' ', '2', '회', ' 운', '항', '한다', '.', ' 에', '어', '부', '산', ' 관계', '자는', ' 이번', ' 증', '편', '은', ' 무', '비', '자', ' 입', '국', '이', ' 복', '원', '�', '�', '을', ' 때', ' 폭', '발', '하는', ' 여행', ' 수', '요', '를', ' 선', '점', '하기', ' 위한', ' 선', '제', '적', ' 조', '치', ' 라', '고', ' 밝', '�', '�다', '.', ' 이', '�', '�', '에도', ' 에', '어', '부', '산', '은', ' 이', '달', ' 중', ' 부산', '에서', ' 출', '발', '하는', ' 코', '타', '키', '나', '발', '루', ' 나', '트', '랑', ' 세', '부', ' 노', '선', ' 등을', ' 운', '항', '할', ' 예정', '이다', '.', ' 인천', '국', '제', '공', '항', '에서는', ' 이', '달', ' ', '14', '일', '부터', ' 순', '차', '적으로', ' 다', '�', '�', ' 방', '�', '�', ' 후', '쿠', '오', '카', ' 등', ' 노', '선을', ' 신규', ' 취', '항', '한다', '.', '<|eot_id|>', '<|start_header_id|>', 'assistant', '<|end_header_id|>', '\\n', '{\"', 'stock', '_related', '\":', 'true', ',\"', 'summary', '\":\"', '에', '어', '부', '산', '이', ' 코로나', '19', ' 팬', '데', '�', '�', ' 이후', ' ', '28', '개월', ' 만', '에', ' 김', '해', '국', '제', '공', '항', '에서', ' 몽', '골', ' 울', '란', '바', '토', '르', '와', ' 일본', ' 오', '사', '카', ' 노', '선', ' 운', '항', '을', ' 주', ' ', '2', '회', ' 재', '개', '하며', ',', ' 곧', ' 코', '타', '키', '나', '발', '루', ',', ' 나', '트', '랑', ',', ' 세', '부', ' 등', ' 동', '남', '아', ' 노', '선', '도', ' 운', '항', '을', ' 시작', '한다', '.', ' 인천', '공', '항', '발', ' 국제', ' 노', '선', ' 확', '대', '도', ' 예', '고', '했다', '.', ' 몽', '골', '의', ' 무', '격', '리', '·', '무', '백', '신', ' 입', '국', ' 정책', ',', ' 일본', ' 오', '사', '카', ' 노', '선', ' 등', ' 해외', '여', '행', ' 수', '요', ' 증', '대를', ' 선', '제', '적으로', ' 겨', '냥', '한', ' 조', '치', '이다', '.\",\"', 'positive', '_st', 'ocks', '\":[\"', '에', '어', '부', '산', '\"],\"', 'positive', '_keywords', '\":[\"', '국', '제', '선', ' 운', '항', ' 재', '개', '\",\"', '해', '외', '여', '행', ' 수', '요', ' 확', '대', '\",\"', '동', '남', '아', '·', '일본', ' 노', '선', ' 확', '대', '\",\"', '코', '로나', ' 이후', ' 노', '선', ' 복', '원', '\"],\"', 'positive', '_reason', 's', '\":\"', '�', '�', '데', '�', '�', ' 이후', ' �', '�', '�', '�', '렸', '던', ' 해외', '여', '행', ' 수', '요', ' 급', '증', ' 기', '대', ',', ' 국제', '선', ' 노', '선', ' 재', '개', ' 및', ' 증', '편', '으로', ' 에', '어', '부', '산', '의', ' 항', '공', ' 노', '선', ' 수', '익', ' 확', '대', '가', ' 예', '상', '되어', ' �', '�', '정', '적', ' 영향을', ' 줄', ' 수', ' 있음', '.', ' 특히', ' 일본', ',', ' 동', '남', '아', ',', ' 몽', '골', ' 등', ' 인기', ' 지역', ' 노', '선', '의', ' 조', '기', ' 복', '원', '은', ' 매', '출', ' 증가', '로', ' 직', '결', '될', ' 가능', '성이', ' 높', '음', '.\",\"', 'negative', '_st', 'ocks', '\":[', '],\"', 'negative', '_keywords', '\":[', '],\"', 'negative', '_reason', 's', '\":\"\"', '}', '<|eot_id|>']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(batch['input_ids'][0].tolist())  # 토큰 ID를 토큰 문자열로 변환\n",
    "\n",
    "text_tokens = []  # 토큰 ID를 1개씩 디코딩한 문자열을 담을 리스트\n",
    "for i, token_id in enumerate(batch['input_ids'][0].tolist()):  # 토큰 ID를 순회\n",
    "    decoded_str = tokenizer.decode([token_id])  # 토큰 1개를 문자열로 디코딩\n",
    "    text_tokens.append(decoded_str)             # 디코딩 결과를 누적 저장\n",
    "\n",
    "print(text_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "39c12925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>117</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "      <th>128</th>\n",
       "      <th>129</th>\n",
       "      <th>130</th>\n",
       "      <th>131</th>\n",
       "      <th>132</th>\n",
       "      <th>133</th>\n",
       "      <th>134</th>\n",
       "      <th>135</th>\n",
       "      <th>136</th>\n",
       "      <th>137</th>\n",
       "      <th>138</th>\n",
       "      <th>139</th>\n",
       "      <th>140</th>\n",
       "      <th>141</th>\n",
       "      <th>142</th>\n",
       "      <th>143</th>\n",
       "      <th>144</th>\n",
       "      <th>145</th>\n",
       "      <th>146</th>\n",
       "      <th>147</th>\n",
       "      <th>148</th>\n",
       "      <th>149</th>\n",
       "      <th>150</th>\n",
       "      <th>151</th>\n",
       "      <th>152</th>\n",
       "      <th>153</th>\n",
       "      <th>154</th>\n",
       "      <th>155</th>\n",
       "      <th>156</th>\n",
       "      <th>157</th>\n",
       "      <th>158</th>\n",
       "      <th>159</th>\n",
       "      <th>160</th>\n",
       "      <th>161</th>\n",
       "      <th>162</th>\n",
       "      <th>163</th>\n",
       "      <th>164</th>\n",
       "      <th>165</th>\n",
       "      <th>166</th>\n",
       "      <th>167</th>\n",
       "      <th>168</th>\n",
       "      <th>169</th>\n",
       "      <th>170</th>\n",
       "      <th>171</th>\n",
       "      <th>172</th>\n",
       "      <th>173</th>\n",
       "      <th>174</th>\n",
       "      <th>175</th>\n",
       "      <th>176</th>\n",
       "      <th>177</th>\n",
       "      <th>178</th>\n",
       "      <th>179</th>\n",
       "      <th>180</th>\n",
       "      <th>181</th>\n",
       "      <th>182</th>\n",
       "      <th>183</th>\n",
       "      <th>184</th>\n",
       "      <th>185</th>\n",
       "      <th>186</th>\n",
       "      <th>187</th>\n",
       "      <th>188</th>\n",
       "      <th>189</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "      <th>200</th>\n",
       "      <th>201</th>\n",
       "      <th>202</th>\n",
       "      <th>203</th>\n",
       "      <th>204</th>\n",
       "      <th>205</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "      <th>216</th>\n",
       "      <th>217</th>\n",
       "      <th>218</th>\n",
       "      <th>219</th>\n",
       "      <th>220</th>\n",
       "      <th>221</th>\n",
       "      <th>222</th>\n",
       "      <th>223</th>\n",
       "      <th>224</th>\n",
       "      <th>225</th>\n",
       "      <th>226</th>\n",
       "      <th>227</th>\n",
       "      <th>228</th>\n",
       "      <th>229</th>\n",
       "      <th>230</th>\n",
       "      <th>231</th>\n",
       "      <th>232</th>\n",
       "      <th>233</th>\n",
       "      <th>234</th>\n",
       "      <th>235</th>\n",
       "      <th>236</th>\n",
       "      <th>237</th>\n",
       "      <th>238</th>\n",
       "      <th>239</th>\n",
       "      <th>240</th>\n",
       "      <th>241</th>\n",
       "      <th>242</th>\n",
       "      <th>243</th>\n",
       "      <th>244</th>\n",
       "      <th>245</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "      <th>256</th>\n",
       "      <th>257</th>\n",
       "      <th>258</th>\n",
       "      <th>259</th>\n",
       "      <th>260</th>\n",
       "      <th>261</th>\n",
       "      <th>262</th>\n",
       "      <th>263</th>\n",
       "      <th>264</th>\n",
       "      <th>265</th>\n",
       "      <th>266</th>\n",
       "      <th>267</th>\n",
       "      <th>268</th>\n",
       "      <th>269</th>\n",
       "      <th>270</th>\n",
       "      <th>271</th>\n",
       "      <th>272</th>\n",
       "      <th>273</th>\n",
       "      <th>274</th>\n",
       "      <th>275</th>\n",
       "      <th>276</th>\n",
       "      <th>277</th>\n",
       "      <th>278</th>\n",
       "      <th>279</th>\n",
       "      <th>280</th>\n",
       "      <th>281</th>\n",
       "      <th>282</th>\n",
       "      <th>283</th>\n",
       "      <th>284</th>\n",
       "      <th>285</th>\n",
       "      <th>286</th>\n",
       "      <th>287</th>\n",
       "      <th>288</th>\n",
       "      <th>289</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "      <th>300</th>\n",
       "      <th>301</th>\n",
       "      <th>302</th>\n",
       "      <th>303</th>\n",
       "      <th>304</th>\n",
       "      <th>305</th>\n",
       "      <th>306</th>\n",
       "      <th>307</th>\n",
       "      <th>308</th>\n",
       "      <th>309</th>\n",
       "      <th>310</th>\n",
       "      <th>311</th>\n",
       "      <th>312</th>\n",
       "      <th>313</th>\n",
       "      <th>314</th>\n",
       "      <th>315</th>\n",
       "      <th>316</th>\n",
       "      <th>317</th>\n",
       "      <th>318</th>\n",
       "      <th>319</th>\n",
       "      <th>320</th>\n",
       "      <th>321</th>\n",
       "      <th>322</th>\n",
       "      <th>323</th>\n",
       "      <th>324</th>\n",
       "      <th>325</th>\n",
       "      <th>326</th>\n",
       "      <th>327</th>\n",
       "      <th>328</th>\n",
       "      <th>329</th>\n",
       "      <th>330</th>\n",
       "      <th>331</th>\n",
       "      <th>332</th>\n",
       "      <th>333</th>\n",
       "      <th>334</th>\n",
       "      <th>335</th>\n",
       "      <th>336</th>\n",
       "      <th>337</th>\n",
       "      <th>338</th>\n",
       "      <th>339</th>\n",
       "      <th>340</th>\n",
       "      <th>341</th>\n",
       "      <th>342</th>\n",
       "      <th>343</th>\n",
       "      <th>344</th>\n",
       "      <th>345</th>\n",
       "      <th>346</th>\n",
       "      <th>347</th>\n",
       "      <th>348</th>\n",
       "      <th>349</th>\n",
       "      <th>350</th>\n",
       "      <th>351</th>\n",
       "      <th>352</th>\n",
       "      <th>353</th>\n",
       "      <th>354</th>\n",
       "      <th>355</th>\n",
       "      <th>356</th>\n",
       "      <th>357</th>\n",
       "      <th>358</th>\n",
       "      <th>359</th>\n",
       "      <th>360</th>\n",
       "      <th>361</th>\n",
       "      <th>362</th>\n",
       "      <th>363</th>\n",
       "      <th>364</th>\n",
       "      <th>365</th>\n",
       "      <th>366</th>\n",
       "      <th>367</th>\n",
       "      <th>368</th>\n",
       "      <th>369</th>\n",
       "      <th>370</th>\n",
       "      <th>371</th>\n",
       "      <th>372</th>\n",
       "      <th>373</th>\n",
       "      <th>374</th>\n",
       "      <th>375</th>\n",
       "      <th>376</th>\n",
       "      <th>377</th>\n",
       "      <th>378</th>\n",
       "      <th>379</th>\n",
       "      <th>380</th>\n",
       "      <th>381</th>\n",
       "      <th>382</th>\n",
       "      <th>383</th>\n",
       "      <th>384</th>\n",
       "      <th>385</th>\n",
       "      <th>386</th>\n",
       "      <th>387</th>\n",
       "      <th>388</th>\n",
       "      <th>389</th>\n",
       "      <th>390</th>\n",
       "      <th>391</th>\n",
       "      <th>392</th>\n",
       "      <th>393</th>\n",
       "      <th>394</th>\n",
       "      <th>395</th>\n",
       "      <th>396</th>\n",
       "      <th>397</th>\n",
       "      <th>398</th>\n",
       "      <th>399</th>\n",
       "      <th>400</th>\n",
       "      <th>401</th>\n",
       "      <th>402</th>\n",
       "      <th>403</th>\n",
       "      <th>404</th>\n",
       "      <th>405</th>\n",
       "      <th>406</th>\n",
       "      <th>407</th>\n",
       "      <th>408</th>\n",
       "      <th>409</th>\n",
       "      <th>410</th>\n",
       "      <th>411</th>\n",
       "      <th>412</th>\n",
       "      <th>413</th>\n",
       "      <th>414</th>\n",
       "      <th>415</th>\n",
       "      <th>416</th>\n",
       "      <th>417</th>\n",
       "      <th>418</th>\n",
       "      <th>419</th>\n",
       "      <th>420</th>\n",
       "      <th>421</th>\n",
       "      <th>422</th>\n",
       "      <th>423</th>\n",
       "      <th>424</th>\n",
       "      <th>425</th>\n",
       "      <th>426</th>\n",
       "      <th>427</th>\n",
       "      <th>428</th>\n",
       "      <th>429</th>\n",
       "      <th>430</th>\n",
       "      <th>431</th>\n",
       "      <th>432</th>\n",
       "      <th>433</th>\n",
       "      <th>434</th>\n",
       "      <th>435</th>\n",
       "      <th>436</th>\n",
       "      <th>437</th>\n",
       "      <th>438</th>\n",
       "      <th>439</th>\n",
       "      <th>440</th>\n",
       "      <th>441</th>\n",
       "      <th>442</th>\n",
       "      <th>443</th>\n",
       "      <th>444</th>\n",
       "      <th>445</th>\n",
       "      <th>446</th>\n",
       "      <th>447</th>\n",
       "      <th>448</th>\n",
       "      <th>449</th>\n",
       "      <th>450</th>\n",
       "      <th>451</th>\n",
       "      <th>452</th>\n",
       "      <th>453</th>\n",
       "      <th>454</th>\n",
       "      <th>455</th>\n",
       "      <th>456</th>\n",
       "      <th>457</th>\n",
       "      <th>458</th>\n",
       "      <th>459</th>\n",
       "      <th>460</th>\n",
       "      <th>461</th>\n",
       "      <th>462</th>\n",
       "      <th>463</th>\n",
       "      <th>464</th>\n",
       "      <th>465</th>\n",
       "      <th>466</th>\n",
       "      <th>467</th>\n",
       "      <th>468</th>\n",
       "      <th>469</th>\n",
       "      <th>470</th>\n",
       "      <th>471</th>\n",
       "      <th>472</th>\n",
       "      <th>473</th>\n",
       "      <th>474</th>\n",
       "      <th>475</th>\n",
       "      <th>476</th>\n",
       "      <th>477</th>\n",
       "      <th>478</th>\n",
       "      <th>479</th>\n",
       "      <th>480</th>\n",
       "      <th>481</th>\n",
       "      <th>482</th>\n",
       "      <th>483</th>\n",
       "      <th>484</th>\n",
       "      <th>485</th>\n",
       "      <th>486</th>\n",
       "      <th>487</th>\n",
       "      <th>488</th>\n",
       "      <th>489</th>\n",
       "      <th>490</th>\n",
       "      <th>491</th>\n",
       "      <th>492</th>\n",
       "      <th>493</th>\n",
       "      <th>494</th>\n",
       "      <th>495</th>\n",
       "      <th>496</th>\n",
       "      <th>497</th>\n",
       "      <th>498</th>\n",
       "      <th>499</th>\n",
       "      <th>500</th>\n",
       "      <th>501</th>\n",
       "      <th>502</th>\n",
       "      <th>503</th>\n",
       "      <th>504</th>\n",
       "      <th>505</th>\n",
       "      <th>506</th>\n",
       "      <th>507</th>\n",
       "      <th>508</th>\n",
       "      <th>509</th>\n",
       "      <th>510</th>\n",
       "      <th>511</th>\n",
       "      <th>512</th>\n",
       "      <th>513</th>\n",
       "      <th>514</th>\n",
       "      <th>515</th>\n",
       "      <th>516</th>\n",
       "      <th>517</th>\n",
       "      <th>518</th>\n",
       "      <th>519</th>\n",
       "      <th>520</th>\n",
       "      <th>521</th>\n",
       "      <th>522</th>\n",
       "      <th>523</th>\n",
       "      <th>524</th>\n",
       "      <th>525</th>\n",
       "      <th>526</th>\n",
       "      <th>527</th>\n",
       "      <th>528</th>\n",
       "      <th>529</th>\n",
       "      <th>530</th>\n",
       "      <th>531</th>\n",
       "      <th>532</th>\n",
       "      <th>533</th>\n",
       "      <th>534</th>\n",
       "      <th>535</th>\n",
       "      <th>536</th>\n",
       "      <th>537</th>\n",
       "      <th>538</th>\n",
       "      <th>539</th>\n",
       "      <th>540</th>\n",
       "      <th>541</th>\n",
       "      <th>542</th>\n",
       "      <th>543</th>\n",
       "      <th>544</th>\n",
       "      <th>545</th>\n",
       "      <th>546</th>\n",
       "      <th>547</th>\n",
       "      <th>548</th>\n",
       "      <th>549</th>\n",
       "      <th>550</th>\n",
       "      <th>551</th>\n",
       "      <th>552</th>\n",
       "      <th>553</th>\n",
       "      <th>554</th>\n",
       "      <th>555</th>\n",
       "      <th>556</th>\n",
       "      <th>557</th>\n",
       "      <th>558</th>\n",
       "      <th>559</th>\n",
       "      <th>560</th>\n",
       "      <th>561</th>\n",
       "      <th>562</th>\n",
       "      <th>563</th>\n",
       "      <th>564</th>\n",
       "      <th>565</th>\n",
       "      <th>566</th>\n",
       "      <th>567</th>\n",
       "      <th>568</th>\n",
       "      <th>569</th>\n",
       "      <th>570</th>\n",
       "      <th>571</th>\n",
       "      <th>572</th>\n",
       "      <th>573</th>\n",
       "      <th>574</th>\n",
       "      <th>575</th>\n",
       "      <th>576</th>\n",
       "      <th>577</th>\n",
       "      <th>578</th>\n",
       "      <th>579</th>\n",
       "      <th>580</th>\n",
       "      <th>581</th>\n",
       "      <th>582</th>\n",
       "      <th>583</th>\n",
       "      <th>584</th>\n",
       "      <th>585</th>\n",
       "      <th>586</th>\n",
       "      <th>587</th>\n",
       "      <th>588</th>\n",
       "      <th>589</th>\n",
       "      <th>590</th>\n",
       "      <th>591</th>\n",
       "      <th>592</th>\n",
       "      <th>593</th>\n",
       "      <th>594</th>\n",
       "      <th>595</th>\n",
       "      <th>596</th>\n",
       "      <th>597</th>\n",
       "      <th>598</th>\n",
       "      <th>599</th>\n",
       "      <th>600</th>\n",
       "      <th>601</th>\n",
       "      <th>602</th>\n",
       "      <th>603</th>\n",
       "      <th>604</th>\n",
       "      <th>605</th>\n",
       "      <th>606</th>\n",
       "      <th>607</th>\n",
       "      <th>608</th>\n",
       "      <th>609</th>\n",
       "      <th>610</th>\n",
       "      <th>611</th>\n",
       "      <th>612</th>\n",
       "      <th>613</th>\n",
       "      <th>614</th>\n",
       "      <th>615</th>\n",
       "      <th>616</th>\n",
       "      <th>617</th>\n",
       "      <th>618</th>\n",
       "      <th>619</th>\n",
       "      <th>620</th>\n",
       "      <th>621</th>\n",
       "      <th>622</th>\n",
       "      <th>623</th>\n",
       "      <th>624</th>\n",
       "      <th>625</th>\n",
       "      <th>626</th>\n",
       "      <th>627</th>\n",
       "      <th>628</th>\n",
       "      <th>629</th>\n",
       "      <th>630</th>\n",
       "      <th>631</th>\n",
       "      <th>632</th>\n",
       "      <th>633</th>\n",
       "      <th>634</th>\n",
       "      <th>635</th>\n",
       "      <th>636</th>\n",
       "      <th>637</th>\n",
       "      <th>638</th>\n",
       "      <th>639</th>\n",
       "      <th>640</th>\n",
       "      <th>641</th>\n",
       "      <th>642</th>\n",
       "      <th>643</th>\n",
       "      <th>644</th>\n",
       "      <th>645</th>\n",
       "      <th>646</th>\n",
       "      <th>647</th>\n",
       "      <th>648</th>\n",
       "      <th>649</th>\n",
       "      <th>650</th>\n",
       "      <th>651</th>\n",
       "      <th>652</th>\n",
       "      <th>653</th>\n",
       "      <th>654</th>\n",
       "      <th>655</th>\n",
       "      <th>656</th>\n",
       "      <th>657</th>\n",
       "      <th>658</th>\n",
       "      <th>659</th>\n",
       "      <th>660</th>\n",
       "      <th>661</th>\n",
       "      <th>662</th>\n",
       "      <th>663</th>\n",
       "      <th>664</th>\n",
       "      <th>665</th>\n",
       "      <th>666</th>\n",
       "      <th>667</th>\n",
       "      <th>668</th>\n",
       "      <th>669</th>\n",
       "      <th>670</th>\n",
       "      <th>671</th>\n",
       "      <th>672</th>\n",
       "      <th>673</th>\n",
       "      <th>674</th>\n",
       "      <th>675</th>\n",
       "      <th>676</th>\n",
       "      <th>677</th>\n",
       "      <th>678</th>\n",
       "      <th>679</th>\n",
       "      <th>680</th>\n",
       "      <th>681</th>\n",
       "      <th>682</th>\n",
       "      <th>683</th>\n",
       "      <th>684</th>\n",
       "      <th>685</th>\n",
       "      <th>686</th>\n",
       "      <th>687</th>\n",
       "      <th>688</th>\n",
       "      <th>689</th>\n",
       "      <th>690</th>\n",
       "      <th>691</th>\n",
       "      <th>692</th>\n",
       "      <th>693</th>\n",
       "      <th>694</th>\n",
       "      <th>695</th>\n",
       "      <th>696</th>\n",
       "      <th>697</th>\n",
       "      <th>698</th>\n",
       "      <th>699</th>\n",
       "      <th>700</th>\n",
       "      <th>701</th>\n",
       "      <th>702</th>\n",
       "      <th>703</th>\n",
       "      <th>704</th>\n",
       "      <th>705</th>\n",
       "      <th>706</th>\n",
       "      <th>707</th>\n",
       "      <th>708</th>\n",
       "      <th>709</th>\n",
       "      <th>710</th>\n",
       "      <th>711</th>\n",
       "      <th>712</th>\n",
       "      <th>713</th>\n",
       "      <th>714</th>\n",
       "      <th>715</th>\n",
       "      <th>716</th>\n",
       "      <th>717</th>\n",
       "      <th>718</th>\n",
       "      <th>719</th>\n",
       "      <th>720</th>\n",
       "      <th>721</th>\n",
       "      <th>722</th>\n",
       "      <th>723</th>\n",
       "      <th>724</th>\n",
       "      <th>725</th>\n",
       "      <th>726</th>\n",
       "      <th>727</th>\n",
       "      <th>728</th>\n",
       "      <th>729</th>\n",
       "      <th>730</th>\n",
       "      <th>731</th>\n",
       "      <th>732</th>\n",
       "      <th>733</th>\n",
       "      <th>734</th>\n",
       "      <th>735</th>\n",
       "      <th>736</th>\n",
       "      <th>737</th>\n",
       "      <th>738</th>\n",
       "      <th>739</th>\n",
       "      <th>740</th>\n",
       "      <th>741</th>\n",
       "      <th>742</th>\n",
       "      <th>743</th>\n",
       "      <th>744</th>\n",
       "      <th>745</th>\n",
       "      <th>746</th>\n",
       "      <th>747</th>\n",
       "      <th>748</th>\n",
       "      <th>749</th>\n",
       "      <th>750</th>\n",
       "      <th>751</th>\n",
       "      <th>752</th>\n",
       "      <th>753</th>\n",
       "      <th>754</th>\n",
       "      <th>755</th>\n",
       "      <th>756</th>\n",
       "      <th>757</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "      <th>768</th>\n",
       "      <th>769</th>\n",
       "      <th>770</th>\n",
       "      <th>771</th>\n",
       "      <th>772</th>\n",
       "      <th>773</th>\n",
       "      <th>774</th>\n",
       "      <th>775</th>\n",
       "      <th>776</th>\n",
       "      <th>777</th>\n",
       "      <th>778</th>\n",
       "      <th>779</th>\n",
       "      <th>780</th>\n",
       "      <th>781</th>\n",
       "      <th>782</th>\n",
       "      <th>783</th>\n",
       "      <th>784</th>\n",
       "      <th>785</th>\n",
       "      <th>786</th>\n",
       "      <th>787</th>\n",
       "      <th>788</th>\n",
       "      <th>789</th>\n",
       "      <th>790</th>\n",
       "      <th>791</th>\n",
       "      <th>792</th>\n",
       "      <th>793</th>\n",
       "      <th>794</th>\n",
       "      <th>795</th>\n",
       "      <th>796</th>\n",
       "      <th>797</th>\n",
       "      <th>798</th>\n",
       "      <th>799</th>\n",
       "      <th>800</th>\n",
       "      <th>801</th>\n",
       "      <th>802</th>\n",
       "      <th>803</th>\n",
       "      <th>804</th>\n",
       "      <th>805</th>\n",
       "      <th>806</th>\n",
       "      <th>807</th>\n",
       "      <th>808</th>\n",
       "      <th>809</th>\n",
       "      <th>810</th>\n",
       "      <th>811</th>\n",
       "      <th>812</th>\n",
       "      <th>813</th>\n",
       "      <th>814</th>\n",
       "      <th>815</th>\n",
       "      <th>816</th>\n",
       "      <th>817</th>\n",
       "      <th>818</th>\n",
       "      <th>819</th>\n",
       "      <th>820</th>\n",
       "      <th>821</th>\n",
       "      <th>822</th>\n",
       "      <th>823</th>\n",
       "      <th>824</th>\n",
       "      <th>825</th>\n",
       "      <th>826</th>\n",
       "      <th>827</th>\n",
       "      <th>828</th>\n",
       "      <th>829</th>\n",
       "      <th>830</th>\n",
       "      <th>831</th>\n",
       "      <th>832</th>\n",
       "      <th>833</th>\n",
       "      <th>834</th>\n",
       "      <th>835</th>\n",
       "      <th>836</th>\n",
       "      <th>837</th>\n",
       "      <th>838</th>\n",
       "      <th>839</th>\n",
       "      <th>840</th>\n",
       "      <th>841</th>\n",
       "      <th>842</th>\n",
       "      <th>843</th>\n",
       "      <th>844</th>\n",
       "      <th>845</th>\n",
       "      <th>846</th>\n",
       "      <th>847</th>\n",
       "      <th>848</th>\n",
       "      <th>849</th>\n",
       "      <th>850</th>\n",
       "      <th>851</th>\n",
       "      <th>852</th>\n",
       "      <th>853</th>\n",
       "      <th>854</th>\n",
       "      <th>855</th>\n",
       "      <th>856</th>\n",
       "      <th>857</th>\n",
       "      <th>858</th>\n",
       "      <th>859</th>\n",
       "      <th>860</th>\n",
       "      <th>861</th>\n",
       "      <th>862</th>\n",
       "      <th>863</th>\n",
       "      <th>864</th>\n",
       "      <th>865</th>\n",
       "      <th>866</th>\n",
       "      <th>867</th>\n",
       "      <th>868</th>\n",
       "      <th>869</th>\n",
       "      <th>870</th>\n",
       "      <th>871</th>\n",
       "      <th>872</th>\n",
       "      <th>873</th>\n",
       "      <th>874</th>\n",
       "      <th>875</th>\n",
       "      <th>876</th>\n",
       "      <th>877</th>\n",
       "      <th>878</th>\n",
       "      <th>879</th>\n",
       "      <th>880</th>\n",
       "      <th>881</th>\n",
       "      <th>882</th>\n",
       "      <th>883</th>\n",
       "      <th>884</th>\n",
       "      <th>885</th>\n",
       "      <th>886</th>\n",
       "      <th>887</th>\n",
       "      <th>888</th>\n",
       "      <th>889</th>\n",
       "      <th>890</th>\n",
       "      <th>891</th>\n",
       "      <th>892</th>\n",
       "      <th>893</th>\n",
       "      <th>894</th>\n",
       "      <th>895</th>\n",
       "      <th>896</th>\n",
       "      <th>897</th>\n",
       "      <th>898</th>\n",
       "      <th>899</th>\n",
       "      <th>900</th>\n",
       "      <th>901</th>\n",
       "      <th>902</th>\n",
       "      <th>903</th>\n",
       "      <th>904</th>\n",
       "      <th>905</th>\n",
       "      <th>906</th>\n",
       "      <th>907</th>\n",
       "      <th>908</th>\n",
       "      <th>909</th>\n",
       "      <th>910</th>\n",
       "      <th>911</th>\n",
       "      <th>912</th>\n",
       "      <th>913</th>\n",
       "      <th>914</th>\n",
       "      <th>915</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <td>&lt;|begin_of_text|&gt;</td>\n",
       "      <td>&lt;|start_header_id|&gt;</td>\n",
       "      <td>system</td>\n",
       "      <td>&lt;|end_header_id|&gt;</td>\n",
       "      <td>\\n</td>\n",
       "      <td>당</td>\n",
       "      <td>신</td>\n",
       "      <td>은</td>\n",
       "      <td>금</td>\n",
       "      <td>융</td>\n",
       "      <td>/</td>\n",
       "      <td>경제</td>\n",
       "      <td>뉴</td>\n",
       "      <td>스의</td>\n",
       "      <td>핵</td>\n",
       "      <td>심</td>\n",
       "      <td>내</td>\n",
       "      <td>용</td>\n",
       "      <td>을</td>\n",
       "      <td>요</td>\n",
       "      <td>약</td>\n",
       "      <td>해</td>\n",
       "      <td>설명</td>\n",
       "      <td>하고</td>\n",
       "      <td>,</td>\n",
       "      <td>\\n</td>\n",
       "      <td>특</td>\n",
       "      <td>정</td>\n",
       "      <td>상</td>\n",
       "      <td>장</td>\n",
       "      <td>종</td>\n",
       "      <td>목</td>\n",
       "      <td>에</td>\n",
       "      <td>미</td>\n",
       "      <td>치는</td>\n",
       "      <td>�</td>\n",
       "      <td>�</td>\n",
       "      <td>정</td>\n",
       "      <td>/</td>\n",
       "      <td>부</td>\n",
       "      <td>정</td>\n",
       "      <td>영향</td>\n",
       "      <td>여</td>\n",
       "      <td>부</td>\n",
       "      <td>,</td>\n",
       "      <td>이유</td>\n",
       "      <td>,</td>\n",
       "      <td>근</td>\n",
       "      <td>거</td>\n",
       "      <td>를</td>\n",
       "      <td>분석</td>\n",
       "      <td>하는</td>\n",
       "      <td>금</td>\n",
       "      <td>융</td>\n",
       "      <td>/</td>\n",
       "      <td>경제</td>\n",
       "      <td>분석</td>\n",
       "      <td>전문</td>\n",
       "      <td>가</td>\n",
       "      <td>입니다</td>\n",
       "      <td>.\\n\\n</td>\n",
       "      <td>다</td>\n",
       "      <td>음</td>\n",
       "      <td>출력</td>\n",
       "      <td>지</td>\n",
       "      <td>시</td>\n",
       "      <td>사항</td>\n",
       "      <td>을</td>\n",
       "      <td>지</td>\n",
       "      <td>켜</td>\n",
       "      <td>주세요</td>\n",
       "      <td>.\\n</td>\n",
       "      <td>1</td>\n",
       "      <td>.</td>\n",
       "      <td>뉴</td>\n",
       "      <td>스</td>\n",
       "      <td>와</td>\n",
       "      <td>종</td>\n",
       "      <td>목</td>\n",
       "      <td>간</td>\n",
       "      <td>의</td>\n",
       "      <td>연</td>\n",
       "      <td>관</td>\n",
       "      <td>성을</td>\n",
       "      <td>발견</td>\n",
       "      <td>할</td>\n",
       "      <td>수</td>\n",
       "      <td>없</td>\n",
       "      <td>다면</td>\n",
       "      <td>:\\n</td>\n",
       "      <td></td>\n",
       "      <td>-</td>\n",
       "      <td>stock</td>\n",
       "      <td>_related</td>\n",
       "      <td>를</td>\n",
       "      <td>False</td>\n",
       "      <td>로</td>\n",
       "      <td>작성</td>\n",
       "      <td>하세요</td>\n",
       "      <td>.\\n</td>\n",
       "      <td></td>\n",
       "      <td>-</td>\n",
       "      <td>summary</td>\n",
       "      <td>에</td>\n",
       "      <td>뉴</td>\n",
       "      <td>스의</td>\n",
       "      <td>요</td>\n",
       "      <td>약</td>\n",
       "      <td>을</td>\n",
       "      <td>작성</td>\n",
       "      <td>하세요</td>\n",
       "      <td>.\\n</td>\n",
       "      <td>2</td>\n",
       "      <td>.</td>\n",
       "      <td>뉴</td>\n",
       "      <td>스</td>\n",
       "      <td>와</td>\n",
       "      <td>종</td>\n",
       "      <td>목</td>\n",
       "      <td>간</td>\n",
       "      <td>의</td>\n",
       "      <td>연</td>\n",
       "      <td>관</td>\n",
       "      <td>성을</td>\n",
       "      <td>발견</td>\n",
       "      <td>했다</td>\n",
       "      <td>면</td>\n",
       "      <td>:\\n</td>\n",
       "      <td></td>\n",
       "      <td>-</td>\n",
       "      <td>stock</td>\n",
       "      <td>_related</td>\n",
       "      <td>를</td>\n",
       "      <td>True</td>\n",
       "      <td>로</td>\n",
       "      <td>작성</td>\n",
       "      <td>하세요</td>\n",
       "      <td>.\\n</td>\n",
       "      <td></td>\n",
       "      <td>-</td>\n",
       "      <td>summary</td>\n",
       "      <td>에</td>\n",
       "      <td>뉴</td>\n",
       "      <td>스의</td>\n",
       "      <td>요</td>\n",
       "      <td>약</td>\n",
       "      <td>을</td>\n",
       "      <td>작성</td>\n",
       "      <td>하세요</td>\n",
       "      <td>.\\n</td>\n",
       "      <td></td>\n",
       "      <td>-</td>\n",
       "      <td>�</td>\n",
       "      <td>�</td>\n",
       "      <td>정</td>\n",
       "      <td>영</td>\n",
       "      <td>향</td>\n",
       "      <td>이</td>\n",
       "      <td>예</td>\n",
       "      <td>상</td>\n",
       "      <td>되는</td>\n",
       "      <td>종</td>\n",
       "      <td>목</td>\n",
       "      <td>이</td>\n",
       "      <td>있다</td>\n",
       "      <td>면</td>\n",
       "      <td>,</td>\n",
       "      <td>positive</td>\n",
       "      <td>_st</td>\n",
       "      <td>ocks</td>\n",
       "      <td>,</td>\n",
       "      <td>positive</td>\n",
       "      <td>_keywords</td>\n",
       "      <td>,</td>\n",
       "      <td>positive</td>\n",
       "      <td>_reason</td>\n",
       "      <td>s</td>\n",
       "      <td>를</td>\n",
       "      <td>작성</td>\n",
       "      <td>하세요</td>\n",
       "      <td>.\\n</td>\n",
       "      <td></td>\n",
       "      <td>-</td>\n",
       "      <td>부</td>\n",
       "      <td>정</td>\n",
       "      <td>영</td>\n",
       "      <td>향</td>\n",
       "      <td>이</td>\n",
       "      <td>예</td>\n",
       "      <td>상</td>\n",
       "      <td>되는</td>\n",
       "      <td>종</td>\n",
       "      <td>목</td>\n",
       "      <td>이</td>\n",
       "      <td>있다</td>\n",
       "      <td>면</td>\n",
       "      <td>,</td>\n",
       "      <td>negative</td>\n",
       "      <td>_st</td>\n",
       "      <td>ocks</td>\n",
       "      <td>,</td>\n",
       "      <td>negative</td>\n",
       "      <td>_keywords</td>\n",
       "      <td>,</td>\n",
       "      <td>negative</td>\n",
       "      <td>_reason</td>\n",
       "      <td>s</td>\n",
       "      <td>를</td>\n",
       "      <td>작성</td>\n",
       "      <td>하세요</td>\n",
       "      <td>.\\n</td>\n",
       "      <td></td>\n",
       "      <td>-</td>\n",
       "      <td>값</td>\n",
       "      <td>이</td>\n",
       "      <td>없는</td>\n",
       "      <td>경우</td>\n",
       "      <td>빈</td>\n",
       "      <td>문자</td>\n",
       "      <td>열</td>\n",
       "      <td>('</td>\n",
       "      <td>'),</td>\n",
       "      <td>빈</td>\n",
       "      <td>리스트</td>\n",
       "      <td>([]</td>\n",
       "      <td>)</td>\n",
       "      <td>로</td>\n",
       "      <td>작성</td>\n",
       "      <td>하세요</td>\n",
       "      <td>.</td>\n",
       "      <td>&lt;|eot_id|&gt;</td>\n",
       "      <td>&lt;|start_header_id|&gt;</td>\n",
       "      <td>user</td>\n",
       "      <td>&lt;|end_header_id|&gt;</td>\n",
       "      <td>\\n</td>\n",
       "      <td>에</td>\n",
       "      <td>어</td>\n",
       "      <td>부</td>\n",
       "      <td>산</td>\n",
       "      <td>울</td>\n",
       "      <td>란</td>\n",
       "      <td>바</td>\n",
       "      <td>토</td>\n",
       "      <td>르</td>\n",
       "      <td>·</td>\n",
       "      <td>오</td>\n",
       "      <td>사</td>\n",
       "      <td>카</td>\n",
       "      <td>노</td>\n",
       "      <td>선</td>\n",
       "      <td>재</td>\n",
       "      <td>개</td>\n",
       "      <td>\\n</td>\n",
       "      <td>에</td>\n",
       "      <td>어</td>\n",
       "      <td>부</td>\n",
       "      <td>산</td>\n",
       "      <td>이</td>\n",
       "      <td>김</td>\n",
       "      <td>해</td>\n",
       "      <td>국</td>\n",
       "      <td>제</td>\n",
       "      <td>공</td>\n",
       "      <td>항</td>\n",
       "      <td>에서</td>\n",
       "      <td>출</td>\n",
       "      <td>발</td>\n",
       "      <td>하는</td>\n",
       "      <td>몽</td>\n",
       "      <td>골</td>\n",
       "      <td>울</td>\n",
       "      <td>란</td>\n",
       "      <td>바</td>\n",
       "      <td>토</td>\n",
       "      <td>르</td>\n",
       "      <td>와</td>\n",
       "      <td>일본</td>\n",
       "      <td>오</td>\n",
       "      <td>사</td>\n",
       "      <td>카</td>\n",
       "      <td>노</td>\n",
       "      <td>선</td>\n",
       "      <td>운</td>\n",
       "      <td>항</td>\n",
       "      <td>을</td>\n",
       "      <td>각각</td>\n",
       "      <td>주</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>회</td>\n",
       "      <td>일정</td>\n",
       "      <td>으로</td>\n",
       "      <td>코로나</td>\n",
       "      <td>19</td>\n",
       "      <td>팬</td>\n",
       "      <td>데</td>\n",
       "      <td>�</td>\n",
       "      <td>�</td>\n",
       "      <td>사</td>\n",
       "      <td>태</td>\n",
       "      <td>이후</td>\n",
       "      <td></td>\n",
       "      <td>28</td>\n",
       "      <td>개월</td>\n",
       "      <td>만</td>\n",
       "      <td>에</td>\n",
       "      <td>재</td>\n",
       "      <td>개</td>\n",
       "      <td>한다</td>\n",
       "      <td>고</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>일</td>\n",
       "      <td>밝</td>\n",
       "      <td>�</td>\n",
       "      <td>�다</td>\n",
       "      <td>.</td>\n",
       "      <td>부산</td>\n",
       "      <td>울</td>\n",
       "      <td>란</td>\n",
       "      <td>바</td>\n",
       "      <td>토</td>\n",
       "      <td>르</td>\n",
       "      <td>노</td>\n",
       "      <td>선</td>\n",
       "      <td>은</td>\n",
       "      <td>김</td>\n",
       "      <td>해</td>\n",
       "      <td>국</td>\n",
       "      <td>제</td>\n",
       "      <td>공</td>\n",
       "      <td>항</td>\n",
       "      <td>에서</td>\n",
       "      <td>오</td>\n",
       "      <td>전</td>\n",
       "      <td></td>\n",
       "      <td>8</td>\n",
       "      <td>시</td>\n",
       "      <td></td>\n",
       "      <td>25</td>\n",
       "      <td>분</td>\n",
       "      <td>에</td>\n",
       "      <td>출</td>\n",
       "      <td>발</td>\n",
       "      <td>해</td>\n",
       "      <td>현</td>\n",
       "      <td>지</td>\n",
       "      <td>공</td>\n",
       "      <td>항</td>\n",
       "      <td>에</td>\n",
       "      <td>오</td>\n",
       "      <td>전</td>\n",
       "      <td></td>\n",
       "      <td>11</td>\n",
       "      <td>시</td>\n",
       "      <td></td>\n",
       "      <td>40</td>\n",
       "      <td>분</td>\n",
       "      <td>도</td>\n",
       "      <td>착</td>\n",
       "      <td>하고</td>\n",
       "      <td>귀</td>\n",
       "      <td>국</td>\n",
       "      <td>편</td>\n",
       "      <td>은</td>\n",
       "      <td>오후</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>시에</td>\n",
       "      <td>출</td>\n",
       "      <td>발</td>\n",
       "      <td>해</td>\n",
       "      <td>김</td>\n",
       "      <td>해</td>\n",
       "      <td>공</td>\n",
       "      <td>항</td>\n",
       "      <td>에</td>\n",
       "      <td>오후</td>\n",
       "      <td></td>\n",
       "      <td>5</td>\n",
       "      <td>시</td>\n",
       "      <td></td>\n",
       "      <td>30</td>\n",
       "      <td>분</td>\n",
       "      <td>도</td>\n",
       "      <td>착</td>\n",
       "      <td>하는</td>\n",
       "      <td>일정</td>\n",
       "      <td>으로</td>\n",
       "      <td>주</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>회</td>\n",
       "      <td>운</td>\n",
       "      <td>항</td>\n",
       "      <td>한다</td>\n",
       "      <td>.</td>\n",
       "      <td>몽</td>\n",
       "      <td>골</td>\n",
       "      <td>은</td>\n",
       "      <td>입</td>\n",
       "      <td>국</td>\n",
       "      <td>시</td>\n",
       "      <td>코로나</td>\n",
       "      <td>19</td>\n",
       "      <td>검</td>\n",
       "      <td>사</td>\n",
       "      <td>와</td>\n",
       "      <td>백</td>\n",
       "      <td>신</td>\n",
       "      <td>접</td>\n",
       "      <td>종</td>\n",
       "      <td>여</td>\n",
       "      <td>부</td>\n",
       "      <td>를</td>\n",
       "      <td>확인</td>\n",
       "      <td>하지</td>\n",
       "      <td>않</td>\n",
       "      <td>아</td>\n",
       "      <td>자유</td>\n",
       "      <td>롭</td>\n",
       "      <td>게</td>\n",
       "      <td>여행</td>\n",
       "      <td>이</td>\n",
       "      <td>가능한</td>\n",
       "      <td>국가</td>\n",
       "      <td>다</td>\n",
       "      <td>.</td>\n",
       "      <td>부산</td>\n",
       "      <td>오</td>\n",
       "      <td>사</td>\n",
       "      <td>카</td>\n",
       "      <td>노</td>\n",
       "      <td>선</td>\n",
       "      <td>은</td>\n",
       "      <td>김</td>\n",
       "      <td>해</td>\n",
       "      <td>공</td>\n",
       "      <td>항</td>\n",
       "      <td>에서</td>\n",
       "      <td>오</td>\n",
       "      <td>전</td>\n",
       "      <td></td>\n",
       "      <td>8</td>\n",
       "      <td>시</td>\n",
       "      <td></td>\n",
       "      <td>35</td>\n",
       "      <td>분</td>\n",
       "      <td>에</td>\n",
       "      <td>출</td>\n",
       "      <td>발</td>\n",
       "      <td>해</td>\n",
       "      <td>간</td>\n",
       "      <td>사이</td>\n",
       "      <td>공</td>\n",
       "      <td>항</td>\n",
       "      <td>에</td>\n",
       "      <td>오</td>\n",
       "      <td>전</td>\n",
       "      <td></td>\n",
       "      <td>10</td>\n",
       "      <td>시</td>\n",
       "      <td>도</td>\n",
       "      <td>착</td>\n",
       "      <td>귀</td>\n",
       "      <td>국</td>\n",
       "      <td>편</td>\n",
       "      <td>은</td>\n",
       "      <td>간</td>\n",
       "      <td>사이</td>\n",
       "      <td>공</td>\n",
       "      <td>항</td>\n",
       "      <td>에서</td>\n",
       "      <td>낮</td>\n",
       "      <td></td>\n",
       "      <td>12</td>\n",
       "      <td>시에</td>\n",
       "      <td>출</td>\n",
       "      <td>발</td>\n",
       "      <td>해</td>\n",
       "      <td>김</td>\n",
       "      <td>해</td>\n",
       "      <td>공</td>\n",
       "      <td>항</td>\n",
       "      <td>에</td>\n",
       "      <td>오후</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>시</td>\n",
       "      <td></td>\n",
       "      <td>30</td>\n",
       "      <td>분</td>\n",
       "      <td>도</td>\n",
       "      <td>착</td>\n",
       "      <td>하는</td>\n",
       "      <td>일정</td>\n",
       "      <td>으로</td>\n",
       "      <td>주</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>회</td>\n",
       "      <td>운</td>\n",
       "      <td>항</td>\n",
       "      <td>한다</td>\n",
       "      <td>.</td>\n",
       "      <td>에</td>\n",
       "      <td>어</td>\n",
       "      <td>부</td>\n",
       "      <td>산</td>\n",
       "      <td>관계</td>\n",
       "      <td>자는</td>\n",
       "      <td>이번</td>\n",
       "      <td>증</td>\n",
       "      <td>편</td>\n",
       "      <td>은</td>\n",
       "      <td>무</td>\n",
       "      <td>비</td>\n",
       "      <td>자</td>\n",
       "      <td>입</td>\n",
       "      <td>국</td>\n",
       "      <td>이</td>\n",
       "      <td>복</td>\n",
       "      <td>원</td>\n",
       "      <td>�</td>\n",
       "      <td>�</td>\n",
       "      <td>을</td>\n",
       "      <td>때</td>\n",
       "      <td>폭</td>\n",
       "      <td>발</td>\n",
       "      <td>하는</td>\n",
       "      <td>여행</td>\n",
       "      <td>수</td>\n",
       "      <td>요</td>\n",
       "      <td>를</td>\n",
       "      <td>선</td>\n",
       "      <td>점</td>\n",
       "      <td>하기</td>\n",
       "      <td>위한</td>\n",
       "      <td>선</td>\n",
       "      <td>제</td>\n",
       "      <td>적</td>\n",
       "      <td>조</td>\n",
       "      <td>치</td>\n",
       "      <td>라</td>\n",
       "      <td>고</td>\n",
       "      <td>밝</td>\n",
       "      <td>�</td>\n",
       "      <td>�다</td>\n",
       "      <td>.</td>\n",
       "      <td>이</td>\n",
       "      <td>�</td>\n",
       "      <td>�</td>\n",
       "      <td>에도</td>\n",
       "      <td>에</td>\n",
       "      <td>어</td>\n",
       "      <td>부</td>\n",
       "      <td>산</td>\n",
       "      <td>은</td>\n",
       "      <td>이</td>\n",
       "      <td>달</td>\n",
       "      <td>중</td>\n",
       "      <td>부산</td>\n",
       "      <td>에서</td>\n",
       "      <td>출</td>\n",
       "      <td>발</td>\n",
       "      <td>하는</td>\n",
       "      <td>코</td>\n",
       "      <td>타</td>\n",
       "      <td>키</td>\n",
       "      <td>나</td>\n",
       "      <td>발</td>\n",
       "      <td>루</td>\n",
       "      <td>나</td>\n",
       "      <td>트</td>\n",
       "      <td>랑</td>\n",
       "      <td>세</td>\n",
       "      <td>부</td>\n",
       "      <td>노</td>\n",
       "      <td>선</td>\n",
       "      <td>등을</td>\n",
       "      <td>운</td>\n",
       "      <td>항</td>\n",
       "      <td>할</td>\n",
       "      <td>예정</td>\n",
       "      <td>이다</td>\n",
       "      <td>.</td>\n",
       "      <td>인천</td>\n",
       "      <td>국</td>\n",
       "      <td>제</td>\n",
       "      <td>공</td>\n",
       "      <td>항</td>\n",
       "      <td>에서는</td>\n",
       "      <td>이</td>\n",
       "      <td>달</td>\n",
       "      <td></td>\n",
       "      <td>14</td>\n",
       "      <td>일</td>\n",
       "      <td>부터</td>\n",
       "      <td>순</td>\n",
       "      <td>차</td>\n",
       "      <td>적으로</td>\n",
       "      <td>다</td>\n",
       "      <td>�</td>\n",
       "      <td>�</td>\n",
       "      <td>방</td>\n",
       "      <td>�</td>\n",
       "      <td>�</td>\n",
       "      <td>후</td>\n",
       "      <td>쿠</td>\n",
       "      <td>오</td>\n",
       "      <td>카</td>\n",
       "      <td>등</td>\n",
       "      <td>노</td>\n",
       "      <td>선을</td>\n",
       "      <td>신규</td>\n",
       "      <td>취</td>\n",
       "      <td>항</td>\n",
       "      <td>한다</td>\n",
       "      <td>.</td>\n",
       "      <td>&lt;|eot_id|&gt;</td>\n",
       "      <td>&lt;|start_header_id|&gt;</td>\n",
       "      <td>assistant</td>\n",
       "      <td>&lt;|end_header_id|&gt;</td>\n",
       "      <td>\\n</td>\n",
       "      <td>{\"</td>\n",
       "      <td>stock</td>\n",
       "      <td>_related</td>\n",
       "      <td>\":</td>\n",
       "      <td>true</td>\n",
       "      <td>,\"</td>\n",
       "      <td>summary</td>\n",
       "      <td>\":\"</td>\n",
       "      <td>에</td>\n",
       "      <td>어</td>\n",
       "      <td>부</td>\n",
       "      <td>산</td>\n",
       "      <td>이</td>\n",
       "      <td>코로나</td>\n",
       "      <td>19</td>\n",
       "      <td>팬</td>\n",
       "      <td>데</td>\n",
       "      <td>�</td>\n",
       "      <td>�</td>\n",
       "      <td>이후</td>\n",
       "      <td></td>\n",
       "      <td>28</td>\n",
       "      <td>개월</td>\n",
       "      <td>만</td>\n",
       "      <td>에</td>\n",
       "      <td>김</td>\n",
       "      <td>해</td>\n",
       "      <td>국</td>\n",
       "      <td>제</td>\n",
       "      <td>공</td>\n",
       "      <td>항</td>\n",
       "      <td>에서</td>\n",
       "      <td>몽</td>\n",
       "      <td>골</td>\n",
       "      <td>울</td>\n",
       "      <td>란</td>\n",
       "      <td>바</td>\n",
       "      <td>토</td>\n",
       "      <td>르</td>\n",
       "      <td>와</td>\n",
       "      <td>일본</td>\n",
       "      <td>오</td>\n",
       "      <td>사</td>\n",
       "      <td>카</td>\n",
       "      <td>노</td>\n",
       "      <td>선</td>\n",
       "      <td>운</td>\n",
       "      <td>항</td>\n",
       "      <td>을</td>\n",
       "      <td>주</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>회</td>\n",
       "      <td>재</td>\n",
       "      <td>개</td>\n",
       "      <td>하며</td>\n",
       "      <td>,</td>\n",
       "      <td>곧</td>\n",
       "      <td>코</td>\n",
       "      <td>타</td>\n",
       "      <td>키</td>\n",
       "      <td>나</td>\n",
       "      <td>발</td>\n",
       "      <td>루</td>\n",
       "      <td>,</td>\n",
       "      <td>나</td>\n",
       "      <td>트</td>\n",
       "      <td>랑</td>\n",
       "      <td>,</td>\n",
       "      <td>세</td>\n",
       "      <td>부</td>\n",
       "      <td>등</td>\n",
       "      <td>동</td>\n",
       "      <td>남</td>\n",
       "      <td>아</td>\n",
       "      <td>노</td>\n",
       "      <td>선</td>\n",
       "      <td>도</td>\n",
       "      <td>운</td>\n",
       "      <td>항</td>\n",
       "      <td>을</td>\n",
       "      <td>시작</td>\n",
       "      <td>한다</td>\n",
       "      <td>.</td>\n",
       "      <td>인천</td>\n",
       "      <td>공</td>\n",
       "      <td>항</td>\n",
       "      <td>발</td>\n",
       "      <td>국제</td>\n",
       "      <td>노</td>\n",
       "      <td>선</td>\n",
       "      <td>확</td>\n",
       "      <td>대</td>\n",
       "      <td>도</td>\n",
       "      <td>예</td>\n",
       "      <td>고</td>\n",
       "      <td>했다</td>\n",
       "      <td>.</td>\n",
       "      <td>몽</td>\n",
       "      <td>골</td>\n",
       "      <td>의</td>\n",
       "      <td>무</td>\n",
       "      <td>격</td>\n",
       "      <td>리</td>\n",
       "      <td>·</td>\n",
       "      <td>무</td>\n",
       "      <td>백</td>\n",
       "      <td>신</td>\n",
       "      <td>입</td>\n",
       "      <td>국</td>\n",
       "      <td>정책</td>\n",
       "      <td>,</td>\n",
       "      <td>일본</td>\n",
       "      <td>오</td>\n",
       "      <td>사</td>\n",
       "      <td>카</td>\n",
       "      <td>노</td>\n",
       "      <td>선</td>\n",
       "      <td>등</td>\n",
       "      <td>해외</td>\n",
       "      <td>여</td>\n",
       "      <td>행</td>\n",
       "      <td>수</td>\n",
       "      <td>요</td>\n",
       "      <td>증</td>\n",
       "      <td>대를</td>\n",
       "      <td>선</td>\n",
       "      <td>제</td>\n",
       "      <td>적으로</td>\n",
       "      <td>겨</td>\n",
       "      <td>냥</td>\n",
       "      <td>한</td>\n",
       "      <td>조</td>\n",
       "      <td>치</td>\n",
       "      <td>이다</td>\n",
       "      <td>.\",\"</td>\n",
       "      <td>positive</td>\n",
       "      <td>_st</td>\n",
       "      <td>ocks</td>\n",
       "      <td>\":[\"</td>\n",
       "      <td>에</td>\n",
       "      <td>어</td>\n",
       "      <td>부</td>\n",
       "      <td>산</td>\n",
       "      <td>\"],\"</td>\n",
       "      <td>positive</td>\n",
       "      <td>_keywords</td>\n",
       "      <td>\":[\"</td>\n",
       "      <td>국</td>\n",
       "      <td>제</td>\n",
       "      <td>선</td>\n",
       "      <td>운</td>\n",
       "      <td>항</td>\n",
       "      <td>재</td>\n",
       "      <td>개</td>\n",
       "      <td>\",\"</td>\n",
       "      <td>해</td>\n",
       "      <td>외</td>\n",
       "      <td>여</td>\n",
       "      <td>행</td>\n",
       "      <td>수</td>\n",
       "      <td>요</td>\n",
       "      <td>확</td>\n",
       "      <td>대</td>\n",
       "      <td>\",\"</td>\n",
       "      <td>동</td>\n",
       "      <td>남</td>\n",
       "      <td>아</td>\n",
       "      <td>·</td>\n",
       "      <td>일본</td>\n",
       "      <td>노</td>\n",
       "      <td>선</td>\n",
       "      <td>확</td>\n",
       "      <td>대</td>\n",
       "      <td>\",\"</td>\n",
       "      <td>코</td>\n",
       "      <td>로나</td>\n",
       "      <td>이후</td>\n",
       "      <td>노</td>\n",
       "      <td>선</td>\n",
       "      <td>복</td>\n",
       "      <td>원</td>\n",
       "      <td>\"],\"</td>\n",
       "      <td>positive</td>\n",
       "      <td>_reason</td>\n",
       "      <td>s</td>\n",
       "      <td>\":\"</td>\n",
       "      <td>�</td>\n",
       "      <td>�</td>\n",
       "      <td>데</td>\n",
       "      <td>�</td>\n",
       "      <td>�</td>\n",
       "      <td>이후</td>\n",
       "      <td>�</td>\n",
       "      <td>�</td>\n",
       "      <td>�</td>\n",
       "      <td>�</td>\n",
       "      <td>렸</td>\n",
       "      <td>던</td>\n",
       "      <td>해외</td>\n",
       "      <td>여</td>\n",
       "      <td>행</td>\n",
       "      <td>수</td>\n",
       "      <td>요</td>\n",
       "      <td>급</td>\n",
       "      <td>증</td>\n",
       "      <td>기</td>\n",
       "      <td>대</td>\n",
       "      <td>,</td>\n",
       "      <td>국제</td>\n",
       "      <td>선</td>\n",
       "      <td>노</td>\n",
       "      <td>선</td>\n",
       "      <td>재</td>\n",
       "      <td>개</td>\n",
       "      <td>및</td>\n",
       "      <td>증</td>\n",
       "      <td>편</td>\n",
       "      <td>으로</td>\n",
       "      <td>에</td>\n",
       "      <td>어</td>\n",
       "      <td>부</td>\n",
       "      <td>산</td>\n",
       "      <td>의</td>\n",
       "      <td>항</td>\n",
       "      <td>공</td>\n",
       "      <td>노</td>\n",
       "      <td>선</td>\n",
       "      <td>수</td>\n",
       "      <td>익</td>\n",
       "      <td>확</td>\n",
       "      <td>대</td>\n",
       "      <td>가</td>\n",
       "      <td>예</td>\n",
       "      <td>상</td>\n",
       "      <td>되어</td>\n",
       "      <td>�</td>\n",
       "      <td>�</td>\n",
       "      <td>정</td>\n",
       "      <td>적</td>\n",
       "      <td>영향을</td>\n",
       "      <td>줄</td>\n",
       "      <td>수</td>\n",
       "      <td>있음</td>\n",
       "      <td>.</td>\n",
       "      <td>특히</td>\n",
       "      <td>일본</td>\n",
       "      <td>,</td>\n",
       "      <td>동</td>\n",
       "      <td>남</td>\n",
       "      <td>아</td>\n",
       "      <td>,</td>\n",
       "      <td>몽</td>\n",
       "      <td>골</td>\n",
       "      <td>등</td>\n",
       "      <td>인기</td>\n",
       "      <td>지역</td>\n",
       "      <td>노</td>\n",
       "      <td>선</td>\n",
       "      <td>의</td>\n",
       "      <td>조</td>\n",
       "      <td>기</td>\n",
       "      <td>복</td>\n",
       "      <td>원</td>\n",
       "      <td>은</td>\n",
       "      <td>매</td>\n",
       "      <td>출</td>\n",
       "      <td>증가</td>\n",
       "      <td>로</td>\n",
       "      <td>직</td>\n",
       "      <td>결</td>\n",
       "      <td>될</td>\n",
       "      <td>가능</td>\n",
       "      <td>성이</td>\n",
       "      <td>높</td>\n",
       "      <td>음</td>\n",
       "      <td>.\",\"</td>\n",
       "      <td>negative</td>\n",
       "      <td>_st</td>\n",
       "      <td>ocks</td>\n",
       "      <td>\":[</td>\n",
       "      <td>],\"</td>\n",
       "      <td>negative</td>\n",
       "      <td>_keywords</td>\n",
       "      <td>\":[</td>\n",
       "      <td>],\"</td>\n",
       "      <td>negative</td>\n",
       "      <td>_reason</td>\n",
       "      <td>s</td>\n",
       "      <td>\":\"\"</td>\n",
       "      <td>}</td>\n",
       "      <td>&lt;|eot_id|&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>input_ids</th>\n",
       "      <td>128000</td>\n",
       "      <td>128006</td>\n",
       "      <td>9125</td>\n",
       "      <td>128007</td>\n",
       "      <td>198</td>\n",
       "      <td>65895</td>\n",
       "      <td>83628</td>\n",
       "      <td>34804</td>\n",
       "      <td>104193</td>\n",
       "      <td>123061</td>\n",
       "      <td>14</td>\n",
       "      <td>127463</td>\n",
       "      <td>111068</td>\n",
       "      <td>120226</td>\n",
       "      <td>125959</td>\n",
       "      <td>102612</td>\n",
       "      <td>96318</td>\n",
       "      <td>27797</td>\n",
       "      <td>18359</td>\n",
       "      <td>87097</td>\n",
       "      <td>103168</td>\n",
       "      <td>34983</td>\n",
       "      <td>114942</td>\n",
       "      <td>101360</td>\n",
       "      <td>11</td>\n",
       "      <td>720</td>\n",
       "      <td>108159</td>\n",
       "      <td>30381</td>\n",
       "      <td>59134</td>\n",
       "      <td>41953</td>\n",
       "      <td>99458</td>\n",
       "      <td>88708</td>\n",
       "      <td>19954</td>\n",
       "      <td>101412</td>\n",
       "      <td>116129</td>\n",
       "      <td>41871</td>\n",
       "      <td>235</td>\n",
       "      <td>30381</td>\n",
       "      <td>14</td>\n",
       "      <td>64189</td>\n",
       "      <td>30381</td>\n",
       "      <td>115754</td>\n",
       "      <td>58126</td>\n",
       "      <td>64189</td>\n",
       "      <td>11</td>\n",
       "      <td>111436</td>\n",
       "      <td>11</td>\n",
       "      <td>106589</td>\n",
       "      <td>93292</td>\n",
       "      <td>18918</td>\n",
       "      <td>109862</td>\n",
       "      <td>44005</td>\n",
       "      <td>104193</td>\n",
       "      <td>123061</td>\n",
       "      <td>14</td>\n",
       "      <td>127463</td>\n",
       "      <td>109862</td>\n",
       "      <td>116425</td>\n",
       "      <td>20565</td>\n",
       "      <td>80052</td>\n",
       "      <td>382</td>\n",
       "      <td>13447</td>\n",
       "      <td>49531</td>\n",
       "      <td>62226</td>\n",
       "      <td>22035</td>\n",
       "      <td>30426</td>\n",
       "      <td>115790</td>\n",
       "      <td>18359</td>\n",
       "      <td>67890</td>\n",
       "      <td>115061</td>\n",
       "      <td>92769</td>\n",
       "      <td>627</td>\n",
       "      <td>16</td>\n",
       "      <td>13</td>\n",
       "      <td>111068</td>\n",
       "      <td>25941</td>\n",
       "      <td>81673</td>\n",
       "      <td>99458</td>\n",
       "      <td>88708</td>\n",
       "      <td>63375</td>\n",
       "      <td>21028</td>\n",
       "      <td>78453</td>\n",
       "      <td>101106</td>\n",
       "      <td>111490</td>\n",
       "      <td>121712</td>\n",
       "      <td>48936</td>\n",
       "      <td>29833</td>\n",
       "      <td>47782</td>\n",
       "      <td>115300</td>\n",
       "      <td>512</td>\n",
       "      <td>262</td>\n",
       "      <td>482</td>\n",
       "      <td>5708</td>\n",
       "      <td>54356</td>\n",
       "      <td>18918</td>\n",
       "      <td>3641</td>\n",
       "      <td>17835</td>\n",
       "      <td>114839</td>\n",
       "      <td>92245</td>\n",
       "      <td>627</td>\n",
       "      <td>262</td>\n",
       "      <td>482</td>\n",
       "      <td>12399</td>\n",
       "      <td>19954</td>\n",
       "      <td>111068</td>\n",
       "      <td>120226</td>\n",
       "      <td>87097</td>\n",
       "      <td>103168</td>\n",
       "      <td>18359</td>\n",
       "      <td>114839</td>\n",
       "      <td>92245</td>\n",
       "      <td>627</td>\n",
       "      <td>17</td>\n",
       "      <td>13</td>\n",
       "      <td>111068</td>\n",
       "      <td>25941</td>\n",
       "      <td>81673</td>\n",
       "      <td>99458</td>\n",
       "      <td>88708</td>\n",
       "      <td>63375</td>\n",
       "      <td>21028</td>\n",
       "      <td>78453</td>\n",
       "      <td>101106</td>\n",
       "      <td>111490</td>\n",
       "      <td>121712</td>\n",
       "      <td>101528</td>\n",
       "      <td>33390</td>\n",
       "      <td>512</td>\n",
       "      <td>262</td>\n",
       "      <td>482</td>\n",
       "      <td>5708</td>\n",
       "      <td>54356</td>\n",
       "      <td>18918</td>\n",
       "      <td>3082</td>\n",
       "      <td>17835</td>\n",
       "      <td>114839</td>\n",
       "      <td>92245</td>\n",
       "      <td>627</td>\n",
       "      <td>262</td>\n",
       "      <td>482</td>\n",
       "      <td>12399</td>\n",
       "      <td>19954</td>\n",
       "      <td>111068</td>\n",
       "      <td>120226</td>\n",
       "      <td>87097</td>\n",
       "      <td>103168</td>\n",
       "      <td>18359</td>\n",
       "      <td>114839</td>\n",
       "      <td>92245</td>\n",
       "      <td>627</td>\n",
       "      <td>262</td>\n",
       "      <td>482</td>\n",
       "      <td>41871</td>\n",
       "      <td>235</td>\n",
       "      <td>30381</td>\n",
       "      <td>101090</td>\n",
       "      <td>104762</td>\n",
       "      <td>13094</td>\n",
       "      <td>96717</td>\n",
       "      <td>57002</td>\n",
       "      <td>107205</td>\n",
       "      <td>99458</td>\n",
       "      <td>88708</td>\n",
       "      <td>13094</td>\n",
       "      <td>91786</td>\n",
       "      <td>33390</td>\n",
       "      <td>11</td>\n",
       "      <td>6928</td>\n",
       "      <td>1284</td>\n",
       "      <td>26246</td>\n",
       "      <td>11</td>\n",
       "      <td>6928</td>\n",
       "      <td>52454</td>\n",
       "      <td>11</td>\n",
       "      <td>6928</td>\n",
       "      <td>39329</td>\n",
       "      <td>82</td>\n",
       "      <td>18918</td>\n",
       "      <td>114839</td>\n",
       "      <td>92245</td>\n",
       "      <td>627</td>\n",
       "      <td>262</td>\n",
       "      <td>482</td>\n",
       "      <td>86503</td>\n",
       "      <td>30381</td>\n",
       "      <td>101090</td>\n",
       "      <td>104762</td>\n",
       "      <td>13094</td>\n",
       "      <td>96717</td>\n",
       "      <td>57002</td>\n",
       "      <td>107205</td>\n",
       "      <td>99458</td>\n",
       "      <td>88708</td>\n",
       "      <td>13094</td>\n",
       "      <td>91786</td>\n",
       "      <td>33390</td>\n",
       "      <td>11</td>\n",
       "      <td>8389</td>\n",
       "      <td>1284</td>\n",
       "      <td>26246</td>\n",
       "      <td>11</td>\n",
       "      <td>8389</td>\n",
       "      <td>52454</td>\n",
       "      <td>11</td>\n",
       "      <td>8389</td>\n",
       "      <td>39329</td>\n",
       "      <td>82</td>\n",
       "      <td>18918</td>\n",
       "      <td>114839</td>\n",
       "      <td>92245</td>\n",
       "      <td>627</td>\n",
       "      <td>262</td>\n",
       "      <td>482</td>\n",
       "      <td>46663</td>\n",
       "      <td>13094</td>\n",
       "      <td>108838</td>\n",
       "      <td>50152</td>\n",
       "      <td>122292</td>\n",
       "      <td>81021</td>\n",
       "      <td>55055</td>\n",
       "      <td>493</td>\n",
       "      <td>4670</td>\n",
       "      <td>122292</td>\n",
       "      <td>84734</td>\n",
       "      <td>10779</td>\n",
       "      <td>8</td>\n",
       "      <td>17835</td>\n",
       "      <td>114839</td>\n",
       "      <td>92245</td>\n",
       "      <td>13</td>\n",
       "      <td>128009</td>\n",
       "      <td>128006</td>\n",
       "      <td>882</td>\n",
       "      <td>128007</td>\n",
       "      <td>198</td>\n",
       "      <td>19954</td>\n",
       "      <td>32179</td>\n",
       "      <td>64189</td>\n",
       "      <td>86157</td>\n",
       "      <td>111535</td>\n",
       "      <td>103272</td>\n",
       "      <td>101974</td>\n",
       "      <td>101665</td>\n",
       "      <td>100968</td>\n",
       "      <td>14260</td>\n",
       "      <td>58368</td>\n",
       "      <td>56154</td>\n",
       "      <td>101436</td>\n",
       "      <td>102058</td>\n",
       "      <td>101151</td>\n",
       "      <td>102888</td>\n",
       "      <td>60861</td>\n",
       "      <td>198</td>\n",
       "      <td>19954</td>\n",
       "      <td>32179</td>\n",
       "      <td>64189</td>\n",
       "      <td>86157</td>\n",
       "      <td>13094</td>\n",
       "      <td>102155</td>\n",
       "      <td>34983</td>\n",
       "      <td>100654</td>\n",
       "      <td>38187</td>\n",
       "      <td>79225</td>\n",
       "      <td>103866</td>\n",
       "      <td>57575</td>\n",
       "      <td>102722</td>\n",
       "      <td>102133</td>\n",
       "      <td>44005</td>\n",
       "      <td>127385</td>\n",
       "      <td>112542</td>\n",
       "      <td>111535</td>\n",
       "      <td>103272</td>\n",
       "      <td>101974</td>\n",
       "      <td>101665</td>\n",
       "      <td>100968</td>\n",
       "      <td>81673</td>\n",
       "      <td>107715</td>\n",
       "      <td>74177</td>\n",
       "      <td>56154</td>\n",
       "      <td>101436</td>\n",
       "      <td>102058</td>\n",
       "      <td>101151</td>\n",
       "      <td>103678</td>\n",
       "      <td>103866</td>\n",
       "      <td>18359</td>\n",
       "      <td>127141</td>\n",
       "      <td>56773</td>\n",
       "      <td>220</td>\n",
       "      <td>17</td>\n",
       "      <td>62841</td>\n",
       "      <td>125274</td>\n",
       "      <td>43139</td>\n",
       "      <td>124141</td>\n",
       "      <td>777</td>\n",
       "      <td>124460</td>\n",
       "      <td>100933</td>\n",
       "      <td>50273</td>\n",
       "      <td>117</td>\n",
       "      <td>33229</td>\n",
       "      <td>87472</td>\n",
       "      <td>111323</td>\n",
       "      <td>220</td>\n",
       "      <td>1591</td>\n",
       "      <td>125085</td>\n",
       "      <td>63207</td>\n",
       "      <td>19954</td>\n",
       "      <td>102888</td>\n",
       "      <td>60861</td>\n",
       "      <td>52976</td>\n",
       "      <td>35495</td>\n",
       "      <td>220</td>\n",
       "      <td>16</td>\n",
       "      <td>33177</td>\n",
       "      <td>116283</td>\n",
       "      <td>35859</td>\n",
       "      <td>104828</td>\n",
       "      <td>13</td>\n",
       "      <td>118089</td>\n",
       "      <td>111535</td>\n",
       "      <td>103272</td>\n",
       "      <td>101974</td>\n",
       "      <td>101665</td>\n",
       "      <td>100968</td>\n",
       "      <td>102058</td>\n",
       "      <td>101151</td>\n",
       "      <td>34804</td>\n",
       "      <td>102155</td>\n",
       "      <td>34983</td>\n",
       "      <td>100654</td>\n",
       "      <td>38187</td>\n",
       "      <td>79225</td>\n",
       "      <td>103866</td>\n",
       "      <td>57575</td>\n",
       "      <td>74177</td>\n",
       "      <td>66965</td>\n",
       "      <td>220</td>\n",
       "      <td>23</td>\n",
       "      <td>30426</td>\n",
       "      <td>220</td>\n",
       "      <td>914</td>\n",
       "      <td>80816</td>\n",
       "      <td>19954</td>\n",
       "      <td>102722</td>\n",
       "      <td>102133</td>\n",
       "      <td>34983</td>\n",
       "      <td>103055</td>\n",
       "      <td>22035</td>\n",
       "      <td>100994</td>\n",
       "      <td>103866</td>\n",
       "      <td>19954</td>\n",
       "      <td>74177</td>\n",
       "      <td>66965</td>\n",
       "      <td>220</td>\n",
       "      <td>806</td>\n",
       "      <td>30426</td>\n",
       "      <td>220</td>\n",
       "      <td>1272</td>\n",
       "      <td>80816</td>\n",
       "      <td>101703</td>\n",
       "      <td>111283</td>\n",
       "      <td>101360</td>\n",
       "      <td>110946</td>\n",
       "      <td>100654</td>\n",
       "      <td>104790</td>\n",
       "      <td>34804</td>\n",
       "      <td>124467</td>\n",
       "      <td>220</td>\n",
       "      <td>16</td>\n",
       "      <td>118472</td>\n",
       "      <td>102722</td>\n",
       "      <td>102133</td>\n",
       "      <td>34983</td>\n",
       "      <td>102155</td>\n",
       "      <td>34983</td>\n",
       "      <td>79225</td>\n",
       "      <td>103866</td>\n",
       "      <td>19954</td>\n",
       "      <td>124467</td>\n",
       "      <td>220</td>\n",
       "      <td>20</td>\n",
       "      <td>30426</td>\n",
       "      <td>220</td>\n",
       "      <td>966</td>\n",
       "      <td>80816</td>\n",
       "      <td>101703</td>\n",
       "      <td>111283</td>\n",
       "      <td>44005</td>\n",
       "      <td>125274</td>\n",
       "      <td>43139</td>\n",
       "      <td>56773</td>\n",
       "      <td>220</td>\n",
       "      <td>17</td>\n",
       "      <td>62841</td>\n",
       "      <td>103678</td>\n",
       "      <td>103866</td>\n",
       "      <td>52976</td>\n",
       "      <td>13</td>\n",
       "      <td>127385</td>\n",
       "      <td>112542</td>\n",
       "      <td>34804</td>\n",
       "      <td>39250</td>\n",
       "      <td>100654</td>\n",
       "      <td>45618</td>\n",
       "      <td>124141</td>\n",
       "      <td>777</td>\n",
       "      <td>86422</td>\n",
       "      <td>56154</td>\n",
       "      <td>81673</td>\n",
       "      <td>107696</td>\n",
       "      <td>83628</td>\n",
       "      <td>108712</td>\n",
       "      <td>102757</td>\n",
       "      <td>84618</td>\n",
       "      <td>64189</td>\n",
       "      <td>18918</td>\n",
       "      <td>74959</td>\n",
       "      <td>88525</td>\n",
       "      <td>51796</td>\n",
       "      <td>54059</td>\n",
       "      <td>117542</td>\n",
       "      <td>120591</td>\n",
       "      <td>58901</td>\n",
       "      <td>121528</td>\n",
       "      <td>13094</td>\n",
       "      <td>125502</td>\n",
       "      <td>109916</td>\n",
       "      <td>13447</td>\n",
       "      <td>13</td>\n",
       "      <td>118089</td>\n",
       "      <td>74177</td>\n",
       "      <td>56154</td>\n",
       "      <td>101436</td>\n",
       "      <td>102058</td>\n",
       "      <td>101151</td>\n",
       "      <td>34804</td>\n",
       "      <td>102155</td>\n",
       "      <td>34983</td>\n",
       "      <td>79225</td>\n",
       "      <td>103866</td>\n",
       "      <td>57575</td>\n",
       "      <td>74177</td>\n",
       "      <td>66965</td>\n",
       "      <td>220</td>\n",
       "      <td>23</td>\n",
       "      <td>30426</td>\n",
       "      <td>220</td>\n",
       "      <td>1758</td>\n",
       "      <td>80816</td>\n",
       "      <td>19954</td>\n",
       "      <td>102722</td>\n",
       "      <td>102133</td>\n",
       "      <td>34983</td>\n",
       "      <td>105131</td>\n",
       "      <td>125166</td>\n",
       "      <td>79225</td>\n",
       "      <td>103866</td>\n",
       "      <td>19954</td>\n",
       "      <td>74177</td>\n",
       "      <td>66965</td>\n",
       "      <td>220</td>\n",
       "      <td>605</td>\n",
       "      <td>30426</td>\n",
       "      <td>101703</td>\n",
       "      <td>111283</td>\n",
       "      <td>110946</td>\n",
       "      <td>100654</td>\n",
       "      <td>104790</td>\n",
       "      <td>34804</td>\n",
       "      <td>105131</td>\n",
       "      <td>125166</td>\n",
       "      <td>79225</td>\n",
       "      <td>103866</td>\n",
       "      <td>57575</td>\n",
       "      <td>120889</td>\n",
       "      <td>220</td>\n",
       "      <td>717</td>\n",
       "      <td>118472</td>\n",
       "      <td>102722</td>\n",
       "      <td>102133</td>\n",
       "      <td>34983</td>\n",
       "      <td>102155</td>\n",
       "      <td>34983</td>\n",
       "      <td>79225</td>\n",
       "      <td>103866</td>\n",
       "      <td>19954</td>\n",
       "      <td>124467</td>\n",
       "      <td>220</td>\n",
       "      <td>16</td>\n",
       "      <td>30426</td>\n",
       "      <td>220</td>\n",
       "      <td>966</td>\n",
       "      <td>80816</td>\n",
       "      <td>101703</td>\n",
       "      <td>111283</td>\n",
       "      <td>44005</td>\n",
       "      <td>125274</td>\n",
       "      <td>43139</td>\n",
       "      <td>56773</td>\n",
       "      <td>220</td>\n",
       "      <td>17</td>\n",
       "      <td>62841</td>\n",
       "      <td>103678</td>\n",
       "      <td>103866</td>\n",
       "      <td>52976</td>\n",
       "      <td>13</td>\n",
       "      <td>91586</td>\n",
       "      <td>32179</td>\n",
       "      <td>64189</td>\n",
       "      <td>86157</td>\n",
       "      <td>116680</td>\n",
       "      <td>112953</td>\n",
       "      <td>117717</td>\n",
       "      <td>107034</td>\n",
       "      <td>104790</td>\n",
       "      <td>34804</td>\n",
       "      <td>101480</td>\n",
       "      <td>71682</td>\n",
       "      <td>26799</td>\n",
       "      <td>39250</td>\n",
       "      <td>100654</td>\n",
       "      <td>13094</td>\n",
       "      <td>107067</td>\n",
       "      <td>55421</td>\n",
       "      <td>33943</td>\n",
       "      <td>238</td>\n",
       "      <td>18359</td>\n",
       "      <td>54718</td>\n",
       "      <td>115062</td>\n",
       "      <td>102133</td>\n",
       "      <td>44005</td>\n",
       "      <td>121528</td>\n",
       "      <td>29833</td>\n",
       "      <td>36811</td>\n",
       "      <td>18918</td>\n",
       "      <td>101585</td>\n",
       "      <td>101838</td>\n",
       "      <td>67525</td>\n",
       "      <td>107472</td>\n",
       "      <td>101585</td>\n",
       "      <td>38187</td>\n",
       "      <td>82068</td>\n",
       "      <td>66610</td>\n",
       "      <td>60798</td>\n",
       "      <td>103959</td>\n",
       "      <td>35495</td>\n",
       "      <td>116283</td>\n",
       "      <td>35859</td>\n",
       "      <td>104828</td>\n",
       "      <td>13</td>\n",
       "      <td>23955</td>\n",
       "      <td>39277</td>\n",
       "      <td>244</td>\n",
       "      <td>109018</td>\n",
       "      <td>91586</td>\n",
       "      <td>32179</td>\n",
       "      <td>64189</td>\n",
       "      <td>86157</td>\n",
       "      <td>34804</td>\n",
       "      <td>23955</td>\n",
       "      <td>104684</td>\n",
       "      <td>72043</td>\n",
       "      <td>118089</td>\n",
       "      <td>57575</td>\n",
       "      <td>102722</td>\n",
       "      <td>102133</td>\n",
       "      <td>44005</td>\n",
       "      <td>103651</td>\n",
       "      <td>101109</td>\n",
       "      <td>102474</td>\n",
       "      <td>61415</td>\n",
       "      <td>102133</td>\n",
       "      <td>102268</td>\n",
       "      <td>74618</td>\n",
       "      <td>29726</td>\n",
       "      <td>102581</td>\n",
       "      <td>101852</td>\n",
       "      <td>64189</td>\n",
       "      <td>102058</td>\n",
       "      <td>101151</td>\n",
       "      <td>120908</td>\n",
       "      <td>103678</td>\n",
       "      <td>103866</td>\n",
       "      <td>48936</td>\n",
       "      <td>126088</td>\n",
       "      <td>101568</td>\n",
       "      <td>13</td>\n",
       "      <td>121772</td>\n",
       "      <td>100654</td>\n",
       "      <td>38187</td>\n",
       "      <td>79225</td>\n",
       "      <td>103866</td>\n",
       "      <td>107031</td>\n",
       "      <td>23955</td>\n",
       "      <td>104684</td>\n",
       "      <td>220</td>\n",
       "      <td>975</td>\n",
       "      <td>33177</td>\n",
       "      <td>103551</td>\n",
       "      <td>106248</td>\n",
       "      <td>101532</td>\n",
       "      <td>104182</td>\n",
       "      <td>50467</td>\n",
       "      <td>40275</td>\n",
       "      <td>255</td>\n",
       "      <td>75908</td>\n",
       "      <td>100966</td>\n",
       "      <td>243</td>\n",
       "      <td>95415</td>\n",
       "      <td>107872</td>\n",
       "      <td>58368</td>\n",
       "      <td>101436</td>\n",
       "      <td>78102</td>\n",
       "      <td>102058</td>\n",
       "      <td>126712</td>\n",
       "      <td>126902</td>\n",
       "      <td>107545</td>\n",
       "      <td>103866</td>\n",
       "      <td>52976</td>\n",
       "      <td>13</td>\n",
       "      <td>128009</td>\n",
       "      <td>128006</td>\n",
       "      <td>78191</td>\n",
       "      <td>128007</td>\n",
       "      <td>198</td>\n",
       "      <td>5018</td>\n",
       "      <td>13787</td>\n",
       "      <td>54356</td>\n",
       "      <td>794</td>\n",
       "      <td>1904</td>\n",
       "      <td>1359</td>\n",
       "      <td>1743</td>\n",
       "      <td>3332</td>\n",
       "      <td>19954</td>\n",
       "      <td>32179</td>\n",
       "      <td>64189</td>\n",
       "      <td>86157</td>\n",
       "      <td>13094</td>\n",
       "      <td>124141</td>\n",
       "      <td>777</td>\n",
       "      <td>124460</td>\n",
       "      <td>100933</td>\n",
       "      <td>50273</td>\n",
       "      <td>117</td>\n",
       "      <td>111323</td>\n",
       "      <td>220</td>\n",
       "      <td>1591</td>\n",
       "      <td>125085</td>\n",
       "      <td>63207</td>\n",
       "      <td>19954</td>\n",
       "      <td>102155</td>\n",
       "      <td>34983</td>\n",
       "      <td>100654</td>\n",
       "      <td>38187</td>\n",
       "      <td>79225</td>\n",
       "      <td>103866</td>\n",
       "      <td>57575</td>\n",
       "      <td>127385</td>\n",
       "      <td>112542</td>\n",
       "      <td>111535</td>\n",
       "      <td>103272</td>\n",
       "      <td>101974</td>\n",
       "      <td>101665</td>\n",
       "      <td>100968</td>\n",
       "      <td>81673</td>\n",
       "      <td>107715</td>\n",
       "      <td>74177</td>\n",
       "      <td>56154</td>\n",
       "      <td>101436</td>\n",
       "      <td>102058</td>\n",
       "      <td>101151</td>\n",
       "      <td>103678</td>\n",
       "      <td>103866</td>\n",
       "      <td>18359</td>\n",
       "      <td>56773</td>\n",
       "      <td>220</td>\n",
       "      <td>17</td>\n",
       "      <td>62841</td>\n",
       "      <td>102888</td>\n",
       "      <td>60861</td>\n",
       "      <td>108859</td>\n",
       "      <td>11</td>\n",
       "      <td>124389</td>\n",
       "      <td>103651</td>\n",
       "      <td>101109</td>\n",
       "      <td>102474</td>\n",
       "      <td>61415</td>\n",
       "      <td>102133</td>\n",
       "      <td>102268</td>\n",
       "      <td>11</td>\n",
       "      <td>74618</td>\n",
       "      <td>29726</td>\n",
       "      <td>102581</td>\n",
       "      <td>11</td>\n",
       "      <td>101852</td>\n",
       "      <td>64189</td>\n",
       "      <td>78102</td>\n",
       "      <td>101604</td>\n",
       "      <td>101963</td>\n",
       "      <td>54059</td>\n",
       "      <td>102058</td>\n",
       "      <td>101151</td>\n",
       "      <td>49085</td>\n",
       "      <td>103678</td>\n",
       "      <td>103866</td>\n",
       "      <td>18359</td>\n",
       "      <td>94821</td>\n",
       "      <td>52976</td>\n",
       "      <td>13</td>\n",
       "      <td>121772</td>\n",
       "      <td>79225</td>\n",
       "      <td>103866</td>\n",
       "      <td>102133</td>\n",
       "      <td>115878</td>\n",
       "      <td>102058</td>\n",
       "      <td>101151</td>\n",
       "      <td>103686</td>\n",
       "      <td>67945</td>\n",
       "      <td>49085</td>\n",
       "      <td>96717</td>\n",
       "      <td>35495</td>\n",
       "      <td>101528</td>\n",
       "      <td>13</td>\n",
       "      <td>127385</td>\n",
       "      <td>112542</td>\n",
       "      <td>21028</td>\n",
       "      <td>101480</td>\n",
       "      <td>102079</td>\n",
       "      <td>29102</td>\n",
       "      <td>14260</td>\n",
       "      <td>100981</td>\n",
       "      <td>106113</td>\n",
       "      <td>83628</td>\n",
       "      <td>39250</td>\n",
       "      <td>100654</td>\n",
       "      <td>126950</td>\n",
       "      <td>11</td>\n",
       "      <td>107715</td>\n",
       "      <td>74177</td>\n",
       "      <td>56154</td>\n",
       "      <td>101436</td>\n",
       "      <td>102058</td>\n",
       "      <td>101151</td>\n",
       "      <td>78102</td>\n",
       "      <td>123102</td>\n",
       "      <td>58126</td>\n",
       "      <td>101066</td>\n",
       "      <td>29833</td>\n",
       "      <td>36811</td>\n",
       "      <td>107034</td>\n",
       "      <td>124784</td>\n",
       "      <td>101585</td>\n",
       "      <td>38187</td>\n",
       "      <td>104182</td>\n",
       "      <td>122358</td>\n",
       "      <td>112012</td>\n",
       "      <td>24486</td>\n",
       "      <td>66610</td>\n",
       "      <td>60798</td>\n",
       "      <td>101568</td>\n",
       "      <td>48991</td>\n",
       "      <td>31587</td>\n",
       "      <td>1284</td>\n",
       "      <td>26246</td>\n",
       "      <td>37899</td>\n",
       "      <td>19954</td>\n",
       "      <td>32179</td>\n",
       "      <td>64189</td>\n",
       "      <td>86157</td>\n",
       "      <td>69982</td>\n",
       "      <td>31587</td>\n",
       "      <td>52454</td>\n",
       "      <td>37899</td>\n",
       "      <td>100654</td>\n",
       "      <td>38187</td>\n",
       "      <td>101151</td>\n",
       "      <td>103678</td>\n",
       "      <td>103866</td>\n",
       "      <td>102888</td>\n",
       "      <td>60861</td>\n",
       "      <td>2247</td>\n",
       "      <td>34983</td>\n",
       "      <td>104065</td>\n",
       "      <td>58126</td>\n",
       "      <td>101066</td>\n",
       "      <td>29833</td>\n",
       "      <td>36811</td>\n",
       "      <td>103686</td>\n",
       "      <td>67945</td>\n",
       "      <td>2247</td>\n",
       "      <td>58189</td>\n",
       "      <td>101963</td>\n",
       "      <td>54059</td>\n",
       "      <td>14260</td>\n",
       "      <td>123256</td>\n",
       "      <td>102058</td>\n",
       "      <td>101151</td>\n",
       "      <td>103686</td>\n",
       "      <td>67945</td>\n",
       "      <td>2247</td>\n",
       "      <td>102525</td>\n",
       "      <td>117465</td>\n",
       "      <td>111323</td>\n",
       "      <td>102058</td>\n",
       "      <td>101151</td>\n",
       "      <td>107067</td>\n",
       "      <td>55421</td>\n",
       "      <td>69982</td>\n",
       "      <td>31587</td>\n",
       "      <td>39329</td>\n",
       "      <td>82</td>\n",
       "      <td>3332</td>\n",
       "      <td>67218</td>\n",
       "      <td>105</td>\n",
       "      <td>100933</td>\n",
       "      <td>50273</td>\n",
       "      <td>117</td>\n",
       "      <td>111323</td>\n",
       "      <td>80402</td>\n",
       "      <td>113</td>\n",
       "      <td>105940</td>\n",
       "      <td>234</td>\n",
       "      <td>105543</td>\n",
       "      <td>101954</td>\n",
       "      <td>123102</td>\n",
       "      <td>58126</td>\n",
       "      <td>101066</td>\n",
       "      <td>29833</td>\n",
       "      <td>36811</td>\n",
       "      <td>117208</td>\n",
       "      <td>102249</td>\n",
       "      <td>55216</td>\n",
       "      <td>67945</td>\n",
       "      <td>11</td>\n",
       "      <td>115878</td>\n",
       "      <td>101151</td>\n",
       "      <td>102058</td>\n",
       "      <td>101151</td>\n",
       "      <td>102888</td>\n",
       "      <td>60861</td>\n",
       "      <td>101824</td>\n",
       "      <td>107034</td>\n",
       "      <td>104790</td>\n",
       "      <td>43139</td>\n",
       "      <td>91586</td>\n",
       "      <td>32179</td>\n",
       "      <td>64189</td>\n",
       "      <td>86157</td>\n",
       "      <td>21028</td>\n",
       "      <td>107744</td>\n",
       "      <td>79225</td>\n",
       "      <td>102058</td>\n",
       "      <td>101151</td>\n",
       "      <td>29833</td>\n",
       "      <td>108964</td>\n",
       "      <td>103686</td>\n",
       "      <td>67945</td>\n",
       "      <td>20565</td>\n",
       "      <td>96717</td>\n",
       "      <td>57002</td>\n",
       "      <td>106910</td>\n",
       "      <td>41871</td>\n",
       "      <td>235</td>\n",
       "      <td>30381</td>\n",
       "      <td>82068</td>\n",
       "      <td>126652</td>\n",
       "      <td>109720</td>\n",
       "      <td>29833</td>\n",
       "      <td>127406</td>\n",
       "      <td>13</td>\n",
       "      <td>125578</td>\n",
       "      <td>107715</td>\n",
       "      <td>11</td>\n",
       "      <td>101604</td>\n",
       "      <td>101963</td>\n",
       "      <td>54059</td>\n",
       "      <td>11</td>\n",
       "      <td>127385</td>\n",
       "      <td>112542</td>\n",
       "      <td>78102</td>\n",
       "      <td>115613</td>\n",
       "      <td>109299</td>\n",
       "      <td>102058</td>\n",
       "      <td>101151</td>\n",
       "      <td>21028</td>\n",
       "      <td>66610</td>\n",
       "      <td>21121</td>\n",
       "      <td>107067</td>\n",
       "      <td>55421</td>\n",
       "      <td>34804</td>\n",
       "      <td>102293</td>\n",
       "      <td>71023</td>\n",
       "      <td>122862</td>\n",
       "      <td>17835</td>\n",
       "      <td>105164</td>\n",
       "      <td>89881</td>\n",
       "      <td>113191</td>\n",
       "      <td>96451</td>\n",
       "      <td>115602</td>\n",
       "      <td>108499</td>\n",
       "      <td>49531</td>\n",
       "      <td>48991</td>\n",
       "      <td>43324</td>\n",
       "      <td>1284</td>\n",
       "      <td>26246</td>\n",
       "      <td>9075</td>\n",
       "      <td>29603</td>\n",
       "      <td>43324</td>\n",
       "      <td>52454</td>\n",
       "      <td>9075</td>\n",
       "      <td>29603</td>\n",
       "      <td>43324</td>\n",
       "      <td>39329</td>\n",
       "      <td>82</td>\n",
       "      <td>63466</td>\n",
       "      <td>92</td>\n",
       "      <td>128009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>attention_mask</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>labels</th>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>198</td>\n",
       "      <td>5018</td>\n",
       "      <td>13787</td>\n",
       "      <td>54356</td>\n",
       "      <td>794</td>\n",
       "      <td>1904</td>\n",
       "      <td>1359</td>\n",
       "      <td>1743</td>\n",
       "      <td>3332</td>\n",
       "      <td>19954</td>\n",
       "      <td>32179</td>\n",
       "      <td>64189</td>\n",
       "      <td>86157</td>\n",
       "      <td>13094</td>\n",
       "      <td>124141</td>\n",
       "      <td>777</td>\n",
       "      <td>124460</td>\n",
       "      <td>100933</td>\n",
       "      <td>50273</td>\n",
       "      <td>117</td>\n",
       "      <td>111323</td>\n",
       "      <td>220</td>\n",
       "      <td>1591</td>\n",
       "      <td>125085</td>\n",
       "      <td>63207</td>\n",
       "      <td>19954</td>\n",
       "      <td>102155</td>\n",
       "      <td>34983</td>\n",
       "      <td>100654</td>\n",
       "      <td>38187</td>\n",
       "      <td>79225</td>\n",
       "      <td>103866</td>\n",
       "      <td>57575</td>\n",
       "      <td>127385</td>\n",
       "      <td>112542</td>\n",
       "      <td>111535</td>\n",
       "      <td>103272</td>\n",
       "      <td>101974</td>\n",
       "      <td>101665</td>\n",
       "      <td>100968</td>\n",
       "      <td>81673</td>\n",
       "      <td>107715</td>\n",
       "      <td>74177</td>\n",
       "      <td>56154</td>\n",
       "      <td>101436</td>\n",
       "      <td>102058</td>\n",
       "      <td>101151</td>\n",
       "      <td>103678</td>\n",
       "      <td>103866</td>\n",
       "      <td>18359</td>\n",
       "      <td>56773</td>\n",
       "      <td>220</td>\n",
       "      <td>17</td>\n",
       "      <td>62841</td>\n",
       "      <td>102888</td>\n",
       "      <td>60861</td>\n",
       "      <td>108859</td>\n",
       "      <td>11</td>\n",
       "      <td>124389</td>\n",
       "      <td>103651</td>\n",
       "      <td>101109</td>\n",
       "      <td>102474</td>\n",
       "      <td>61415</td>\n",
       "      <td>102133</td>\n",
       "      <td>102268</td>\n",
       "      <td>11</td>\n",
       "      <td>74618</td>\n",
       "      <td>29726</td>\n",
       "      <td>102581</td>\n",
       "      <td>11</td>\n",
       "      <td>101852</td>\n",
       "      <td>64189</td>\n",
       "      <td>78102</td>\n",
       "      <td>101604</td>\n",
       "      <td>101963</td>\n",
       "      <td>54059</td>\n",
       "      <td>102058</td>\n",
       "      <td>101151</td>\n",
       "      <td>49085</td>\n",
       "      <td>103678</td>\n",
       "      <td>103866</td>\n",
       "      <td>18359</td>\n",
       "      <td>94821</td>\n",
       "      <td>52976</td>\n",
       "      <td>13</td>\n",
       "      <td>121772</td>\n",
       "      <td>79225</td>\n",
       "      <td>103866</td>\n",
       "      <td>102133</td>\n",
       "      <td>115878</td>\n",
       "      <td>102058</td>\n",
       "      <td>101151</td>\n",
       "      <td>103686</td>\n",
       "      <td>67945</td>\n",
       "      <td>49085</td>\n",
       "      <td>96717</td>\n",
       "      <td>35495</td>\n",
       "      <td>101528</td>\n",
       "      <td>13</td>\n",
       "      <td>127385</td>\n",
       "      <td>112542</td>\n",
       "      <td>21028</td>\n",
       "      <td>101480</td>\n",
       "      <td>102079</td>\n",
       "      <td>29102</td>\n",
       "      <td>14260</td>\n",
       "      <td>100981</td>\n",
       "      <td>106113</td>\n",
       "      <td>83628</td>\n",
       "      <td>39250</td>\n",
       "      <td>100654</td>\n",
       "      <td>126950</td>\n",
       "      <td>11</td>\n",
       "      <td>107715</td>\n",
       "      <td>74177</td>\n",
       "      <td>56154</td>\n",
       "      <td>101436</td>\n",
       "      <td>102058</td>\n",
       "      <td>101151</td>\n",
       "      <td>78102</td>\n",
       "      <td>123102</td>\n",
       "      <td>58126</td>\n",
       "      <td>101066</td>\n",
       "      <td>29833</td>\n",
       "      <td>36811</td>\n",
       "      <td>107034</td>\n",
       "      <td>124784</td>\n",
       "      <td>101585</td>\n",
       "      <td>38187</td>\n",
       "      <td>104182</td>\n",
       "      <td>122358</td>\n",
       "      <td>112012</td>\n",
       "      <td>24486</td>\n",
       "      <td>66610</td>\n",
       "      <td>60798</td>\n",
       "      <td>101568</td>\n",
       "      <td>48991</td>\n",
       "      <td>31587</td>\n",
       "      <td>1284</td>\n",
       "      <td>26246</td>\n",
       "      <td>37899</td>\n",
       "      <td>19954</td>\n",
       "      <td>32179</td>\n",
       "      <td>64189</td>\n",
       "      <td>86157</td>\n",
       "      <td>69982</td>\n",
       "      <td>31587</td>\n",
       "      <td>52454</td>\n",
       "      <td>37899</td>\n",
       "      <td>100654</td>\n",
       "      <td>38187</td>\n",
       "      <td>101151</td>\n",
       "      <td>103678</td>\n",
       "      <td>103866</td>\n",
       "      <td>102888</td>\n",
       "      <td>60861</td>\n",
       "      <td>2247</td>\n",
       "      <td>34983</td>\n",
       "      <td>104065</td>\n",
       "      <td>58126</td>\n",
       "      <td>101066</td>\n",
       "      <td>29833</td>\n",
       "      <td>36811</td>\n",
       "      <td>103686</td>\n",
       "      <td>67945</td>\n",
       "      <td>2247</td>\n",
       "      <td>58189</td>\n",
       "      <td>101963</td>\n",
       "      <td>54059</td>\n",
       "      <td>14260</td>\n",
       "      <td>123256</td>\n",
       "      <td>102058</td>\n",
       "      <td>101151</td>\n",
       "      <td>103686</td>\n",
       "      <td>67945</td>\n",
       "      <td>2247</td>\n",
       "      <td>102525</td>\n",
       "      <td>117465</td>\n",
       "      <td>111323</td>\n",
       "      <td>102058</td>\n",
       "      <td>101151</td>\n",
       "      <td>107067</td>\n",
       "      <td>55421</td>\n",
       "      <td>69982</td>\n",
       "      <td>31587</td>\n",
       "      <td>39329</td>\n",
       "      <td>82</td>\n",
       "      <td>3332</td>\n",
       "      <td>67218</td>\n",
       "      <td>105</td>\n",
       "      <td>100933</td>\n",
       "      <td>50273</td>\n",
       "      <td>117</td>\n",
       "      <td>111323</td>\n",
       "      <td>80402</td>\n",
       "      <td>113</td>\n",
       "      <td>105940</td>\n",
       "      <td>234</td>\n",
       "      <td>105543</td>\n",
       "      <td>101954</td>\n",
       "      <td>123102</td>\n",
       "      <td>58126</td>\n",
       "      <td>101066</td>\n",
       "      <td>29833</td>\n",
       "      <td>36811</td>\n",
       "      <td>117208</td>\n",
       "      <td>102249</td>\n",
       "      <td>55216</td>\n",
       "      <td>67945</td>\n",
       "      <td>11</td>\n",
       "      <td>115878</td>\n",
       "      <td>101151</td>\n",
       "      <td>102058</td>\n",
       "      <td>101151</td>\n",
       "      <td>102888</td>\n",
       "      <td>60861</td>\n",
       "      <td>101824</td>\n",
       "      <td>107034</td>\n",
       "      <td>104790</td>\n",
       "      <td>43139</td>\n",
       "      <td>91586</td>\n",
       "      <td>32179</td>\n",
       "      <td>64189</td>\n",
       "      <td>86157</td>\n",
       "      <td>21028</td>\n",
       "      <td>107744</td>\n",
       "      <td>79225</td>\n",
       "      <td>102058</td>\n",
       "      <td>101151</td>\n",
       "      <td>29833</td>\n",
       "      <td>108964</td>\n",
       "      <td>103686</td>\n",
       "      <td>67945</td>\n",
       "      <td>20565</td>\n",
       "      <td>96717</td>\n",
       "      <td>57002</td>\n",
       "      <td>106910</td>\n",
       "      <td>41871</td>\n",
       "      <td>235</td>\n",
       "      <td>30381</td>\n",
       "      <td>82068</td>\n",
       "      <td>126652</td>\n",
       "      <td>109720</td>\n",
       "      <td>29833</td>\n",
       "      <td>127406</td>\n",
       "      <td>13</td>\n",
       "      <td>125578</td>\n",
       "      <td>107715</td>\n",
       "      <td>11</td>\n",
       "      <td>101604</td>\n",
       "      <td>101963</td>\n",
       "      <td>54059</td>\n",
       "      <td>11</td>\n",
       "      <td>127385</td>\n",
       "      <td>112542</td>\n",
       "      <td>78102</td>\n",
       "      <td>115613</td>\n",
       "      <td>109299</td>\n",
       "      <td>102058</td>\n",
       "      <td>101151</td>\n",
       "      <td>21028</td>\n",
       "      <td>66610</td>\n",
       "      <td>21121</td>\n",
       "      <td>107067</td>\n",
       "      <td>55421</td>\n",
       "      <td>34804</td>\n",
       "      <td>102293</td>\n",
       "      <td>71023</td>\n",
       "      <td>122862</td>\n",
       "      <td>17835</td>\n",
       "      <td>105164</td>\n",
       "      <td>89881</td>\n",
       "      <td>113191</td>\n",
       "      <td>96451</td>\n",
       "      <td>115602</td>\n",
       "      <td>108499</td>\n",
       "      <td>49531</td>\n",
       "      <td>48991</td>\n",
       "      <td>43324</td>\n",
       "      <td>1284</td>\n",
       "      <td>26246</td>\n",
       "      <td>9075</td>\n",
       "      <td>29603</td>\n",
       "      <td>43324</td>\n",
       "      <td>52454</td>\n",
       "      <td>9075</td>\n",
       "      <td>29603</td>\n",
       "      <td>43324</td>\n",
       "      <td>39329</td>\n",
       "      <td>82</td>\n",
       "      <td>63466</td>\n",
       "      <td>92</td>\n",
       "      <td>128009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              0                    1       2    \\\n",
       "token           <|begin_of_text|>  <|start_header_id|>  system   \n",
       "input_ids                  128000               128006    9125   \n",
       "attention_mask                  1                    1       1   \n",
       "labels                       -100                 -100    -100   \n",
       "\n",
       "                              3     4      5      6      7       8       9    \\\n",
       "token           <|end_header_id|>    \\n      당      신      은       금       융   \n",
       "input_ids                  128007   198  65895  83628  34804  104193  123061   \n",
       "attention_mask                  1     1      1      1      1       1       1   \n",
       "labels                       -100  -100   -100   -100   -100    -100    -100   \n",
       "\n",
       "                 10      11      12      13      14      15     16     17   \\\n",
       "token              /      경제       뉴      스의       핵       심      내      용   \n",
       "input_ids         14  127463  111068  120226  125959  102612  96318  27797   \n",
       "attention_mask     1       1       1       1       1       1      1      1   \n",
       "labels          -100    -100    -100    -100    -100    -100   -100   -100   \n",
       "\n",
       "                  18     19      20     21      22      23    24    25   \\\n",
       "token               을      요       약      해      설명      하고     ,    \\n   \n",
       "input_ids       18359  87097  103168  34983  114942  101360    11   720   \n",
       "attention_mask      1      1       1      1       1       1     1     1   \n",
       "labels           -100   -100    -100   -100    -100    -100  -100  -100   \n",
       "\n",
       "                   26     27     28     29     30     31     32      33   \\\n",
       "token                특      정      상      장      종      목      에       미   \n",
       "input_ids       108159  30381  59134  41953  99458  88708  19954  101412   \n",
       "attention_mask       1      1      1      1      1      1      1       1   \n",
       "labels            -100   -100   -100   -100   -100   -100   -100    -100   \n",
       "\n",
       "                   34     35    36     37    38     39     40      41     42   \\\n",
       "token               치는      �     �      정     /      부      정      영향      여   \n",
       "input_ids       116129  41871   235  30381    14  64189  30381  115754  58126   \n",
       "attention_mask       1      1     1      1     1      1      1       1      1   \n",
       "labels            -100   -100  -100   -100  -100   -100   -100    -100   -100   \n",
       "\n",
       "                  43    44      45    46      47     48     49      50   \\\n",
       "token               부     ,      이유     ,       근      거      를      분석   \n",
       "input_ids       64189    11  111436    11  106589  93292  18918  109862   \n",
       "attention_mask      1     1       1     1       1      1      1       1   \n",
       "labels           -100  -100    -100  -100    -100   -100   -100    -100   \n",
       "\n",
       "                  51      52      53    54      55      56      57     58   \\\n",
       "token              하는       금       융     /      경제      분석      전문      가   \n",
       "input_ids       44005  104193  123061    14  127463  109862  116425  20565   \n",
       "attention_mask      1       1       1     1       1       1       1      1   \n",
       "labels           -100    -100    -100  -100    -100    -100    -100   -100   \n",
       "\n",
       "                  59     60     61     62     63     64     65      66   \\\n",
       "token             입니다  .\\n\\n      다      음     출력      지      시      사항   \n",
       "input_ids       80052    382  13447  49531  62226  22035  30426  115790   \n",
       "attention_mask      1      1      1      1      1      1      1       1   \n",
       "labels           -100   -100   -100   -100   -100   -100   -100    -100   \n",
       "\n",
       "                  67     68      69     70    71    72    73      74     75   \\\n",
       "token               을      지       켜    주세요   .\\n     1     .       뉴      스   \n",
       "input_ids       18359  67890  115061  92769   627    16    13  111068  25941   \n",
       "attention_mask      1      1       1      1     1     1     1       1      1   \n",
       "labels           -100   -100    -100   -100  -100  -100  -100    -100   -100   \n",
       "\n",
       "                  76     77     78     79     80     81      82      83   \\\n",
       "token               와      종      목      간      의      연       관      성을   \n",
       "input_ids       81673  99458  88708  63375  21028  78453  101106  111490   \n",
       "attention_mask      1      1      1      1      1      1       1       1   \n",
       "labels           -100   -100   -100   -100   -100   -100    -100    -100   \n",
       "\n",
       "                   84     85     86     87      88    89    90    91      92   \\\n",
       "token               발견      할      수      없      다면   :\\n           -   stock   \n",
       "input_ids       121712  48936  29833  47782  115300   512   262   482    5708   \n",
       "attention_mask       1      1      1      1       1     1     1     1       1   \n",
       "labels            -100   -100   -100   -100    -100  -100  -100  -100    -100   \n",
       "\n",
       "                     93     94      95     96      97     98    99    100  \\\n",
       "token           _related      를   False      로      작성    하세요   .\\n         \n",
       "input_ids          54356  18918    3641  17835  114839  92245   627   262   \n",
       "attention_mask         1      1       1      1       1      1     1     1   \n",
       "labels              -100   -100    -100   -100    -100   -100  -100  -100   \n",
       "\n",
       "                 101       102    103     104     105    106     107    108  \\\n",
       "token              -   summary      에       뉴      스의      요       약      을   \n",
       "input_ids        482     12399  19954  111068  120226  87097  103168  18359   \n",
       "attention_mask     1         1      1       1       1      1       1      1   \n",
       "labels          -100      -100   -100    -100    -100   -100    -100   -100   \n",
       "\n",
       "                   109    110   111   112   113     114    115    116    117  \\\n",
       "token               작성    하세요   .\\n     2     .       뉴      스      와      종   \n",
       "input_ids       114839  92245   627    17    13  111068  25941  81673  99458   \n",
       "attention_mask       1      1     1     1     1       1      1      1      1   \n",
       "labels            -100   -100  -100  -100  -100    -100   -100   -100   -100   \n",
       "\n",
       "                  118    119    120    121     122     123     124     125  \\\n",
       "token               목      간      의      연       관      성을      발견      했다   \n",
       "input_ids       88708  63375  21028  78453  101106  111490  121712  101528   \n",
       "attention_mask      1      1      1      1       1       1       1       1   \n",
       "labels           -100   -100   -100   -100    -100    -100    -100    -100   \n",
       "\n",
       "                  126   127   128   129     130       131    132    133  \\\n",
       "token               면   :\\n           -   stock  _related      를   True   \n",
       "input_ids       33390   512   262   482    5708     54356  18918   3082   \n",
       "attention_mask      1     1     1     1       1         1      1      1   \n",
       "labels           -100  -100  -100  -100    -100      -100   -100   -100   \n",
       "\n",
       "                  134     135    136   137   138   139       140    141  \\\n",
       "token               로      작성    하세요   .\\n           -   summary      에   \n",
       "input_ids       17835  114839  92245   627   262   482     12399  19954   \n",
       "attention_mask      1       1      1     1     1     1         1      1   \n",
       "labels           -100    -100   -100  -100  -100  -100      -100   -100   \n",
       "\n",
       "                   142     143    144     145    146     147    148   149  \\\n",
       "token                뉴      스의      요       약      을      작성    하세요   .\\n   \n",
       "input_ids       111068  120226  87097  103168  18359  114839  92245   627   \n",
       "attention_mask       1       1      1       1      1       1      1     1   \n",
       "labels            -100    -100   -100    -100   -100    -100   -100  -100   \n",
       "\n",
       "                 150   151    152   153    154     155     156    157    158  \\\n",
       "token                    -      �     �      정       영       향      이      예   \n",
       "input_ids        262   482  41871   235  30381  101090  104762  13094  96717   \n",
       "attention_mask     1     1      1     1      1       1       1      1      1   \n",
       "labels          -100  -100   -100  -100   -100    -100    -100   -100   -100   \n",
       "\n",
       "                  159     160    161    162    163    164    165   166  \\\n",
       "token               상      되는      종      목      이     있다      면     ,   \n",
       "input_ids       57002  107205  99458  88708  13094  91786  33390    11   \n",
       "attention_mask      1       1      1      1      1      1      1     1   \n",
       "labels           -100    -100   -100   -100   -100   -100   -100  -100   \n",
       "\n",
       "                      167   168    169   170        171        172   173  \\\n",
       "token            positive   _st   ocks     ,   positive  _keywords     ,   \n",
       "input_ids            6928  1284  26246    11       6928      52454    11   \n",
       "attention_mask          1     1      1     1          1          1     1   \n",
       "labels               -100  -100   -100  -100       -100       -100  -100   \n",
       "\n",
       "                      174      175   176    177     178    179   180   181  \\\n",
       "token            positive  _reason     s      를      작성    하세요   .\\n         \n",
       "input_ids            6928    39329    82  18918  114839  92245   627   262   \n",
       "attention_mask          1        1     1      1       1      1     1     1   \n",
       "labels               -100     -100  -100   -100    -100   -100  -100  -100   \n",
       "\n",
       "                 182    183    184     185     186    187    188    189  \\\n",
       "token              -      부      정       영       향      이      예      상   \n",
       "input_ids        482  86503  30381  101090  104762  13094  96717  57002   \n",
       "attention_mask     1      1      1       1       1      1      1      1   \n",
       "labels          -100   -100   -100    -100    -100   -100   -100   -100   \n",
       "\n",
       "                   190    191    192    193    194    195   196        197  \\\n",
       "token               되는      종      목      이     있다      면     ,   negative   \n",
       "input_ids       107205  99458  88708  13094  91786  33390    11       8389   \n",
       "attention_mask       1      1      1      1      1      1     1          1   \n",
       "labels            -100   -100   -100   -100   -100   -100  -100       -100   \n",
       "\n",
       "                 198    199   200        201        202   203        204  \\\n",
       "token            _st   ocks     ,   negative  _keywords     ,   negative   \n",
       "input_ids       1284  26246    11       8389      52454    11       8389   \n",
       "attention_mask     1      1     1          1          1     1          1   \n",
       "labels          -100   -100  -100       -100       -100  -100       -100   \n",
       "\n",
       "                    205   206    207     208    209   210   211   212    213  \\\n",
       "token           _reason     s      를      작성    하세요   .\\n           -      값   \n",
       "input_ids         39329    82  18918  114839  92245   627   262   482  46663   \n",
       "attention_mask        1     1      1       1      1     1     1     1      1   \n",
       "labels             -100  -100   -100    -100   -100  -100  -100  -100   -100   \n",
       "\n",
       "                  214     215    216     217    218    219   220   221  \\\n",
       "token               이      없는     경우       빈     문자      열    ('   '),   \n",
       "input_ids       13094  108838  50152  122292  81021  55055   493  4670   \n",
       "attention_mask      1       1      1       1      1      1     1     1   \n",
       "labels           -100    -100   -100    -100   -100   -100  -100  -100   \n",
       "\n",
       "                   222    223    224   225    226     227    228   229  \\\n",
       "token                빈    리스트    ([]     )      로      작성    하세요     .   \n",
       "input_ids       122292  84734  10779     8  17835  114839  92245    13   \n",
       "attention_mask       1      1      1     1      1       1      1     1   \n",
       "labels            -100   -100   -100  -100   -100    -100   -100  -100   \n",
       "\n",
       "                       230                  231   232                233  \\\n",
       "token           <|eot_id|>  <|start_header_id|>  user  <|end_header_id|>   \n",
       "input_ids           128009               128006   882             128007   \n",
       "attention_mask           1                    1     1                  1   \n",
       "labels                -100                 -100  -100               -100   \n",
       "\n",
       "                 234    235    236    237    238     239     240     241  \\\n",
       "token             \\n      에      어      부      산       울       란       바   \n",
       "input_ids        198  19954  32179  64189  86157  111535  103272  101974   \n",
       "attention_mask     1      1      1      1      1       1       1       1   \n",
       "labels          -100   -100   -100   -100   -100    -100    -100    -100   \n",
       "\n",
       "                   242     243    244    245    246     247     248     249  \\\n",
       "token                토       르      ·      오      사       카       노       선   \n",
       "input_ids       101665  100968  14260  58368  56154  101436  102058  101151   \n",
       "attention_mask       1       1      1      1      1       1       1       1   \n",
       "labels            -100    -100   -100   -100   -100    -100    -100    -100   \n",
       "\n",
       "                   250    251   252    253    254    255    256    257  \\\n",
       "token                재      개    \\n      에      어      부      산      이   \n",
       "input_ids       102888  60861   198  19954  32179  64189  86157  13094   \n",
       "attention_mask       1      1     1      1      1      1      1      1   \n",
       "labels            -100   -100  -100   -100   -100   -100   -100   -100   \n",
       "\n",
       "                   258    259     260    261    262     263    264     265  \\\n",
       "token                김      해       국      제      공       항     에서       출   \n",
       "input_ids       102155  34983  100654  38187  79225  103866  57575  102722   \n",
       "attention_mask       1      1       1      1      1       1      1       1   \n",
       "labels            -100   -100    -100   -100   -100    -100   -100    -100   \n",
       "\n",
       "                   266    267     268     269     270     271     272     273  \\\n",
       "token                발     하는       몽       골       울       란       바       토   \n",
       "input_ids       102133  44005  127385  112542  111535  103272  101974  101665   \n",
       "attention_mask       1      1       1       1       1       1       1       1   \n",
       "labels            -100   -100    -100    -100    -100    -100    -100    -100   \n",
       "\n",
       "                   274    275     276    277    278     279     280     281  \\\n",
       "token                르      와      일본      오      사       카       노       선   \n",
       "input_ids       100968  81673  107715  74177  56154  101436  102058  101151   \n",
       "attention_mask       1      1       1      1      1       1       1       1   \n",
       "labels            -100   -100    -100   -100   -100    -100    -100    -100   \n",
       "\n",
       "                   282     283    284     285    286   287   288    289  \\\n",
       "token                운       항      을      각각      주           2      회   \n",
       "input_ids       103678  103866  18359  127141  56773   220    17  62841   \n",
       "attention_mask       1       1      1       1      1     1     1      1   \n",
       "labels            -100    -100   -100    -100   -100  -100  -100   -100   \n",
       "\n",
       "                   290    291     292   293     294     295    296   297  \\\n",
       "token               일정     으로     코로나    19       팬       데      �     �   \n",
       "input_ids       125274  43139  124141   777  124460  100933  50273   117   \n",
       "attention_mask       1      1       1     1       1       1      1     1   \n",
       "labels            -100   -100    -100  -100    -100    -100   -100  -100   \n",
       "\n",
       "                  298    299     300   301   302     303    304    305  \\\n",
       "token               사      태      이후          28      개월      만      에   \n",
       "input_ids       33229  87472  111323   220  1591  125085  63207  19954   \n",
       "attention_mask      1      1       1     1     1       1      1      1   \n",
       "labels           -100   -100    -100  -100  -100    -100   -100   -100   \n",
       "\n",
       "                   306    307    308    309   310   311    312     313    314  \\\n",
       "token                재      개     한다      고           1      일       밝      �   \n",
       "input_ids       102888  60861  52976  35495   220    16  33177  116283  35859   \n",
       "attention_mask       1      1      1      1     1     1      1       1      1   \n",
       "labels            -100   -100   -100   -100  -100  -100   -100    -100   -100   \n",
       "\n",
       "                   315   316     317     318     319     320     321     322  \\\n",
       "token               �다     .      부산       울       란       바       토       르   \n",
       "input_ids       104828    13  118089  111535  103272  101974  101665  100968   \n",
       "attention_mask       1     1       1       1       1       1       1       1   \n",
       "labels            -100  -100    -100    -100    -100    -100    -100    -100   \n",
       "\n",
       "                   323     324    325     326    327     328    329    330  \\\n",
       "token                노       선      은       김      해       국      제      공   \n",
       "input_ids       102058  101151  34804  102155  34983  100654  38187  79225   \n",
       "attention_mask       1       1      1       1      1       1      1      1   \n",
       "labels            -100    -100   -100    -100   -100    -100   -100   -100   \n",
       "\n",
       "                   331    332    333    334   335   336    337   338   339  \\\n",
       "token                항     에서      오      전           8      시          25   \n",
       "input_ids       103866  57575  74177  66965   220    23  30426   220   914   \n",
       "attention_mask       1      1      1      1     1     1      1     1     1   \n",
       "labels            -100   -100   -100   -100  -100  -100   -100  -100  -100   \n",
       "\n",
       "                  340    341     342     343    344     345    346     347  \\\n",
       "token               분      에       출       발      해       현      지       공   \n",
       "input_ids       80816  19954  102722  102133  34983  103055  22035  100994   \n",
       "attention_mask      1      1       1       1      1       1      1       1   \n",
       "labels           -100   -100    -100    -100   -100    -100   -100    -100   \n",
       "\n",
       "                   348    349    350    351   352   353    354   355   356  \\\n",
       "token                항      에      오      전          11      시          40   \n",
       "input_ids       103866  19954  74177  66965   220   806  30426   220  1272   \n",
       "attention_mask       1      1      1      1     1     1      1     1     1   \n",
       "labels            -100   -100   -100   -100  -100  -100   -100  -100  -100   \n",
       "\n",
       "                  357     358     359     360     361     362     363    364  \\\n",
       "token               분       도       착      하고       귀       국       편      은   \n",
       "input_ids       80816  101703  111283  101360  110946  100654  104790  34804   \n",
       "attention_mask      1       1       1       1       1       1       1      1   \n",
       "labels           -100    -100    -100    -100    -100    -100    -100   -100   \n",
       "\n",
       "                   365   366   367     368     369     370    371     372  \\\n",
       "token               오후           1      시에       출       발      해       김   \n",
       "input_ids       124467   220    16  118472  102722  102133  34983  102155   \n",
       "attention_mask       1     1     1       1       1       1      1       1   \n",
       "labels            -100  -100  -100    -100    -100    -100   -100    -100   \n",
       "\n",
       "                  373    374     375    376     377   378   379    380   381  \\\n",
       "token               해      공       항      에      오후           5      시         \n",
       "input_ids       34983  79225  103866  19954  124467   220    20  30426   220   \n",
       "attention_mask      1      1       1      1       1     1     1      1     1   \n",
       "labels           -100   -100    -100   -100    -100  -100  -100   -100  -100   \n",
       "\n",
       "                 382    383     384     385    386     387    388    389  \\\n",
       "token             30      분       도       착     하는      일정     으로      주   \n",
       "input_ids        966  80816  101703  111283  44005  125274  43139  56773   \n",
       "attention_mask     1      1       1       1      1       1      1      1   \n",
       "labels          -100   -100    -100    -100   -100    -100   -100   -100   \n",
       "\n",
       "                 390   391    392     393     394    395   396     397  \\\n",
       "token                    2      회       운       항     한다     .       몽   \n",
       "input_ids        220    17  62841  103678  103866  52976    13  127385   \n",
       "attention_mask     1     1      1       1       1      1     1       1   \n",
       "labels          -100  -100   -100    -100    -100   -100  -100    -100   \n",
       "\n",
       "                   398    399    400     401    402     403   404    405  \\\n",
       "token                골      은      입       국      시     코로나    19      검   \n",
       "input_ids       112542  34804  39250  100654  45618  124141   777  86422   \n",
       "attention_mask       1      1      1       1      1       1     1      1   \n",
       "labels            -100   -100   -100    -100   -100    -100  -100   -100   \n",
       "\n",
       "                  406    407     408    409     410     411    412    413  \\\n",
       "token               사      와       백      신       접       종      여      부   \n",
       "input_ids       56154  81673  107696  83628  108712  102757  84618  64189   \n",
       "attention_mask      1      1       1      1       1       1      1      1   \n",
       "labels           -100   -100    -100   -100    -100    -100   -100   -100   \n",
       "\n",
       "                  414    415    416    417    418     419     420    421  \\\n",
       "token               를     확인     하지      않      아      자유       롭      게   \n",
       "input_ids       18918  74959  88525  51796  54059  117542  120591  58901   \n",
       "attention_mask      1      1      1      1      1       1       1      1   \n",
       "labels           -100   -100   -100   -100   -100    -100    -100   -100   \n",
       "\n",
       "                   422    423     424     425    426   427     428    429  \\\n",
       "token               여행      이     가능한      국가      다     .      부산      오   \n",
       "input_ids       121528  13094  125502  109916  13447    13  118089  74177   \n",
       "attention_mask       1      1       1       1      1     1       1      1   \n",
       "labels            -100   -100    -100    -100   -100  -100    -100   -100   \n",
       "\n",
       "                  430     431     432     433    434     435    436    437  \\\n",
       "token               사       카       노       선      은       김      해      공   \n",
       "input_ids       56154  101436  102058  101151  34804  102155  34983  79225   \n",
       "attention_mask      1       1       1       1      1       1      1      1   \n",
       "labels           -100    -100    -100    -100   -100    -100   -100   -100   \n",
       "\n",
       "                   438    439    440    441   442   443    444   445   446  \\\n",
       "token                항     에서      오      전           8      시          35   \n",
       "input_ids       103866  57575  74177  66965   220    23  30426   220  1758   \n",
       "attention_mask       1      1      1      1     1     1      1     1     1   \n",
       "labels            -100   -100   -100   -100  -100  -100   -100  -100  -100   \n",
       "\n",
       "                  447    448     449     450    451     452     453    454  \\\n",
       "token               분      에       출       발      해       간      사이      공   \n",
       "input_ids       80816  19954  102722  102133  34983  105131  125166  79225   \n",
       "attention_mask      1      1       1       1      1       1       1      1   \n",
       "labels           -100   -100    -100    -100   -100    -100    -100   -100   \n",
       "\n",
       "                   455    456    457    458   459   460    461     462  \\\n",
       "token                항      에      오      전          10      시       도   \n",
       "input_ids       103866  19954  74177  66965   220   605  30426  101703   \n",
       "attention_mask       1      1      1      1     1     1      1       1   \n",
       "labels            -100   -100   -100   -100  -100  -100   -100    -100   \n",
       "\n",
       "                   463     464     465     466    467     468     469    470  \\\n",
       "token                착       귀       국       편      은       간      사이      공   \n",
       "input_ids       111283  110946  100654  104790  34804  105131  125166  79225   \n",
       "attention_mask       1       1       1       1      1       1       1      1   \n",
       "labels            -100    -100    -100    -100   -100    -100    -100   -100   \n",
       "\n",
       "                   471    472     473   474   475     476     477     478  \\\n",
       "token                항     에서       낮          12      시에       출       발   \n",
       "input_ids       103866  57575  120889   220   717  118472  102722  102133   \n",
       "attention_mask       1      1       1     1     1       1       1       1   \n",
       "labels            -100   -100    -100  -100  -100    -100    -100    -100   \n",
       "\n",
       "                  479     480    481    482     483    484     485   486  \\\n",
       "token               해       김      해      공       항      에      오후         \n",
       "input_ids       34983  102155  34983  79225  103866  19954  124467   220   \n",
       "attention_mask      1       1      1      1       1      1       1     1   \n",
       "labels           -100    -100   -100   -100    -100   -100    -100  -100   \n",
       "\n",
       "                 487    488   489   490    491     492     493    494     495  \\\n",
       "token              1      시          30      분       도       착     하는      일정   \n",
       "input_ids         16  30426   220   966  80816  101703  111283  44005  125274   \n",
       "attention_mask     1      1     1     1      1       1       1      1       1   \n",
       "labels          -100   -100  -100  -100   -100    -100    -100   -100    -100   \n",
       "\n",
       "                  496    497   498   499    500     501     502    503   504  \\\n",
       "token              으로      주           2      회       운       항     한다     .   \n",
       "input_ids       43139  56773   220    17  62841  103678  103866  52976    13   \n",
       "attention_mask      1      1     1     1      1       1       1      1     1   \n",
       "labels           -100   -100  -100  -100   -100    -100    -100   -100  -100   \n",
       "\n",
       "                  505    506    507    508     509     510     511     512  \\\n",
       "token               에      어      부      산      관계      자는      이번       증   \n",
       "input_ids       91586  32179  64189  86157  116680  112953  117717  107034   \n",
       "attention_mask      1      1      1      1       1       1       1       1   \n",
       "labels           -100   -100   -100   -100    -100    -100    -100    -100   \n",
       "\n",
       "                   513    514     515    516    517    518     519    520  \\\n",
       "token                편      은       무      비      자      입       국      이   \n",
       "input_ids       104790  34804  101480  71682  26799  39250  100654  13094   \n",
       "attention_mask       1      1       1      1      1      1       1      1   \n",
       "labels            -100   -100    -100   -100   -100   -100    -100   -100   \n",
       "\n",
       "                   521    522    523   524    525    526     527     528  \\\n",
       "token                복      원      �     �      을      때       폭       발   \n",
       "input_ids       107067  55421  33943   238  18359  54718  115062  102133   \n",
       "attention_mask       1      1      1     1      1      1       1       1   \n",
       "labels            -100   -100   -100  -100   -100   -100    -100    -100   \n",
       "\n",
       "                  529     530    531    532    533     534     535    536  \\\n",
       "token              하는      여행      수      요      를       선       점     하기   \n",
       "input_ids       44005  121528  29833  36811  18918  101585  101838  67525   \n",
       "attention_mask      1       1      1      1      1       1       1      1   \n",
       "labels           -100    -100   -100   -100   -100    -100    -100   -100   \n",
       "\n",
       "                   537     538    539    540    541    542     543    544  \\\n",
       "token               위한       선      제      적      조      치       라      고   \n",
       "input_ids       107472  101585  38187  82068  66610  60798  103959  35495   \n",
       "attention_mask       1       1      1      1      1      1       1      1   \n",
       "labels            -100    -100   -100   -100   -100   -100    -100   -100   \n",
       "\n",
       "                   545    546     547   548    549    550   551     552  \\\n",
       "token                밝      �      �다     .      이      �     �      에도   \n",
       "input_ids       116283  35859  104828    13  23955  39277   244  109018   \n",
       "attention_mask       1      1       1     1      1      1     1       1   \n",
       "labels            -100   -100    -100  -100   -100   -100  -100    -100   \n",
       "\n",
       "                  553    554    555    556    557    558     559    560  \\\n",
       "token               에      어      부      산      은      이       달      중   \n",
       "input_ids       91586  32179  64189  86157  34804  23955  104684  72043   \n",
       "attention_mask      1      1      1      1      1      1       1      1   \n",
       "labels           -100   -100   -100   -100   -100   -100    -100   -100   \n",
       "\n",
       "                   561    562     563     564    565     566     567     568  \\\n",
       "token               부산     에서       출       발     하는       코       타       키   \n",
       "input_ids       118089  57575  102722  102133  44005  103651  101109  102474   \n",
       "attention_mask       1      1       1       1      1       1       1       1   \n",
       "labels            -100   -100    -100    -100   -100    -100    -100    -100   \n",
       "\n",
       "                  569     570     571    572    573     574     575    576  \\\n",
       "token               나       발       루      나      트       랑       세      부   \n",
       "input_ids       61415  102133  102268  74618  29726  102581  101852  64189   \n",
       "attention_mask      1       1       1      1      1       1       1      1   \n",
       "labels           -100    -100    -100   -100   -100    -100    -100   -100   \n",
       "\n",
       "                   577     578     579     580     581    582     583     584  \\\n",
       "token                노       선      등을       운       항      할      예정      이다   \n",
       "input_ids       102058  101151  120908  103678  103866  48936  126088  101568   \n",
       "attention_mask       1       1       1       1       1      1       1       1   \n",
       "labels            -100    -100    -100    -100    -100   -100    -100    -100   \n",
       "\n",
       "                 585     586     587    588    589     590     591    592  \\\n",
       "token              .      인천       국      제      공       항     에서는      이   \n",
       "input_ids         13  121772  100654  38187  79225  103866  107031  23955   \n",
       "attention_mask     1       1       1      1      1       1       1      1   \n",
       "labels          -100    -100    -100   -100   -100    -100    -100   -100   \n",
       "\n",
       "                   593   594   595    596     597     598     599     600  \\\n",
       "token                달          14      일      부터       순       차     적으로   \n",
       "input_ids       104684   220   975  33177  103551  106248  101532  104182   \n",
       "attention_mask       1     1     1      1       1       1       1       1   \n",
       "labels            -100  -100  -100   -100    -100    -100    -100    -100   \n",
       "\n",
       "                  601    602   603    604     605   606    607     608    609  \\\n",
       "token               다      �     �      방       �     �      후       쿠      오   \n",
       "input_ids       50467  40275   255  75908  100966   243  95415  107872  58368   \n",
       "attention_mask      1      1     1      1       1     1      1       1      1   \n",
       "labels           -100   -100  -100   -100    -100  -100   -100    -100   -100   \n",
       "\n",
       "                   610    611     612     613     614     615     616    617  \\\n",
       "token                카      등       노      선을      신규       취       항     한다   \n",
       "input_ids       101436  78102  102058  126712  126902  107545  103866  52976   \n",
       "attention_mask       1      1       1       1       1       1       1      1   \n",
       "labels            -100   -100    -100    -100    -100    -100    -100   -100   \n",
       "\n",
       "                 618         619                  620        621  \\\n",
       "token              .  <|eot_id|>  <|start_header_id|>  assistant   \n",
       "input_ids         13      128009               128006      78191   \n",
       "attention_mask     1           1                    1          1   \n",
       "labels          -100        -100                 -100       -100   \n",
       "\n",
       "                              622  623   624    625       626  627   628  \\\n",
       "token           <|end_header_id|>   \\n    {\"  stock  _related   \":  true   \n",
       "input_ids                  128007  198  5018  13787     54356  794  1904   \n",
       "attention_mask                  1    1     1      1         1    1     1   \n",
       "labels                       -100  198  5018  13787     54356  794  1904   \n",
       "\n",
       "                 629      630   631    632    633    634    635    636  \\\n",
       "token             ,\"  summary   \":\"      에      어      부      산      이   \n",
       "input_ids       1359     1743  3332  19954  32179  64189  86157  13094   \n",
       "attention_mask     1        1     1      1      1      1      1      1   \n",
       "labels          1359     1743  3332  19954  32179  64189  86157  13094   \n",
       "\n",
       "                   637  638     639     640    641  642     643  644   645  \\\n",
       "token              코로나   19       팬       데      �    �      이후         28   \n",
       "input_ids       124141  777  124460  100933  50273  117  111323  220  1591   \n",
       "attention_mask       1    1       1       1      1    1       1    1     1   \n",
       "labels          124141  777  124460  100933  50273  117  111323  220  1591   \n",
       "\n",
       "                   646    647    648     649    650     651    652    653  \\\n",
       "token               개월      만      에       김      해       국      제      공   \n",
       "input_ids       125085  63207  19954  102155  34983  100654  38187  79225   \n",
       "attention_mask       1      1      1       1      1       1      1      1   \n",
       "labels          125085  63207  19954  102155  34983  100654  38187  79225   \n",
       "\n",
       "                   654    655     656     657     658     659     660     661  \\\n",
       "token                항     에서       몽       골       울       란       바       토   \n",
       "input_ids       103866  57575  127385  112542  111535  103272  101974  101665   \n",
       "attention_mask       1      1       1       1       1       1       1       1   \n",
       "labels          103866  57575  127385  112542  111535  103272  101974  101665   \n",
       "\n",
       "                   662    663     664    665    666     667     668     669  \\\n",
       "token                르      와      일본      오      사       카       노       선   \n",
       "input_ids       100968  81673  107715  74177  56154  101436  102058  101151   \n",
       "attention_mask       1      1       1      1      1       1       1       1   \n",
       "labels          100968  81673  107715  74177  56154  101436  102058  101151   \n",
       "\n",
       "                   670     671    672    673  674 675    676     677    678  \\\n",
       "token                운       항      을      주        2      회       재      개   \n",
       "input_ids       103678  103866  18359  56773  220  17  62841  102888  60861   \n",
       "attention_mask       1       1      1      1    1   1      1       1      1   \n",
       "labels          103678  103866  18359  56773  220  17  62841  102888  60861   \n",
       "\n",
       "                   679 680     681     682     683     684    685     686  \\\n",
       "token               하며   ,       곧       코       타       키      나       발   \n",
       "input_ids       108859  11  124389  103651  101109  102474  61415  102133   \n",
       "attention_mask       1   1       1       1       1       1      1       1   \n",
       "labels          108859  11  124389  103651  101109  102474  61415  102133   \n",
       "\n",
       "                   687 688    689    690     691 692     693    694    695  \\\n",
       "token                루   ,      나      트       랑   ,       세      부      등   \n",
       "input_ids       102268  11  74618  29726  102581  11  101852  64189  78102   \n",
       "attention_mask       1   1      1      1       1   1       1      1      1   \n",
       "labels          102268  11  74618  29726  102581  11  101852  64189  78102   \n",
       "\n",
       "                   696     697    698     699     700    701     702     703  \\\n",
       "token                동       남      아       노       선      도       운       항   \n",
       "input_ids       101604  101963  54059  102058  101151  49085  103678  103866   \n",
       "attention_mask       1       1      1       1       1      1       1       1   \n",
       "labels          101604  101963  54059  102058  101151  49085  103678  103866   \n",
       "\n",
       "                  704    705    706 707     708    709     710     711  \\\n",
       "token               을     시작     한다   .      인천      공       항       발   \n",
       "input_ids       18359  94821  52976  13  121772  79225  103866  102133   \n",
       "attention_mask      1      1      1   1       1      1       1       1   \n",
       "labels          18359  94821  52976  13  121772  79225  103866  102133   \n",
       "\n",
       "                   712     713     714     715    716    717    718    719  \\\n",
       "token               국제       노       선       확      대      도      예      고   \n",
       "input_ids       115878  102058  101151  103686  67945  49085  96717  35495   \n",
       "attention_mask       1       1       1       1      1      1      1      1   \n",
       "labels          115878  102058  101151  103686  67945  49085  96717  35495   \n",
       "\n",
       "                   720 721     722     723    724     725     726    727  \\\n",
       "token               했다   .       몽       골      의       무       격      리   \n",
       "input_ids       101528  13  127385  112542  21028  101480  102079  29102   \n",
       "attention_mask       1   1       1       1      1       1       1      1   \n",
       "labels          101528  13  127385  112542  21028  101480  102079  29102   \n",
       "\n",
       "                  728     729     730    731    732     733     734 735  \\\n",
       "token               ·       무       백      신      입       국      정책   ,   \n",
       "input_ids       14260  100981  106113  83628  39250  100654  126950  11   \n",
       "attention_mask      1       1       1      1      1       1       1   1   \n",
       "labels          14260  100981  106113  83628  39250  100654  126950  11   \n",
       "\n",
       "                   736    737    738     739     740     741    742     743  \\\n",
       "token               일본      오      사       카       노       선      등      해외   \n",
       "input_ids       107715  74177  56154  101436  102058  101151  78102  123102   \n",
       "attention_mask       1      1      1       1       1       1      1       1   \n",
       "labels          107715  74177  56154  101436  102058  101151  78102  123102   \n",
       "\n",
       "                  744     745    746    747     748     749     750    751  \\\n",
       "token               여       행      수      요       증      대를       선      제   \n",
       "input_ids       58126  101066  29833  36811  107034  124784  101585  38187   \n",
       "attention_mask      1       1      1      1       1       1       1      1   \n",
       "labels          58126  101066  29833  36811  107034  124784  101585  38187   \n",
       "\n",
       "                   752     753     754    755    756    757     758    759  \\\n",
       "token              적으로       겨       냥      한      조      치      이다   .\",\"   \n",
       "input_ids       104182  122358  112012  24486  66610  60798  101568  48991   \n",
       "attention_mask       1       1       1      1      1      1       1      1   \n",
       "labels          104182  122358  112012  24486  66610  60798  101568  48991   \n",
       "\n",
       "                     760   761    762    763    764    765    766    767  \\\n",
       "token           positive   _st   ocks   \":[\"      에      어      부      산   \n",
       "input_ids          31587  1284  26246  37899  19954  32179  64189  86157   \n",
       "attention_mask         1     1      1      1      1      1      1      1   \n",
       "labels             31587  1284  26246  37899  19954  32179  64189  86157   \n",
       "\n",
       "                  768       769        770    771     772    773     774  \\\n",
       "token            \"],\"  positive  _keywords   \":[\"       국      제       선   \n",
       "input_ids       69982     31587      52454  37899  100654  38187  101151   \n",
       "attention_mask      1         1          1      1       1      1       1   \n",
       "labels          69982     31587      52454  37899  100654  38187  101151   \n",
       "\n",
       "                   775     776     777    778   779    780     781    782  \\\n",
       "token                운       항       재      개   \",\"      해       외      여   \n",
       "input_ids       103678  103866  102888  60861  2247  34983  104065  58126   \n",
       "attention_mask       1       1       1      1     1      1       1      1   \n",
       "labels          103678  103866  102888  60861  2247  34983  104065  58126   \n",
       "\n",
       "                   783    784    785     786    787   788    789     790  \\\n",
       "token                행      수      요       확      대   \",\"      동       남   \n",
       "input_ids       101066  29833  36811  103686  67945  2247  58189  101963   \n",
       "attention_mask       1      1      1       1      1     1      1       1   \n",
       "labels          101066  29833  36811  103686  67945  2247  58189  101963   \n",
       "\n",
       "                  791    792     793     794     795     796    797   798  \\\n",
       "token               아      ·      일본       노       선       확      대   \",\"   \n",
       "input_ids       54059  14260  123256  102058  101151  103686  67945  2247   \n",
       "attention_mask      1      1       1       1       1       1      1     1   \n",
       "labels          54059  14260  123256  102058  101151  103686  67945  2247   \n",
       "\n",
       "                   799     800     801     802     803     804    805    806  \\\n",
       "token                코      로나      이후       노       선       복      원   \"],\"   \n",
       "input_ids       102525  117465  111323  102058  101151  107067  55421  69982   \n",
       "attention_mask       1       1       1       1       1       1      1      1   \n",
       "labels          102525  117465  111323  102058  101151  107067  55421  69982   \n",
       "\n",
       "                     807      808 809   810    811  812     813    814  815  \\\n",
       "token           positive  _reason   s   \":\"      �    �       데      �    �   \n",
       "input_ids          31587    39329  82  3332  67218  105  100933  50273  117   \n",
       "attention_mask         1        1   1     1      1    1       1      1    1   \n",
       "labels             31587    39329  82  3332  67218  105  100933  50273  117   \n",
       "\n",
       "                   816    817  818     819  820     821     822     823  \\\n",
       "token               이후      �    �       �    �       렸       던      해외   \n",
       "input_ids       111323  80402  113  105940  234  105543  101954  123102   \n",
       "attention_mask       1      1    1       1    1       1       1       1   \n",
       "labels          111323  80402  113  105940  234  105543  101954  123102   \n",
       "\n",
       "                  824     825    826    827     828     829    830    831 832  \\\n",
       "token               여       행      수      요       급       증      기      대   ,   \n",
       "input_ids       58126  101066  29833  36811  117208  102249  55216  67945  11   \n",
       "attention_mask      1       1      1      1       1       1      1      1   1   \n",
       "labels          58126  101066  29833  36811  117208  102249  55216  67945  11   \n",
       "\n",
       "                   833     834     835     836     837    838     839     840  \\\n",
       "token               국제       선       노       선       재      개       및       증   \n",
       "input_ids       115878  101151  102058  101151  102888  60861  101824  107034   \n",
       "attention_mask       1       1       1       1       1      1       1       1   \n",
       "labels          115878  101151  102058  101151  102888  60861  101824  107034   \n",
       "\n",
       "                   841    842    843    844    845    846    847     848  \\\n",
       "token                편     으로      에      어      부      산      의       항   \n",
       "input_ids       104790  43139  91586  32179  64189  86157  21028  107744   \n",
       "attention_mask       1      1      1      1      1      1      1       1   \n",
       "labels          104790  43139  91586  32179  64189  86157  21028  107744   \n",
       "\n",
       "                  849     850     851    852     853     854    855    856  \\\n",
       "token               공       노       선      수       익       확      대      가   \n",
       "input_ids       79225  102058  101151  29833  108964  103686  67945  20565   \n",
       "attention_mask      1       1       1      1       1       1      1      1   \n",
       "labels          79225  102058  101151  29833  108964  103686  67945  20565   \n",
       "\n",
       "                  857    858     859    860  861    862    863     864  \\\n",
       "token               예      상      되어      �    �      정      적     영향을   \n",
       "input_ids       96717  57002  106910  41871  235  30381  82068  126652   \n",
       "attention_mask      1      1       1      1    1      1      1       1   \n",
       "labels          96717  57002  106910  41871  235  30381  82068  126652   \n",
       "\n",
       "                   865    866     867 868     869     870 871     872     873  \\\n",
       "token                줄      수      있음   .      특히      일본   ,       동       남   \n",
       "input_ids       109720  29833  127406  13  125578  107715  11  101604  101963   \n",
       "attention_mask       1      1       1   1       1       1   1       1       1   \n",
       "labels          109720  29833  127406  13  125578  107715  11  101604  101963   \n",
       "\n",
       "                  874 875     876     877    878     879     880     881  \\\n",
       "token               아   ,       몽       골      등      인기      지역       노   \n",
       "input_ids       54059  11  127385  112542  78102  115613  109299  102058   \n",
       "attention_mask      1   1       1       1      1       1       1       1   \n",
       "labels          54059  11  127385  112542  78102  115613  109299  102058   \n",
       "\n",
       "                   882    883    884    885     886    887    888     889  \\\n",
       "token                선      의      조      기       복      원      은       매   \n",
       "input_ids       101151  21028  66610  21121  107067  55421  34804  102293   \n",
       "attention_mask       1      1      1      1       1      1      1       1   \n",
       "labels          101151  21028  66610  21121  107067  55421  34804  102293   \n",
       "\n",
       "                  890     891    892     893    894     895    896     897  \\\n",
       "token               출      증가      로       직      결       될     가능      성이   \n",
       "input_ids       71023  122862  17835  105164  89881  113191  96451  115602   \n",
       "attention_mask      1       1      1       1      1       1      1       1   \n",
       "labels          71023  122862  17835  105164  89881  113191  96451  115602   \n",
       "\n",
       "                   898    899    900       901   902    903   904    905  \\\n",
       "token                높      음   .\",\"  negative   _st   ocks   \":[    ],\"   \n",
       "input_ids       108499  49531  48991     43324  1284  26246  9075  29603   \n",
       "attention_mask       1      1      1         1     1      1     1      1   \n",
       "labels          108499  49531  48991     43324  1284  26246  9075  29603   \n",
       "\n",
       "                     906        907   908    909       910      911 912  \\\n",
       "token           negative  _keywords   \":[    ],\"  negative  _reason   s   \n",
       "input_ids          43324      52454  9075  29603     43324    39329  82   \n",
       "attention_mask         1          1     1      1         1        1   1   \n",
       "labels             43324      52454  9075  29603     43324    39329  82   \n",
       "\n",
       "                  913 914         915  \n",
       "token            \":\"\"   }  <|eot_id|>  \n",
       "input_ids       63466  92      128009  \n",
       "attention_mask      1   1           1  \n",
       "labels          63466  92      128009  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'token': text_tokens,  # 토큰 (1개씩 디코딩된 문자열)\n",
    "    'input_ids': batch['input_ids'][0].tolist(),  # 입력 토큰 ID\n",
    "    'attention_mask': batch['attention_mask'][0].tolist(),  # 패딩 여부(1/0)\n",
    "    'labels': batch['labels'][0].tolist()  # 정답 라벨(-100 마스킹 포함)\n",
    "}).transpose()  # 컬럼을 행으로 전치\n",
    "\n",
    "pd.set_option('display.max_columns', None)  # 출력시 컬럼 생략 없이 표시\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a922eb",
   "metadata": {},
   "source": [
    "## PEFT Finetuning - LoRA\n",
    "\n",
    "* LoRA는 **\"Low-Rank Adapter(저랭크 어댑터)\"**\n",
    "* 거대한 대형언어모델(LLM)의 **전체 파라미터를 일일이 미세조정(파인튜닝)하지 않고**,\n",
    "  **딱 필요한 핵심 부분만 저렴하게 빠르게 학습**하는 최신 파인튜닝.\n",
    "* **\"LLM의 성능은 그대로, 비용/시간/메모리/유지보수는 최소로\"** 파인튜닝을 할 수 있게 해주는 AI 실무에서 가장 중요한 기법 중 하나이다.\n",
    "\n",
    "**왜 LoRA가 등장했을까?**\n",
    "\n",
    "* GPT, Llama, DeepSeek 같은 대형언어모델은 **파라미터(매개변수) 수가 수십억\\~수조 개**나 된다.\n",
    "* 이런 모델을 파인튜닝하려면 **막대한 GPU 메모리와 시간, 저장 공간**이 필요.\n",
    "* 하지만, 실제로 특정 태스크에 맞게 모델을 조정할 때 **전체를 다 바꿀 필요가 없다.**\n",
    "* 대부분의 정보는 기존 모델에 이미 들어있고,\n",
    "  **특정 입력(질문)과 특정 출력(답변)의 관계만 살짝 조정**해주면 충분하다.\n",
    "\n",
    "**LoRA의 원리**\n",
    "\n",
    "* 기존 대형 모델의 핵심 연산(주로 \"곱셈\" 부분)에\n",
    "  **작고 얇은 \"보조 네트워크(어댑터 레이어)\"**를 덧붙인다.\n",
    "* 전체 모델은 거의 건드리지 않고,\n",
    "  **이 어댑터 레이어의 파라미터만 새로 추가해서 학습**\n",
    "* 학습이 끝나면,\n",
    "\n",
    "  * 원본 모델은 그대로\n",
    "  * 어댑터(작은 추가 파라미터)만 별도로 저장하면 끝!\n",
    "* 추론할 땐 **원본 모델 + LoRA 어댑터**를 합쳐서 쓸 수 있다.\n",
    "\n",
    "**LoRA의 장점**\n",
    "\n",
    "* **파인튜닝 비용(시간, 메모리, 저장 용량)이 압도적으로 절약**된다.\n",
    "* 7B, 13B, 70B 등 대형 모델도\n",
    "  **일반 GPU(24GB/48GB)로도 쉽게 파인튜닝**이 가능하다.\n",
    "* **동일한 원본 모델에 다양한 LoRA 어댑터만 바꿔 끼우며\n",
    "  다양한 분야별 파인튜닝 결과를 쉽게 쓸 수 있다.**\n",
    "\n",
    "**LoRA와 기존 방식의 비교**\n",
    "\n",
    "* **기존 파인튜닝:**\n",
    "  전체 파라미터(수십\\~수백 GB)를 새로 저장/관리/학습 → 비효율적\n",
    "* **LoRA:**\n",
    "  원본은 그대로 두고,\n",
    "  변화가 필요한 부분(수 MB\\~수십 MB)만 별도로 학습/저장\n",
    "\n",
    "\n",
    "**실전에서의 활용 예시**\n",
    "\n",
    "* 번역 LoRA, 요약 LoRA, 감정분석 LoRA 등\n",
    "  **하나의 원본 모델에 여러 용도별 어댑터를 저장/관리**할 수 있다.\n",
    "* **A100 80GB, 3090, T4 등 다양한 GPU 환경에서도\n",
    "  고성능 LLM 튜닝이 매우 쉽게 가능하다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b4ac57c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model  # LoRA 설정 / PEFT 적용 함수\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r = 8,               # 저랭크 행렬 rank\n",
    "    lora_alpha = 32,     # LoRA 스케일 계수 (alpha/r로 적용 강도 결정)\n",
    "    lora_dropout = 0.1,  # 드롭아웃 비율\n",
    "    bias = 'none',       # bais 학습 안함\n",
    "    target_modules = ['q_proj', 'v_proj'],  # LoRA를 주입할 모듈 (Q/V projection)\n",
    "    task_type = 'CAUSAL_LM'  # 작업 유형(생성형 언어모델)\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)  # 기존 모델에 LoRA 어댑터 적용\n",
    "model.print_trainable_parameters()          # 학습 가능한 파라미터 수/비율 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2aff5d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warmup_ratio is deprecated and will be removed in v5.2. Use `warmup_steps` instead.\n"
     ]
    }
   ],
   "source": [
    "# SFTConfig\n",
    "from trl import SFTConfig  # TRL SFT 학습 설정 클래스\n",
    "\n",
    "hub_model_id = 'capybaraOh/Llama-VARCO-8b-news2stock-analyzer-4bit'  # 학습 완료 후 업로드할 Hub 모델 ID\n",
    "\n",
    "sft_config = SFTConfig(  # SFT 학습 하이퍼파라미터/저장/로그 설정\n",
    "    output_dir=\"Llama-VARCO-8b-news2stock-analyzer-4bit\", # 학습 완료된 모델과 체크포인트가 저장될 경로이다.\n",
    "    num_train_epochs=3,                              # 전체 데이터셋을 반복 학습할 횟수(Epoch)이다.\n",
    "    per_device_train_batch_size=2,                   # 각 GPU(장치)당 한 번에 처리할 데이터 샘플의 개수이다.\n",
    "    gradient_accumulation_steps=2,                   # 그래디언트를 2번 누적한 후 가중치를 업데이트한다. (실제 배치 크기 = 2 * 2 = 4 효과를 낸다.)\n",
    "    gradient_checkpointing=True,                     # VRAM 절약을 위해 중간 활성화 값을 저장하지 않고 역전파 시 재계산하는 설정이다.\n",
    "    optim=\"adamw_torch_fused\",                       # 최적화 알고리즘 설정이다. fused 버전은 CUDA에서 더 빠르다.\n",
    "    logging_steps=10,                                # 10 스텝마다 학습 로그(Loss 등)를 출력한다.\n",
    "    save_strategy=\"steps\",                           # 체크포인트 저장 기준을 'steps'(스텝 수)로 설정한다. (옵션: 'epoch')\n",
    "    save_steps=100,                                  # 100 스텝마다 모델 체크포인트를 저장한다.\n",
    "    bf16=True,                                       # BF16(Brain Float 16) 정밀도를 사용하여 메모리를 아끼고 연산 속도를 높인다. (Ampere GPU 이상 권장)\n",
    "    learning_rate=1e-4,                              # 학습률(Learning Rate)이다. 가중치 업데이트의 크기를 결정한다.\n",
    "    max_grad_norm=0.3,                               # 그래디언트 클리핑 임계값이다. 그래디언트 폭주를 막아 학습 안정성을 높인다.\n",
    "    warmup_ratio=0.03,                               # 전체 학습 단계의 3% 동안 학습률을 서서히 올리는 웜업(Warmup)을 수행한다.\n",
    "    lr_scheduler_type=\"constant\",                    # 학습률 스케줄러 타입이다. 여기서는 학습률을 변동 없이 상수로 유지한다.\n",
    "    push_to_hub=True,                                # 학습이 끝나면 Hugging Face Hub에 모델을 자동으로 업로드한다.\n",
    "    hub_model_id=hub_model_id,                       # Hub에 업로드될 때 사용될 저장소(Repository) ID이다.\n",
    "    hub_token=True,                                  # Hub 업로드를 위해 인증 토큰을 사용한다.\n",
    "    remove_unused_columns=False,                     # 데이터셋에서 모델의 forward 메서드 시그니처에 없는 컬럼을 자동으로 삭제하지 않도록 한다.\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True},   # 데이터셋 처리 과정(packing 등)을 건너뛰도록 하는 설정이다.\n",
    "    report_to=['wandb'],                             # 학습 기록을 전송할 툴(WandB, Tensorboard 등)을 지정한다. 빈 리스트는 기록하지 않음을 의미한다.\n",
    "    label_names=[\"labels\"],                          # 손실(Loss) 계산 시 정답(Target)으로 사용할 데이터셋의 컬럼 이름이다.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "489d74e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': 0}.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into https://api.wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Create a new API key at: https://wandb.ai/authorize?ref=models\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Store your API key securely and do not share it.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste your API key and hit enter:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcapybara-ohgiraffers\u001b[0m (\u001b[33mcapybara-ohgiraffers-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.24.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20260212_032838-aqofizzu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/capybara-ohgiraffers-/huggingface/runs/aqofizzu' target=\"_blank\">winter-spaceship-1</a></strong> to <a href='https://wandb.ai/capybara-ohgiraffers-/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/capybara-ohgiraffers-/huggingface' target=\"_blank\">https://wandb.ai/capybara-ohgiraffers-/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/capybara-ohgiraffers-/huggingface/runs/aqofizzu' target=\"_blank\">https://wandb.ai/capybara-ohgiraffers-/huggingface/runs/aqofizzu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='600' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [600/600 20:16, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.546736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.192319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.191058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.111726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.062651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.125188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.107885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.070426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.015354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.091138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.040715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.042363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.112570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.991856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.072713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.038031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.013076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.051406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.986301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.984007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.971738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.972485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>1.016071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.946921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.992240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.995082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>1.004292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.904462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.979731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.932436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>1.011318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.952906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.938268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.948312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.921359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.947583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.948380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.901548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.954061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.963545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.883766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.909651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.907864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.902767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.902679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.868322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.865687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.841246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.860808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.866550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.891152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.883888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.895307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.938735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.924048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.887327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.893537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.886966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>0.919671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.911109</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=600, training_loss=0.9815223137537639, metrics={'train_runtime': 1671.0496, 'train_samples_per_second': 1.436, 'train_steps_per_second': 0.359, 'total_flos': 1.6049502302753587e+17, 'train_loss': 0.9815223137537639})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from trl import SFTTrainer  # SFT 학습용 Trainer\n",
    "\n",
    "# SFT 학습 객체 생성\n",
    "trainer = SFTTrainer(\n",
    "    model = model,                  # 학습할(LoRA 적용된) 모델\n",
    "    args = sft_config,              # SFT 학습 설정\n",
    "    train_dataset = train_dataset,  # 학습 데이터셋\n",
    "    data_collator = data_collator   # 배치 텐서 생성 함수\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b9111d",
   "metadata": {},
   "source": [
    "## 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "68fe14bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트셋 messages에서 프롬프트/정답(assistant) 텍스트 분리\n",
    "prompt_list = []   # 프롬프트(assistant메시지 이전) 저장 리스트\n",
    "label_list = []    # 정답 (assistant 답변) 저장 리스트\n",
    "\n",
    "for messages in test_dataset['messages']:\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)  # 채팅 템플릿 문자열로 변환\n",
    "    # assistant 시작 전까지는 프롬프트로 사용 + assistant 헤더만 프롬프트 끝에 붙여준다.\n",
    "    input = text.split('<|start_header_id|>assistant<|end_header_id|>\\n')[0] + \\\n",
    "        '<|start_header_id|>assistant<|end_header_id|>\\n'\n",
    "    # assistant 답변(종료 토큰 전)만 추출\n",
    "    label = text.split('<|start_header_id|>assistant<|end_header_id|>\\n')[1].split('<|eot_id|>')[0]\n",
    "    \n",
    "    prompt_list.append(input)\n",
    "    label_list.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0b2433a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n당신은 금융/경제 뉴스의 핵심내용을 요약해 설명하고, \\n특정 상장 종목에 미치는 긍정/부정 영향여부, 이유, 근거를 분석하는 금융/경제 분석 전문가입니다.\\n\\n다음 출력지시사항을 지켜주세요.\\n1. 뉴스와 종목간의 연관성을 발견할 수 없다면:\\n    - stock_related를 False로 작성하세요.\\n    - summary에 뉴스의 요약을 작성하세요.\\n2. 뉴스와 종목간의 연관성을 발견했다면:\\n    - stock_related를 True로 작성하세요.\\n    - summary에 뉴스의 요약을 작성하세요.\\n    - 긍정영향이 예상되는 종목이 있다면, positive_stocks, positive_keywords, positive_reasons를 작성하세요.\\n    - 부정영향이 예상되는 종목이 있다면, negative_stocks, negative_keywords, negative_reasons를 작성하세요.\\n    - 값이 없는 경우 빈 문자열(''), 빈 리스트([])로 작성하세요.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n공유수면 사용하려면 어입인 의견 들어야\\n해수부 5일 개정 공유수면 관리 및 매립에 관한 법률 시행 헤럴드경제 홍태화 기자 앞으로 공유수면관리청이 어업·환경 등에 영향을 미칠 것으로 예상되는 공유수면 점용·사용 허가를 할 때 미리 어업인 등 이해관계자들의 의견을 들어야 한다. 해양수산부는 5일 이같은 내용이 담긴 개정 공유수면 관리 및 매립에 관한 법률과 같은 법 시행령·시행규칙이 이날부터 시행된다고 밝혔다. 바다·바닷가·하천 등 공유수면은 공유재이기 때문에 이를 점용·사용하기 위해서는 별도의 허가를 받아야 한다. 최근 해상풍력 발전시설 해변을 이용한 관광시설 등 대규모 시설이 공유수면을 장기적으로 점용·사용하는 경우가 늘어났지만 이해 관계자의 의견을 사전에 수렴할 수 없는 문제가 있었다. 이에 공유수면 점용·사용으로 인한 사회적 갈등이 증가했다. 이러한 문제를 해결하기 위해 해수부는 지난 1월 공유수면 점용·사용 허가를 할 때 이해관계자의 의견을 듣도록 공유수면 관리 및 매립에 관한 법률을 개정했다. 법 개정에 따라 공유수면관리청이 해양환경·수산자원·자연경관 보호 등에 영향을 끼칠 수 있는 공유수면 점용·사용 신청을 받은 경우 이를 관보 공보 와 인터넷 홈페이지에 공고해야 한다. 또 점용·사용 허가를 했을 때 피해를 볼 것으로 예상되는 어업인에 대한 의견 조사도 별도로 진행해야 한다. 황준성 해수부 해양공간정책과장은 공유수면 점용·사용으로 인한 이해 관계자의 피해를 방지하려는 법령 개정의 취지를 달성할 수 있도록 각 공유수면관리청과 협력해 관련 제도의 차질 없는 운영을 지원하겠다 고 말했다.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_list[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bb37277f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n{\"stock_related\":true,\"summary\":\"해양수산부가 공유수면 관리 및 매립에 관한 법률을 개정해, 해상풍력·관광시설 등 공유수면(바다·강 등) 점용·사용 허가 시 어업인 등 이해관계자 의견을 반드시 수렴하도록 의무화했다. 이는 공유수면 사용으로 인한 사회적 갈등 및 어업인 피해를 방지하기 위한 조치로, 앞으로 대규모 해상 개발사업 진행 과정에서 이해관계자의 이견 수렴과정이 강화된다.\",\"positive_stocks\":[],\"positive_keywords\":[],\"positive_reasons\":\"\",\"negative_stocks\":[\"코오롱글로벌\",\"씨에스윈드\",\"유니슨\",\"두산에너빌리티\"],\"negative_keywords\":[\"해상풍력\",\"공유수면 허가\",\"어업인 의견청취\",\"사회적 갈등 가능성\",\"사업 절차 복잡화\"],\"negative_reasons\":\"공유수면 점용/사용 허가에 어업인 등 이해관계자의 의견 수렴 절차가 추가되어, 해상풍력 및 연관 해양개발 사업의 인허가 절차가 길어지고 불확실성이 커질 수 있다. 이에 따라 해상풍력발전소 구축, 해상개발, 관련 EPC·터빈업체에 단기적으로는 부정적인 영향이 예상된다.\"}'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ccd43d",
   "metadata": {},
   "source": [
    "### 추론모델 - 런타임결합\n",
    "1. lora모델을 로드\n",
    "2. base모델 + lora adapter 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "358db1bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41fc3d15c40e4bdbb636fe582f5bb4ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/291 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TextGenerationPipeline: {'model': 'PeftModelForCausalLM', 'dtype': 'bfloat16', 'device': 'cuda', 'input_modalities': 'text', 'output_modalities': ('text',)}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import AutoPeftModelForCausalLM  # Hub에 저장된 PEFT 모델 로더\n",
    "from transformers import AutoTokenizer, pipeline  # 토크나이저 / 파이프라인 생성\n",
    "import torch\n",
    "\n",
    "peft_model_id = hub_model_id\n",
    "\n",
    "finetuned_model = AutoPeftModelForCausalLM.from_pretrained(  # 파인튜닝 된 PEFT 모델 로드\n",
    "    peft_model_id,\n",
    "    dtype = torch.bfloat16,  # bf16 로드\n",
    "    device_map = 'auto',     # CPU/GPU 자동 배치\n",
    "    quantization_config = quant_config  # 4bit 양자화 설정 적용\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(peft_model_id)  # 같은 repo에서 토크나이저 로드\n",
    "pipe = pipeline('text-generation', model=finetuned_model, tokenizer=tokenizer)  # 텍스트 생성 파이프라인\n",
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e2bbd714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128009"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eos_token = tokenizer('<|eot_id|>', add_special_tokens=False)['input_ids'][0]  # <|eot_id|>의 토큰ID 추출\n",
    "eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0dbf3b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Passing `generation_config` together with generation-related arguments=({'eos_token_id', 'do_sample', 'max_new_tokens'}) is deprecated and will be removed in future versions. Please pass either a `generation_config` object OR all generation parameters explicitly, but not both.\n",
      "Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[prompt]\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "당신은 금융/경제 뉴스의 핵심내용을 요약해 설명하고, \n",
      "특정 상장 종목에 미치는 긍정/부정 영향여부, 이유, 근거를 분석하는 금융/경제 분석 전문가입니다.\n",
      "\n",
      "다음 출력지시사항을 지켜주세요.\n",
      "1. 뉴스와 종목간의 연관성을 발견할 수 없다면:\n",
      "    - stock_related를 False로 작성하세요.\n",
      "    - summary에 뉴스의 요약을 작성하세요.\n",
      "2. 뉴스와 종목간의 연관성을 발견했다면:\n",
      "    - stock_related를 True로 작성하세요.\n",
      "    - summary에 뉴스의 요약을 작성하세요.\n",
      "    - 긍정영향이 예상되는 종목이 있다면, positive_stocks, positive_keywords, positive_reasons를 작성하세요.\n",
      "    - 부정영향이 예상되는 종목이 있다면, negative_stocks, negative_keywords, negative_reasons를 작성하세요.\n",
      "    - 값이 없는 경우 빈 문자열(''), 빈 리스트([])로 작성하세요.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "글로벌 비즈 가트너 올해 전세계 스마트폰 판매량 7% 감소 전망\n",
      "경제와이드 모닝벨 글로벌 비즈 임선우 외신캐스터 글로벌 비즈입니다. ◇ 올해 스마트폰 판매 감소 올해 전세계 스마트폰 판매량이 크게 줄어들 것이란 전망이 나왔습니다. 시장조사업체 가트너는 글로벌 스마트폰 판매가 7% 하락할 것으로 내다봤는데요. 경제 전반에 걸친 침체 우려와 중국의 봉쇄조치 여파 그리고 인플레이션으로 소비자들이 지갑을 열기 주저하면서 수요가 줄어들 것 이라고 설명했습니다. 그러면서 올해 전체 출하량은 14억6천만대 수준에 그칠 것으로 예측했는데요. 종전 전망치인 16억대에서 대폭 낮춰 잡았습니다. 특히 세계 최대 스마트폰 시장인 중국에서 판매량은 18%가 감소할 것으로 전망했는데요. 가트너는 이같은 수요 부진으로 애플을 비롯한 스마트폰 제조사부터 엔비디아 TSMC 같은 반도체 업체까지 압력이 가해질 것이라고 진단했습니다. ◇ EU 가상자산 돈세탁 막는다 유럽연합이 가상자산을 이용한 돈세탁을 막기위해 관련 기업을 규제하는 방안에 잠정 합의했습니다. 잠정안에는 가상자산 업체가 당국에 모든 디지털자산 거래에 대한 신원 확인 정보를 제공하도록 하는 내용이 담겼는데요. 이에 따라 업체들은 관련 개인정보를 확보해야하고 당국이 이를 요구할 경우 제출해야 합니다. 또 거래액이 1천 유로 우리돈 130만 원을 넘길 경우 비인증 거래소가 관리하는 가상자산 지갑도 똑같은 규칙이 적용되는데요. 여기에 더해 송금 규제를 활용해 거래를 상시 추적하고 불법성이 의심되는 거래를 막을 수 있도록 할 방침입니다. 이와 관련해 미국 최대 가상자산 거래소 코인베이스 등 관련 기업 40여 곳은 개인정보 침해 가능성을 언급하며 줄곧 반대 입장을 밝혀왔는데요. 하지만 최근 가상자산 관련 범죄 사례가 급증하고 있는 만큼 규제 움직임이 힘을 얻고 있는 것으로 풀이됩니다. 관련 기관 논의는 오는 30일까지 마무리될 예정인데 이후 EU 위원회와 의회의 승인 절차만 남게 됩니다. ◇ 스피릿 M A 주주투표 또 미뤄 미국 저비용 항공사 스피릿 항공 인수전이 뜨거워지고 있습니다. 경쟁을 벌이고 있는 프론티어와 제트블루 항공이 앞다퉈 인수가를 높이고 있는데요. 더 높은 가격이 제시되면서 스피릿항공은 당초 어제 진행하기로 했던 프론티어 항공과의 합병안에 대한 주주투표를 이달 8일로 연기했습니다. 이미 한 차례 미뤘는데 다시 한번 주주투표를 연기하면서 스피릿이 제트블루와의 합병 여지를 되살리는 것 아니냐는 추측이 나오고 있는데요. 규제 등을 이유로 프론티어와의 인수합병에 무게가 실리는 듯 했지만 제트블루가 계속해서 인수가를 높이며 러브콜을 보내자 고민하는 모습입니다. 스피릿을 품게 되면 미국 항공 업계 다섯 손가락 안에 들 수 있게 되기 때문에 최후의 승자가 누가 될지에 관심이 쏠리고 있습니다. ◇ 텐센트·바이트댄스 하반기 또 감원 중국 대표 기술기업들이 연일 몸집 줄이기에 나서고 있습니다. 월스트리트저널은 텐센트와 바이트댄스를 비롯한 빅테크 기업들이 지난해 수만명의 인력을 줄인데 이어 또 한번 대규모 구조조정을 준비하고 있다고 전했는데요. 과거 수익성이 없는 사업을 정리하기 위해 감원에 나섰다면 최근에는 중국 경제가 고비를 겪으면서 비용을 줄이기 위해 몸집을 줄이고 있다고 설명했습니다. 특히 최근 당국이 빅테크 때리기 기조를 거둬들이고 있다는 점에서 거시적 위기가 새로운 숙제로 떠올랐다고 분석했는데요. 이미 상반기 전 부문에 걸쳐 인력의 10%가 넘게 정리한 텐센트는 하반기 대규모 추가 감원을 예고한데 더해 일부 사업의 전면 철수까지 검토 중인 것으로 알려졌는데요. 그간 규제 철퇴에 허덕이던 중국 빅테크들이 경기 둔화라는 악재까지 겹치면서 고민이 깊어지는 모습입니다. 지금까지 글로벌 비즈였습니다.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "[label]\n",
      "\n",
      "{\"stock_related\":true,\"summary\":\"글로벌 시장조사업체 가트너가 올해 전세계 스마트폰 판매가 7% 감소할 것이라고 전망했습니다. 주요 원인으로는 경제 침체, 중국 봉쇄, 인플레이션에 따른 수요 위축이 꼽힙니다. 특히 중국 시장에선 18% 감소가 예상됩니다. 수요 부진으로 애플, 삼성 등 스마트폰 제조사와 엔비디아, TSMC 등 반도체 공급사에도 부정적 영향을 미칠 것으로 보입니다. 추가로, EU의 가상자산 돈세탁 방지 규제 강화, 미국 스피릿 항공 M&A 지연, 중국 빅테크(텐센트·바이트댄스) 추가 감원 계획 소식이 주요 뉴스로 다뤄졌습니다.\",\"positive_stocks\":[],\"positive_keywords\":[],\"positive_reasons\":\"\",\"negative_stocks\":[\"삼성전자\",\"SK하이닉스\",\"애플\",\"엔비디아\",\"TSMC\",\"DB하이텍\",\"LX세미콘\"],\"negative_keywords\":[\"글로벌 스마트폰 판매 감소\",\"중국 수요 급감\",\"반도체 수요 위축\",\"중국 봉쇄\",\"경제 침체\",\"인플레이션\"],\"negative_reasons\":\"글로벌 스마트폰 판매 부진과 중국의 판매 급감, 경제 전반의 위축으로 인해 스마트폰 제조사 및 반도체 공급업체의 실적 압박이 가중될 것으로 예상됩니다. 이에 따라 삼성전자, SK하이닉스 등 관련 국내외 상장사의 실적과 주가에 부정적 영향이 예상됩니다.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[response]\n",
      "{\"stock_related\":true,\"summary\":\"가트너가 올해 전세계 스마트폰 판매량이 7% 감소할 것으로 전망했습니다. 이는 경제 침체, 중국의 봉쇄조치, 인플레이션 등으로 소비자 수요가 줄어들고 있기 때문입니다. 특히 중국 시장에서 판매량이 18% 감소할 것으로 예상되며, 애플과 같은 스마트폰 제조사뿐만 아니라 반도체 업체(TSMC 등에도 영향을 미칠 것으로 보입니다. 또한, EU가 가상자산을 이용한 돈세탁을 막기 위해 관련 기업에 신원 확인 정보 제공을 의무화하는 등 규제를 강화하고 있습니다. 스피릿 항공의 인수 경쟁도 계속되고 있으며, 텐센트와 바이트댄스 등 중국 빅테크 기업들은 중국 경제 침체와 규제 위험에 더해 비용 절감을 위해 대규모 감원을 실시할 계획입니다.\",\"positive_stocks\":[],\"positive_keywords\":[],\"positive_reasons\":\"\",\"negative_stocks\":[\"애플\",\"삼성전자\",\"LG전자\",\"SK하이닉스\",\"TSMC\"],\"negative_keywords\":[\"스마트폰 판매 감소\",\"중국 시장 감소\",\"비용 절감\",\"감원\",\"반도체 수요 감소\"],\"negative_reasons\":\"가트너의 전망에 따르면 전세계 스마트폰 판매량이 크게 감소할 것으로 예상되며, 특히 중국 시장의 감소가 큰 비중을 차지합니다. 이는 애플, 삼성전자, LG전자 등 스마트폰 제조사뿐만 아니라 반도체 공급업체(TSMC 등에도 직접적인 부정적 영향을 미칩니다. 중국 시장의 감소와 글로벌 수요 감소로 인해 반도체 수요도 줄어들 수 있어 반도체 업체의 실적에 부정적 영향을 줄 수 있습니다.\"}\n",
      "====================================================================================================\n",
      "[prompt]\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "당신은 금융/경제 뉴스의 핵심내용을 요약해 설명하고, \n",
      "특정 상장 종목에 미치는 긍정/부정 영향여부, 이유, 근거를 분석하는 금융/경제 분석 전문가입니다.\n",
      "\n",
      "다음 출력지시사항을 지켜주세요.\n",
      "1. 뉴스와 종목간의 연관성을 발견할 수 없다면:\n",
      "    - stock_related를 False로 작성하세요.\n",
      "    - summary에 뉴스의 요약을 작성하세요.\n",
      "2. 뉴스와 종목간의 연관성을 발견했다면:\n",
      "    - stock_related를 True로 작성하세요.\n",
      "    - summary에 뉴스의 요약을 작성하세요.\n",
      "    - 긍정영향이 예상되는 종목이 있다면, positive_stocks, positive_keywords, positive_reasons를 작성하세요.\n",
      "    - 부정영향이 예상되는 종목이 있다면, negative_stocks, negative_keywords, negative_reasons를 작성하세요.\n",
      "    - 값이 없는 경우 빈 문자열(''), 빈 리스트([])로 작성하세요.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "야놀자 포커스미디어와 ‘동네가게 오래함께’ 캠페인 진행\n",
      "사진 야놀자 제공 야놀자가 포커스미디어와 동네가게 오래함께 캠페인을 진행한다고 4일 밝혔다. 이번 캠페인은 지역 내 우수 소상공인을 발굴하고 이들을 위한 맞춤형 광고를 제작해 해당 지역 내 홍보를 지원한다. 총 14억 원 규모의 광고 제작 및 송출 비용은 양사가 전액 부담한다. 야놀자는 제휴점을 대상으로 사연을 공모하고 상권 빅데이터 분석을 진행해 지원 대상을 선정한다. 포커스미디어는 전국 5천800여 개 아파트에서 하루 800만 시청자를 확보한 엘리베이터 TV 등 자체 인프라를 통해 광고를 송출한다. 캠페인의 첫 광고는 서울시 노원구 동작구를 시작으로 오는 11일부터 2개월 간 방영되며 연말까지 대상 범위를 지속 확대할 예정이다. 야놀자 관계자는 소상공인들에게 직접적인 도움을 줄 수 있는 이번 캠페인을 통해 지역사회와 상생하고 지역경제 활성화에도 기여할 것으로 기대한다 면서 지역 내 우수 소상공인들의 인지도를 제고하고 매출 증대에도 기여할 수 있도록 캠페인의 성공적인 진행을 위한 지원을 아끼지 않을 계획 이라고 말했다.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "[label]\n",
      "\n",
      "{\"stock_related\":true,\"summary\":\"야놀자가 포커스미디어와 함께 '동네가게 오래함께' 캠페인을 시작한다. 이 캠페인은 지역 소상공인에게 맞춤형 광고를 지원해 상권 활성화와 소상공인 매출 증대를 목표로 하며, 광고 제작 및 송출 비용은 양사가 전액 부담한다. 광고는 포커스미디어의 엘리베이터 TV를 통해 시작되며, 향후 대상 지역을 계속 확대한다는 방침이다.\",\"positive_stocks\":[\"야놀자\"],\"positive_keywords\":[\"소상공인 지원\",\"상생\",\"지역경제 활성화\",\"광고\",\"인지도 제고\"],\"positive_reasons\":\"이번 캠페인은 야놀자가 지역 소상공인을 지원하는 긍정적 이미지를 구축하면서, 제휴점 확대와 매출 증진, 브랜드 가치 증대에 기여할 것으로 기대된다. 지역사회와 상생 및 협력을 통해 장기적으로 이용자 저변 확대에 긍정적 영향을 미칠 수 있다.\",\"negative_stocks\":[\"포커스미디어\"],\"negative_keywords\":[\"광고비 부담\",\"비용증가\"],\"negative_reasons\":\"포커스미디어는 대규모 광고 비용을 무상 지원해야 하므로 단기적으로 실적에 부담이 될 수 있다. 다만, 긍정적 인지도 제고 효과도 함께 고려할 필요가 있다.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[response]\n",
      "{\"stock_related\":true,\"summary\":\"야놀자와 포커스미디어가 동네가게 오래함께 캠페인을 진행한다. 이 캠페인은 지역 우수 소상공인을 발굴하고, 맞춤형 광고를 통해 해당 지역 내 홍보를 지원한다. 14억 원 규모의 광고 비용은 양사가 전액 부담하며, 엘리베이터 TV 등 자체 인프라를 통해 전국적으로 광고를 송출한다. 캠페인은 지역경제 활성화와 소상공인 매출 증대에 기여할 것으로 기대된다.\",\"positive_stocks\":[\"야놀자\"],\"positive_keywords\":[\"지역경제 활성화\",\"소상공인 지원\",\"광고 캠페인\",\"매출 증대 기대\"],\"positive_reasons\":\"야놀자는 지역 우수 소상공인 발굴 및 홍보를 통해 지역경제 활성화에 직접적으로 기여할 수 있으며, 브랜드 인지도 제고와 신규 고객 유치에 긍정적 영향을 줄 수 있다. 또한, 소상공인 매출 증대에 기여함으로써 장기적으로 야놀자의 플랫폼 이용자 확대와 수익성 개선에 도움이 될 수 있다.\",\"negative_stocks\":[],\"negative_keywords\":[],\"negative_reasons\":\"\"}\n",
      "====================================================================================================\n",
      "[prompt]\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "당신은 금융/경제 뉴스의 핵심내용을 요약해 설명하고, \n",
      "특정 상장 종목에 미치는 긍정/부정 영향여부, 이유, 근거를 분석하는 금융/경제 분석 전문가입니다.\n",
      "\n",
      "다음 출력지시사항을 지켜주세요.\n",
      "1. 뉴스와 종목간의 연관성을 발견할 수 없다면:\n",
      "    - stock_related를 False로 작성하세요.\n",
      "    - summary에 뉴스의 요약을 작성하세요.\n",
      "2. 뉴스와 종목간의 연관성을 발견했다면:\n",
      "    - stock_related를 True로 작성하세요.\n",
      "    - summary에 뉴스의 요약을 작성하세요.\n",
      "    - 긍정영향이 예상되는 종목이 있다면, positive_stocks, positive_keywords, positive_reasons를 작성하세요.\n",
      "    - 부정영향이 예상되는 종목이 있다면, negative_stocks, negative_keywords, negative_reasons를 작성하세요.\n",
      "    - 값이 없는 경우 빈 문자열(''), 빈 리스트([])로 작성하세요.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "삼성바이오로직스 MSD와 2768억원 위탁생산계약 체결\n",
      "미국 제약기업과 의약품 위탁생산 공급계약 지난해 9월 공시된 본 계약 체결 삼성바이오로직스 3공장 전경. ⓒ삼성바이오로직스 데일리안 이홍석 기자 삼성바이오로직스는 4일 공시를 통해 미국 제약기업 MSD MSD International Business GmbH 와 2768억2938만원 규모의 의약품 위탁생산 공급계약을 체결했다고 밝혔다. 이번 계약은 최근 매출액 대비 17.65% 규모로 계약기간은 2022년 7월 1일부터 2028년 12월 31일이다. 상기 계약금액은 고객사의 수요증가에 따라 3억8186만 달러로 증가할 수 있다. 이번 계약건은 지난해 9월 29일 공시된 ‘투자판단 관련 주요경영사항’에 대한 본계약 체결건이다. 당시 삼성바이오로직스는 MSD 위탁생산계약 의향서를 체결했으며 계약금은 491억원이라고 공시했다.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "[label]\n",
      "\n",
      "{\"stock_related\":true,\"summary\":\"삼성바이오로직스가 미국 제약회사 MSD와 약 2,768억 원 규모의 의약품 위탁생산(CMO) 공급계약을 체결했다. 계약 기간은 2022년 7월부터 2028년 12월까지이며, 고객사의 수요 증가에 따라 계약금액이 최대 3억8천만 달러(한화 약 5천억 원)까지 늘어날 수 있다. 이번 계약은 작년 9월 체결된 의향서에 기반한 본계약이다.\",\"positive_stocks\":[\"삼성바이오로직스\"],\"positive_keywords\":[\"위탁생산계약\",\"미국 제약기업 MSD\",\"매출 증가\",\"글로벌 공급계약\"],\"positive_reasons\":\"대규모 글로벌 제약사와의 장기 위탁생산 본계약 체결로 실적 가시성이 높아지고, 매출 증가 및 신뢰도 상승 효과가 기대됨. 추가로 계약금액이 확장될 수 있어 성장 모멘텀도 확대된다.\",\"negative_stocks\":[],\"negative_keywords\":[],\"negative_reasons\":\"\"}\n",
      "[response]\n",
      "{\"stock_related\":true,\"summary\":\"삼성바이오로직스가 미국 제약기업 MSD와 2768억2938만원 규모의 의약품 위탁생산 공급계약을 체결했다. 계약 기간은 2022년 7월 1일부터 2028년 12월 31일까지이며, 계약금액은 수요에 따라 증가할 수 있다. 이번 계약은 지난해 9월에 의향서를 공시했으며, 실제 계약금액이 크게 확대됐다.\",\"positive_stocks\":[\"삼성바이오로직스\"],\"positive_keywords\":[\"위탁생산 계약\",\"MSD\",\"미국 제약기업\",\"의약품 공급\",\"매출 확대\"],\"positive_reasons\":\"MSD와의 대규모 위탁생산 계약 체결은 삼성바이오로직스의 매출 확대와 실적 개선에 긍정적으로 작용할 수 있다. 계약 기간이 길고, 계약금액이 상당히 크며, 수요에 따라 추가 매출 증가 가능성이 있다.\",\"negative_stocks\":[],\"negative_keywords\":[],\"negative_reasons\":\"\"}\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# 프롬프트를 입력받으면 assistant 생성 결과만 반환하는 함수\n",
    "def test_inference(pipe, prompt):\n",
    "    outputs = pipe(prompt, max_new_tokens=1024, eos_token_id=eos_token, do_sample=False)  # 응답을 생성\n",
    "    assistant_start = len(prompt)  # 생성 결과에서 프롬프트 길이만큼은 입력 구간\n",
    "    return outputs[0]['generated_text'][assistant_start:].strip()  # 프롬프트 이후(생성된 부분)만 잘라서 반환\n",
    "\n",
    "for prompt, label in zip(prompt_list[10:13], label_list[10:13]):  # 10~12 샘플 비교\n",
    "    print(f'[prompt]\\n{prompt}')\n",
    "    print(f'[label]\\n{label}')\n",
    "    print(f'[response]\\n{test_inference(pipe, prompt)}')  # 모델 생성 응답 출력\n",
    "    print('=' * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465bece8",
   "metadata": {},
   "source": [
    "### 추론모델 - 사전병합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dab4dff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05a41425db784ff6a9b0038b2e21585f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/291 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/peft/tuners/lora/bnb.py:397: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d13493d9dbb4bdebbdf17ddf8597df7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afef6d83be2d4c75b2461e746bbd6964",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77f774ae7c1a4037a242953c3c7984c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f816a1a12afb49eea5ca09650a0ea7cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6e5ac73a3394db9a720983f13f376ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/capybaraOh/Llama-VARCO-8b-news2stock-analyzer-4bit-merged/commit/3c9be91b5691b87c8e1057a68a17055e5099e420', commit_message='Upload tokenizer', commit_description='', oid='3c9be91b5691b87c8e1057a68a17055e5099e420', pr_url=None, repo_url=RepoUrl('https://huggingface.co/capybaraOh/Llama-VARCO-8b-news2stock-analyzer-4bit-merged', endpoint='https://huggingface.co', repo_type='model', repo_id='capybaraOh/Llama-VARCO-8b-news2stock-analyzer-4bit-merged'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import AutoPeftModelForCausalLM  # PEFT(LoRA) 모델 로더\n",
    "from transformers import AutoTokenizer, pipeline  # 토크나이저 로더\n",
    "\n",
    "merged_model_id = 'capybaraOh/Llama-VARCO-8b-news2stock-analyzer-4bit-merged'  # 병합 모델 REPO\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(peft_model_id)  # PEFT 모델 토크나이저 로드\n",
    "\n",
    "# LoRA 어댑터 포함한 4bit 양자화 모델 로드\n",
    "finetuned_model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    peft_model_id,\n",
    "    dtype = torch.bfloat16,\n",
    "    device_map = 'auto',\n",
    "    quantization_config = quant_config\n",
    ")\n",
    "\n",
    "merged_model = finetuned_model.merge_and_unload()  # LoRA 가중치를 베이스 모델에 병합 후 어댑터는 언로드\n",
    "\n",
    "merged_model.push_to_hub(merged_model_id, token=True)  # 병합된 모델 업로드\n",
    "tokenizer.push_to_hub(merged_model_id, token=True)     # 토크나이저 동일 REPO에 업로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d088feff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메모리 해제\n",
    "del finetuned_model, merged_model  # 사용 완료된 모델/ 병합모델 참조 제거\n",
    "\n",
    "import gc     # 가비지 컬렉션 모듈\n",
    "gc.collect()  # 참조가 끊긴 객체 메모리 정리\n",
    "\n",
    "torch.cuda.empty_cache()  # Pytorch CUDA 캐시메모리 비우기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56939ba2-ac48-41ff-b005-c32749e9f895",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c676cfb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "870e272c3c114abca2fbbc6143934449",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/291 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TextGenerationPipeline: {'model': 'LlamaForCausalLM', 'dtype': 'bfloat16', 'device': 'cuda', 'input_modalities': 'text', 'output_modalities': ('text',)}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline  # 토크나이저 / 파이프라인 생성\n",
    "import torch\n",
    "\n",
    "model_id = 'capybaraOh/Llama-VARCO-8b-news2stock-analyzer-4bit-merged'\n",
    "\n",
    "finetuned_model = AutoModelForCausalLM.from_pretrained(  # 병합된 단일 모델 로드\n",
    "    model_id,\n",
    "    dtype = torch.bfloat16,  # bf16 로드\n",
    "    device_map = 'auto'      # CPU/GPU 자동 배치\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)  # 같은 repo에서 토크나이저 로드\n",
    "pipe = pipeline('text-generation', model=finetuned_model, tokenizer=tokenizer)  # 텍스트 생성 파이프라인\n",
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c9f7adc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[prompt]\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "당신은 금융/경제 뉴스의 핵심내용을 요약해 설명하고, \n",
      "특정 상장 종목에 미치는 긍정/부정 영향여부, 이유, 근거를 분석하는 금융/경제 분석 전문가입니다.\n",
      "\n",
      "다음 출력지시사항을 지켜주세요.\n",
      "1. 뉴스와 종목간의 연관성을 발견할 수 없다면:\n",
      "    - stock_related를 False로 작성하세요.\n",
      "    - summary에 뉴스의 요약을 작성하세요.\n",
      "2. 뉴스와 종목간의 연관성을 발견했다면:\n",
      "    - stock_related를 True로 작성하세요.\n",
      "    - summary에 뉴스의 요약을 작성하세요.\n",
      "    - 긍정영향이 예상되는 종목이 있다면, positive_stocks, positive_keywords, positive_reasons를 작성하세요.\n",
      "    - 부정영향이 예상되는 종목이 있다면, negative_stocks, negative_keywords, negative_reasons를 작성하세요.\n",
      "    - 값이 없는 경우 빈 문자열(''), 빈 리스트([])로 작성하세요.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "글로벌 비즈 가트너 올해 전세계 스마트폰 판매량 7% 감소 전망\n",
      "경제와이드 모닝벨 글로벌 비즈 임선우 외신캐스터 글로벌 비즈입니다. ◇ 올해 스마트폰 판매 감소 올해 전세계 스마트폰 판매량이 크게 줄어들 것이란 전망이 나왔습니다. 시장조사업체 가트너는 글로벌 스마트폰 판매가 7% 하락할 것으로 내다봤는데요. 경제 전반에 걸친 침체 우려와 중국의 봉쇄조치 여파 그리고 인플레이션으로 소비자들이 지갑을 열기 주저하면서 수요가 줄어들 것 이라고 설명했습니다. 그러면서 올해 전체 출하량은 14억6천만대 수준에 그칠 것으로 예측했는데요. 종전 전망치인 16억대에서 대폭 낮춰 잡았습니다. 특히 세계 최대 스마트폰 시장인 중국에서 판매량은 18%가 감소할 것으로 전망했는데요. 가트너는 이같은 수요 부진으로 애플을 비롯한 스마트폰 제조사부터 엔비디아 TSMC 같은 반도체 업체까지 압력이 가해질 것이라고 진단했습니다. ◇ EU 가상자산 돈세탁 막는다 유럽연합이 가상자산을 이용한 돈세탁을 막기위해 관련 기업을 규제하는 방안에 잠정 합의했습니다. 잠정안에는 가상자산 업체가 당국에 모든 디지털자산 거래에 대한 신원 확인 정보를 제공하도록 하는 내용이 담겼는데요. 이에 따라 업체들은 관련 개인정보를 확보해야하고 당국이 이를 요구할 경우 제출해야 합니다. 또 거래액이 1천 유로 우리돈 130만 원을 넘길 경우 비인증 거래소가 관리하는 가상자산 지갑도 똑같은 규칙이 적용되는데요. 여기에 더해 송금 규제를 활용해 거래를 상시 추적하고 불법성이 의심되는 거래를 막을 수 있도록 할 방침입니다. 이와 관련해 미국 최대 가상자산 거래소 코인베이스 등 관련 기업 40여 곳은 개인정보 침해 가능성을 언급하며 줄곧 반대 입장을 밝혀왔는데요. 하지만 최근 가상자산 관련 범죄 사례가 급증하고 있는 만큼 규제 움직임이 힘을 얻고 있는 것으로 풀이됩니다. 관련 기관 논의는 오는 30일까지 마무리될 예정인데 이후 EU 위원회와 의회의 승인 절차만 남게 됩니다. ◇ 스피릿 M A 주주투표 또 미뤄 미국 저비용 항공사 스피릿 항공 인수전이 뜨거워지고 있습니다. 경쟁을 벌이고 있는 프론티어와 제트블루 항공이 앞다퉈 인수가를 높이고 있는데요. 더 높은 가격이 제시되면서 스피릿항공은 당초 어제 진행하기로 했던 프론티어 항공과의 합병안에 대한 주주투표를 이달 8일로 연기했습니다. 이미 한 차례 미뤘는데 다시 한번 주주투표를 연기하면서 스피릿이 제트블루와의 합병 여지를 되살리는 것 아니냐는 추측이 나오고 있는데요. 규제 등을 이유로 프론티어와의 인수합병에 무게가 실리는 듯 했지만 제트블루가 계속해서 인수가를 높이며 러브콜을 보내자 고민하는 모습입니다. 스피릿을 품게 되면 미국 항공 업계 다섯 손가락 안에 들 수 있게 되기 때문에 최후의 승자가 누가 될지에 관심이 쏠리고 있습니다. ◇ 텐센트·바이트댄스 하반기 또 감원 중국 대표 기술기업들이 연일 몸집 줄이기에 나서고 있습니다. 월스트리트저널은 텐센트와 바이트댄스를 비롯한 빅테크 기업들이 지난해 수만명의 인력을 줄인데 이어 또 한번 대규모 구조조정을 준비하고 있다고 전했는데요. 과거 수익성이 없는 사업을 정리하기 위해 감원에 나섰다면 최근에는 중국 경제가 고비를 겪으면서 비용을 줄이기 위해 몸집을 줄이고 있다고 설명했습니다. 특히 최근 당국이 빅테크 때리기 기조를 거둬들이고 있다는 점에서 거시적 위기가 새로운 숙제로 떠올랐다고 분석했는데요. 이미 상반기 전 부문에 걸쳐 인력의 10%가 넘게 정리한 텐센트는 하반기 대규모 추가 감원을 예고한데 더해 일부 사업의 전면 철수까지 검토 중인 것으로 알려졌는데요. 그간 규제 철퇴에 허덕이던 중국 빅테크들이 경기 둔화라는 악재까지 겹치면서 고민이 깊어지는 모습입니다. 지금까지 글로벌 비즈였습니다.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "[label]\n",
      "\n",
      "{\"stock_related\":true,\"summary\":\"글로벌 시장조사업체 가트너가 올해 전세계 스마트폰 판매가 7% 감소할 것이라고 전망했습니다. 주요 원인으로는 경제 침체, 중국 봉쇄, 인플레이션에 따른 수요 위축이 꼽힙니다. 특히 중국 시장에선 18% 감소가 예상됩니다. 수요 부진으로 애플, 삼성 등 스마트폰 제조사와 엔비디아, TSMC 등 반도체 공급사에도 부정적 영향을 미칠 것으로 보입니다. 추가로, EU의 가상자산 돈세탁 방지 규제 강화, 미국 스피릿 항공 M&A 지연, 중국 빅테크(텐센트·바이트댄스) 추가 감원 계획 소식이 주요 뉴스로 다뤄졌습니다.\",\"positive_stocks\":[],\"positive_keywords\":[],\"positive_reasons\":\"\",\"negative_stocks\":[\"삼성전자\",\"SK하이닉스\",\"애플\",\"엔비디아\",\"TSMC\",\"DB하이텍\",\"LX세미콘\"],\"negative_keywords\":[\"글로벌 스마트폰 판매 감소\",\"중국 수요 급감\",\"반도체 수요 위축\",\"중국 봉쇄\",\"경제 침체\",\"인플레이션\"],\"negative_reasons\":\"글로벌 스마트폰 판매 부진과 중국의 판매 급감, 경제 전반의 위축으로 인해 스마트폰 제조사 및 반도체 공급업체의 실적 압박이 가중될 것으로 예상됩니다. 이에 따라 삼성전자, SK하이닉스 등 관련 국내외 상장사의 실적과 주가에 부정적 영향이 예상됩니다.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[response]\n",
      "stock_related: True\n",
      "summary: 가트너는 올해 전세계 스마트폰 판매량이 7% 감소할 것으로 전망했습니다. 이는 경제 전반의 침체, 중국의 봉쇄조치, 인플레이션 등으로 소비자들이 스마트폰 구매를 주저하기 때문이라고 설명했습니다. 특히 중국 시장에서 판매량 감소가 크게 예상되며, 이는 애플과 같은 스마트폰 제조사뿐만 아니라 반도체 업체에도 부정적인 영향을 미칠 것으로 보입니다.\n",
      "\n",
      "positive_stocks:\n",
      "positive_keywords:\n",
      "positive_reasons:\n",
      "- 현재 글로벌 경제 침체로 인해 스마트폰 수요가 감소할 것으로 예상되는 상황에서 반도체 공급업체인 TSMC 등은 상대적으로 안정적인 성과를 보일 수 있습니다. \n",
      "- TSMC는 스마트폰 제조사에 대한 주요 공급업체로서, 스마트폰 생산량 감소에도 불구하고 수요가 상대적으로 안정적일 수 있습니다.\n",
      "\n",
      "negative_stocks:\n",
      "negative_keywords:\n",
      "negative_reasons:\n",
      "- 스마트폰 판매량 감소로 인해 애플과 같은 스마트폰 제조사의 주가에 부정적인 영향을 미칠 것으로 예상됩니다.\n",
      "- 스마트폰 수요 감소는 반도체 시장 전체에 영향을 미칠 수 있으며, TSMC를 포함한 반도체 제조사들에게도 부정적인 영향을 줄 수 있습니다.\n",
      "- 특히 중국 시장의 스마트폰 판매량 감소는 중국 내 반도체 업체에도 부정적인 영향을 미칠 수 있습니다. \n",
      "\n",
      "이 외에도, 가상자산 규제와 관련된 뉴스도 주목할 만합니다. 가상자산 거래소들이 개인정보 제공 의무를 부담하게 되면 비용 증가로 운영 효율성이 저하될 수 있습니다. 이는 가상자산 시장 전반에 부정적인 영향을 미칠 수 있습니다. 하지만 이에 대한 직접적인 종목 연관성은 명확히 제시하기 어렵습니다.\n",
      "====================================================================================================\n",
      "[prompt]\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "당신은 금융/경제 뉴스의 핵심내용을 요약해 설명하고, \n",
      "특정 상장 종목에 미치는 긍정/부정 영향여부, 이유, 근거를 분석하는 금융/경제 분석 전문가입니다.\n",
      "\n",
      "다음 출력지시사항을 지켜주세요.\n",
      "1. 뉴스와 종목간의 연관성을 발견할 수 없다면:\n",
      "    - stock_related를 False로 작성하세요.\n",
      "    - summary에 뉴스의 요약을 작성하세요.\n",
      "2. 뉴스와 종목간의 연관성을 발견했다면:\n",
      "    - stock_related를 True로 작성하세요.\n",
      "    - summary에 뉴스의 요약을 작성하세요.\n",
      "    - 긍정영향이 예상되는 종목이 있다면, positive_stocks, positive_keywords, positive_reasons를 작성하세요.\n",
      "    - 부정영향이 예상되는 종목이 있다면, negative_stocks, negative_keywords, negative_reasons를 작성하세요.\n",
      "    - 값이 없는 경우 빈 문자열(''), 빈 리스트([])로 작성하세요.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "야놀자 포커스미디어와 ‘동네가게 오래함께’ 캠페인 진행\n",
      "사진 야놀자 제공 야놀자가 포커스미디어와 동네가게 오래함께 캠페인을 진행한다고 4일 밝혔다. 이번 캠페인은 지역 내 우수 소상공인을 발굴하고 이들을 위한 맞춤형 광고를 제작해 해당 지역 내 홍보를 지원한다. 총 14억 원 규모의 광고 제작 및 송출 비용은 양사가 전액 부담한다. 야놀자는 제휴점을 대상으로 사연을 공모하고 상권 빅데이터 분석을 진행해 지원 대상을 선정한다. 포커스미디어는 전국 5천800여 개 아파트에서 하루 800만 시청자를 확보한 엘리베이터 TV 등 자체 인프라를 통해 광고를 송출한다. 캠페인의 첫 광고는 서울시 노원구 동작구를 시작으로 오는 11일부터 2개월 간 방영되며 연말까지 대상 범위를 지속 확대할 예정이다. 야놀자 관계자는 소상공인들에게 직접적인 도움을 줄 수 있는 이번 캠페인을 통해 지역사회와 상생하고 지역경제 활성화에도 기여할 것으로 기대한다 면서 지역 내 우수 소상공인들의 인지도를 제고하고 매출 증대에도 기여할 수 있도록 캠페인의 성공적인 진행을 위한 지원을 아끼지 않을 계획 이라고 말했다.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "[label]\n",
      "\n",
      "{\"stock_related\":true,\"summary\":\"야놀자가 포커스미디어와 함께 '동네가게 오래함께' 캠페인을 시작한다. 이 캠페인은 지역 소상공인에게 맞춤형 광고를 지원해 상권 활성화와 소상공인 매출 증대를 목표로 하며, 광고 제작 및 송출 비용은 양사가 전액 부담한다. 광고는 포커스미디어의 엘리베이터 TV를 통해 시작되며, 향후 대상 지역을 계속 확대한다는 방침이다.\",\"positive_stocks\":[\"야놀자\"],\"positive_keywords\":[\"소상공인 지원\",\"상생\",\"지역경제 활성화\",\"광고\",\"인지도 제고\"],\"positive_reasons\":\"이번 캠페인은 야놀자가 지역 소상공인을 지원하는 긍정적 이미지를 구축하면서, 제휴점 확대와 매출 증진, 브랜드 가치 증대에 기여할 것으로 기대된다. 지역사회와 상생 및 협력을 통해 장기적으로 이용자 저변 확대에 긍정적 영향을 미칠 수 있다.\",\"negative_stocks\":[\"포커스미디어\"],\"negative_keywords\":[\"광고비 부담\",\"비용증가\"],\"negative_reasons\":\"포커스미디어는 대규모 광고 비용을 무상 지원해야 하므로 단기적으로 실적에 부담이 될 수 있다. 다만, 긍정적 인지도 제고 효과도 함께 고려할 필요가 있다.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[response]\n",
      "stock_related: False\n",
      "summary: 야놀자와 포커스미디어가 '동네가게 오래함께' 캠페인을 진행한다고 밝혔다. 이 캠페인은 지역 내 우수 소상공인을 발굴하고, 이들을 위한 맞춤형 광고를 제작해 해당 지역 내 홍보를 지원한다. 양사는 총 14억 원 규모의 광고 제작 및 송출 비용을 전액 부담하며, 야놀자는 제휴점의 사연을 공모하고 빅데이터 분석을 통해 지원 대상을 선정한다. 포커스미디어는 전국 아파트 엘리베이터 TV 등 자체 인프라를 통해 광고를 송출한다. 캠페인은 서울시 노원구 동작구를 시작으로 2개월간 방영되며, 연말까지 대상 범위를 확대할 예정이다.\n",
      "\n",
      "이 캠페인은 야놀자와 포커스미디어의 협업을 통한 지역사회와 상생, 그리고 지역경제 활성화에 초점을 맞추고 있으나, 직접적으로 특정 상장 종목에 미치는 영향은 명확히 보이지 않는다. 따라서 이 정보만으로는 주식 시장에 미치는 긍정/부정 영향을 분석하기 어렵다. 캠페인의 성공 여부와 실제로 소상공인들의 매출 증가 등에 따라 장기적으로 경제에 미치는 영향이 달라질 수 있으나, 현재로서는 주식 시장과의 직접적인 연관성을 찾기 어렵다.\n",
      "====================================================================================================\n",
      "[prompt]\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "당신은 금융/경제 뉴스의 핵심내용을 요약해 설명하고, \n",
      "특정 상장 종목에 미치는 긍정/부정 영향여부, 이유, 근거를 분석하는 금융/경제 분석 전문가입니다.\n",
      "\n",
      "다음 출력지시사항을 지켜주세요.\n",
      "1. 뉴스와 종목간의 연관성을 발견할 수 없다면:\n",
      "    - stock_related를 False로 작성하세요.\n",
      "    - summary에 뉴스의 요약을 작성하세요.\n",
      "2. 뉴스와 종목간의 연관성을 발견했다면:\n",
      "    - stock_related를 True로 작성하세요.\n",
      "    - summary에 뉴스의 요약을 작성하세요.\n",
      "    - 긍정영향이 예상되는 종목이 있다면, positive_stocks, positive_keywords, positive_reasons를 작성하세요.\n",
      "    - 부정영향이 예상되는 종목이 있다면, negative_stocks, negative_keywords, negative_reasons를 작성하세요.\n",
      "    - 값이 없는 경우 빈 문자열(''), 빈 리스트([])로 작성하세요.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "삼성바이오로직스 MSD와 2768억원 위탁생산계약 체결\n",
      "미국 제약기업과 의약품 위탁생산 공급계약 지난해 9월 공시된 본 계약 체결 삼성바이오로직스 3공장 전경. ⓒ삼성바이오로직스 데일리안 이홍석 기자 삼성바이오로직스는 4일 공시를 통해 미국 제약기업 MSD MSD International Business GmbH 와 2768억2938만원 규모의 의약품 위탁생산 공급계약을 체결했다고 밝혔다. 이번 계약은 최근 매출액 대비 17.65% 규모로 계약기간은 2022년 7월 1일부터 2028년 12월 31일이다. 상기 계약금액은 고객사의 수요증가에 따라 3억8186만 달러로 증가할 수 있다. 이번 계약건은 지난해 9월 29일 공시된 ‘투자판단 관련 주요경영사항’에 대한 본계약 체결건이다. 당시 삼성바이오로직스는 MSD 위탁생산계약 의향서를 체결했으며 계약금은 491억원이라고 공시했다.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "[label]\n",
      "\n",
      "{\"stock_related\":true,\"summary\":\"삼성바이오로직스가 미국 제약회사 MSD와 약 2,768억 원 규모의 의약품 위탁생산(CMO) 공급계약을 체결했다. 계약 기간은 2022년 7월부터 2028년 12월까지이며, 고객사의 수요 증가에 따라 계약금액이 최대 3억8천만 달러(한화 약 5천억 원)까지 늘어날 수 있다. 이번 계약은 작년 9월 체결된 의향서에 기반한 본계약이다.\",\"positive_stocks\":[\"삼성바이오로직스\"],\"positive_keywords\":[\"위탁생산계약\",\"미국 제약기업 MSD\",\"매출 증가\",\"글로벌 공급계약\"],\"positive_reasons\":\"대규모 글로벌 제약사와의 장기 위탁생산 본계약 체결로 실적 가시성이 높아지고, 매출 증가 및 신뢰도 상승 효과가 기대됨. 추가로 계약금액이 확장될 수 있어 성장 모멘텀도 확대된다.\",\"negative_stocks\":[],\"negative_keywords\":[],\"negative_reasons\":\"\"}\n",
      "[response]\n",
      "stock_related: True\n",
      "summary: 삼성바이오로직스가 미국 제약기업 MSD와 2768억2938만원 규모의 의약품 위탁생산 공급계약을 체결했다고 밝혔다. 계약 기간은 2022년 7월 1일부터 2028년 12월 31일이며, 계약금액은 고객사의 수요 증가에 따라 3억8186만 달러까지 증가할 수 있다.\n",
      "\n",
      "positive_stocks:\n",
      "- 삼성바이오로직스\n",
      "\n",
      "positive_keywords:\n",
      "- 위탁생산 계약 체결\n",
      "- 의약품 공급계약\n",
      "- 매출 증가 기대\n",
      "\n",
      "positive_reasons:\n",
      "- 이번 계약은 삼성바이오로직스의 매출에 상당한 기여를 할 것으로 보인다. 계약 규모가 2768억2938만원으로 큰 규모이며, 계약 기간 동안 매출을 지속적으로 증가시킬 수 있다. 또한 계약금액이 고객사의 수요에 따라 증가할 수 있어 추가적인 매출 증대 가능성이 있다.\n",
      "\n",
      "negative_stocks:\n",
      "- 없음\n",
      "\n",
      "negative_keywords:\n",
      "- 없음\n",
      "\n",
      "negative_reasons:\n",
      "- 없음\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "for prompt, label in zip(prompt_list[10:13], label_list[10:13]):  # 10~12 샘플 비교\n",
    "    print(f'[prompt]\\n{prompt}')\n",
    "    print(f'[label]\\n{label}')\n",
    "    print(f'[response]\\n{test_inference(pipe, prompt)}')  # 모델 생성 응답 출력\n",
    "    print('=' * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b9454b",
   "metadata": {},
   "source": [
    "## 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bff1fe6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 뉴스 본문을 넣으면 모델의 분석 응답 텍스트를 반환하는 함수\n",
    "def inference(news):\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': '''\n",
    "당신은 금융/경제 뉴스의 핵심내용을 요약해 설명하고,\n",
    "특정 상장 종목에 미치는 긍정/부정 영향여부, 이유, 근거를 분석하는 금융/경제 분석 전문가입니다.\n",
    "\n",
    "다음 출력지시사항을 지켜주세요.\n",
    "1. 뉴스와 종목간의 연관성을 발견할 수 없다면:\n",
    "    - stock_related를 False로 작성하세요.\n",
    "    - summary에 뉴스의 요약을 작성하세요.\n",
    "2. 뉴스와 종목간의 연관성을 발견했다면:\n",
    "    - stock_related를 True로 작성하세요.\n",
    "    - summary에 뉴스의 요약을 작성하세요.\n",
    "    - 긍정영향이 예상되는 종목이 있다면, positive_stocks, positive_keywords, positive_reasons를 작성하세요.\n",
    "    - 부정영향이 예상되는 종목이 있다면, negative_stocks, negative_keywords, negative_reasons를 작성하세요.\n",
    "    - 값이 없는 경우 빈 문자열(''), 빈 리스트([])로 작성하세요.\n",
    "'''},  # 시스템 지시문\n",
    "        {'role': 'user', 'content': news}  # 사용자 입력(뉴스)        \n",
    "    ]\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False)  # messages를 채팅 프롬프트 문자열로 변환\n",
    "    # 파이프라인으로 텍스트 생성 수행\n",
    "    outputs = pipe(\n",
    "        prompt,  # 변환된 프롬프트\n",
    "        max_new_tokens = 1024,    # 생성 최대 토큰 수\n",
    "        eos_token_id = eos_token, # 종료 토큰 ID\n",
    "        do_sample = False         # False: Greedy, True: 샘플링 적용\n",
    "    )\n",
    "    assistant_start = len(prompt)  # 프롬프트 길이 계산\n",
    "    return outputs[0]['generated_text'][assistant_start:].strip()  # 프롬프트 이후(생성된 부분)만 잘라서 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6d10ba75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'stock_related: True\\n\\nsummary:\\n현대자동차 그룹 정의선 회장이 캐나다에서 열리는 \\'한국·캐나다 자동차 산업 협력 포럼\\'에 참석할 예정이다. 이 행사에서는 한국과 캐나다의 자동차 산업 협력 방안을 논의할 예정이며, 정 회장은 현지에서 구체적인 캐나다 투자 방안을 공개할 계획이다. 특히, 현대차는 캐나다 자원을 활용한 수소 분야 협력 등 다양한 협력 방안을 검토 중이다. 하지만 캐나다 정부가 요구한 전기차 전용 공장 건설은 투자 명단에서 제외되었다. 정 회장이 캐나다 초계 잠수함 사업(CPSP) 수주를 지원하기 위해 캐나다 출장길에 올랐으며, 이는 현지 투자와 기술이전 등을 요구하는 절충교역의 일환이다. \\n\\npositive_stocks:\\n[\\n  \"현대자동차\"\\n]\\n\\npositive_keywords:\\n[\\n  \"자동차 산업 협력\",\\n  \"캐나다 투자\",\\n  \"수소 분야 협력\",\\n  \"초계 잠수함 사업 수주\"\\n]\\n\\npositive_reasons:\\n\"현대자동차는 캐나다와의 자동차 산업 협력을 통해 현지 투자를 확대할 수 있을 것으로 보인다. 캐나다 초계 잠수함 사업 수주 성공 시, 60조원 규모의 MRO 시장 주도권 확보 가능성이 높다. 또한 수소 분야 협력 등 다양한 사업 기회도 열릴 전망이다.\"\\n\\nnegative_stocks:\\n[\\n  \\n]\\n\\nnegative_keywords:\\n[\\n  \\n]\\n\\nnegative_reasons:\\n\"이번 뉴스에서는 부정적인 영향을 미칠 수 있는 특정 종목에 대한 언급이 없다.\"'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 뉴스 1건을 inference 함수로 분석\n",
    "news = '''\n",
    "강훈식 청와대 비서실장이 이끄는 캐나다 방산 특사단 출장 기간 현지에서 한·캐나다 자동차 포럼이 열린다. 한국과 캐나다의 자동차 산업 협력에 대한 논의가 이뤄질 전망이다. 이자리에는 정의선 현대자동차그룹 회장 등 주요 경영진도 참석할 것으로 알려졌다.\n",
    "\n",
    "26일 자동차 업계 및 정부 관계자에 따르면 정 회장과 장재훈 부회장은 이번주 캐나다에서 열리는 ‘한국·캐나다 자동차 산업 협력 포럼’에 참석한다. 이 행사에는 김정관 산업통상자원부 장관과 멜라니 졸리 캐나다 산업부 장관 등 주요 인사가 모여 자동차 산업 협력 방안에 대해 논의할 예정이다. 정 회장이 현지 일정상 참석이 어려울 경우 장재훈 현대차 부회장만 참석할 가능성도 남아있다.\n",
    "\n",
    "정 회장은 현지에서 구체적인 캐나다 투자 방안을 공개할 예정이다. 현대차는 캐나다 자원 등 장점을 활용해 수소 분야를 포함한 다양한 협력 방안을 검토 중인 것으로 알려졌다. 캐나다 정부가 요구해 온 ‘전기차 전용 공장 건설’은 투자 명단에서 제외하기로 가닥이 잡혔다. 북미 시장 공략을 위해 지난해 초 미국 조지아주에 완공한 메타플랜트아메리카(HMGMA)와의 중복 투자를 피하기 위해서다.\n",
    "\n",
    "정 회장은 60조원 규모의 캐나다 초계 잠수함 사업(CPSP) 수주를 지원하기 위해 캐나다 출장길에 올랐다. 정 회장이 전면에 나선 배경에는 ‘절충교역’이 있다. 절충교역은 대규모 방산 계약을 발주하는 국가가 수주국에 현지 투자나 기술이전, 공급망 구축 등을 요구하는 방식이다. 캐나다 정부는 3000t급 디젤 잠수함 12척을 도입하는 대가로 현대차의 현지 투자를 강력히 희망해 왔다. 캐나다는 한국과 경쟁 상대인 독일 측에도 폭스바겐의 현지 생산 확대를 입찰 조건으로 제시한 것으로 전해졌다. 현대차는 캐나다에 생산 시설이 없는 반면 독일 폭스바겐은 배터리 셀 공장을 건설 중이다.\n",
    "\n",
    "이번 수주전의 성패는 향후 30년간의 유지·보수·정비(MRO) 시장 주도권과 직결된다. 이 사업은 디젤 잠수함 최대 12척을 건조하는 프로젝트다. 건조비용(약 20조원)에 도입 후 30년간 유지·보수·운영(MRO) 비용까지 포함하면 최대 60조원까지 규모가 커질 전망이다.\n",
    "'''\n",
    "inference(news)  # news 문자열을 입력으로 넣어 모델 분석 결과(assistant 응답)를 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "346299e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"stock_related: True\\nsummary: 달러 대비 원화가 20원 가까이 급등하며 1440원대를 기록하고 있습니다. 이는 미국과 일본 외환당국의 시장 개입 가능성에 따른 엔화 급등이 원화 강세로 이어진 결과로 보입니다. 일본은행이 '레이트 체크'를 실시한 것과 미국 뉴욕 연방준비은행도 같은 조치를 취했다는 소식이 전해졌습니다. 일본 총리 다카이치 사나에 의하면, 일본은 투기적이고 비정상적인 움직임에 대응할 모든 조치를 취할 것이라고 밝혔습니다. 이에 따라 달러당 엔화값은 155엔대로 상승했습니다. 또한 오늘 열리는 국민연금 기금운용위원회에서 환헤지 전략 등이 논의될 예정이라는 점도 주목되고 있습니다.\\n\\npositive_stocks:\\n- KRW (한국원)\\n- JPY (일본엔)\\n\\npositive_keywords:\\n- 달러 대비 원화 급등\\n- 일본은행의 레이트 체크\\n- 미국 연방준비은행의 레이트 체크\\n- 일본 총리 발언\\n\\npositive_reasons:\\n- 일본은행과 미국 연방준비은행의 레이트 체크로 인해 엔화가 급등하였고, 이는 원화 강세로 이어졌습니다.\\n- 일본 총리의 강력한 발언으로 인해 투기적 움직임에 대응할 조치가 취해질 가능성이 높아졌습니다.\\n\\nnegative_stocks:\\n- USD (미국달러)\\n\\nnegative_keywords:\\n- 외환 시장 개입 불확실성\\n- 환율 변동성 증가\\n\\nnegative_reasons:\\n- 미국과 일본 외환당국의 시장 개입 가능성에 대한 명확한 전망이 없기 때문에, 달러에 대한 압력이 계속될 수 있습니다.\\n- 외환 시장의 불확실성으로 인해 환율 변동성이 높아질 수 있습니다.\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 뉴스 1건을 inference 함수로 분석\n",
    "news = '''\n",
    "달러당 원화값이 26일 전 거래일 대비 20원 가까이 급등하며 1440원대를 이어가고 있다.\n",
    "\n",
    "이날 서울외환시장에서 원화값은 전 거래일보다 19.7원 오른 1446.1원에 출발해 오전 10시25분 현재 1446.2원에 거래되고 있다.\n",
    "\n",
    "미국과 일본 외환당국의 시장 개입 가능성이 거론되면서 엔화가 급등한 점이 원화 강세로 이어졌다는 분석이 나온다. 일본은행은 최근 외환시장 개입에 앞서 주요 은행을 상대로 거래 상황을 점검하는 ‘레이트 체크’를 실시한 것으로 전해졌다. 미국 뉴욕 연방준비은행도 미 재무부 지시에 따라 레이트 체크에 나섰다는 보도가 나왔다.\n",
    "\n",
    "다카이치 사나에 일본 총리는 “투기적이고 비정상적인 움직임에 필요한 모든 조처를 할 것”이라고 밝힌 바 있다. 이에 따라 지난주 달러당 160엔에 육박했던 달러당 엔화값은 155엔대 초반까지 상승했다. 현재 엔화값은 전 거래일 대비 0.50% 오른 155.04엔이다.\n",
    "\n",
    "한편 이날 열리는 국민연금 기금운용위원회에서 환헤지 전략 등이 논의될 예정이어서 원화값에 어떤 영향을 미칠지도 주목된다.\n",
    "'''\n",
    "inference(news)  # news 문자열을 입력으로 넣어 모델 분석 결과(assistant 응답)를 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8c5bf1",
   "metadata": {},
   "source": [
    "### Base모델과 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c633d92c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b6635068c1c4621a818d597d382b921",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/291 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[샘플 1]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 31.36 GiB of which 996.88 MiB is free. Including non-PyTorch memory, this process has 30.38 GiB memory in use. Of the allocated memory 28.77 GiB is allocated by PyTorch, and 1.03 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx, (prompt, label) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(prompt_list[\u001b[32m10\u001b[39m:\u001b[32m13\u001b[39m], label_list[\u001b[32m10\u001b[39m:\u001b[32m13\u001b[39m])):  \u001b[38;5;66;03m# 샘플 3개만 비교\u001b[39;00m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[샘플 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# 샘플 번호 출력\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     base_resp = \u001b[43mtest_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_pipe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 베이스 모델 응답 생성\u001b[39;00m\n\u001b[32m     15\u001b[39m     lora_resp = test_inference(pipe, prompt)  \u001b[38;5;66;03m# LoRA 모델 응답 생성\u001b[39;00m\n\u001b[32m     16\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  [Base - 파인튜닝 전]\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mbase_resp\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# 베이스 모델 응답 출력\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mtest_inference\u001b[39m\u001b[34m(pipe, prompt)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtest_inference\u001b[39m(pipe, prompt):\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     outputs = \u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43meos_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 응답을 생성\u001b[39;00m\n\u001b[32m      4\u001b[39m     assistant_start = \u001b[38;5;28mlen\u001b[39m(prompt)  \u001b[38;5;66;03m# 생성 결과에서 프롬프트 길이만큼은 입력 구간\u001b[39;00m\n\u001b[32m      5\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs[\u001b[32m0\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mgenerated_text\u001b[39m\u001b[33m'\u001b[39m][assistant_start:].strip()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/pipelines/text_generation.py:293\u001b[39m, in \u001b[36mTextGenerationPipeline.__call__\u001b[39m\u001b[34m(self, text_inputs, **kwargs)\u001b[39m\n\u001b[32m    243\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, text_inputs, **kwargs):\n\u001b[32m    244\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    245\u001b[39m \u001b[33;03m    Complete the prompt(s) given as inputs.\u001b[39;00m\n\u001b[32m    246\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    291\u001b[39m \u001b[33;03m          ids of the generated text.\u001b[39;00m\n\u001b[32m    292\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m293\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/pipelines/base.py:1274\u001b[39m, in \u001b[36mPipeline.__call__\u001b[39m\u001b[34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[39m\n\u001b[32m   1266\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[32m   1267\u001b[39m         \u001b[38;5;28miter\u001b[39m(\n\u001b[32m   1268\u001b[39m             \u001b[38;5;28mself\u001b[39m.get_iterator(\n\u001b[32m   (...)\u001b[39m\u001b[32m   1271\u001b[39m         )\n\u001b[32m   1272\u001b[39m     )\n\u001b[32m   1273\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1274\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/pipelines/base.py:1281\u001b[39m, in \u001b[36mPipeline.run_single\u001b[39m\u001b[34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[39m\n\u001b[32m   1279\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[32m   1280\u001b[39m     model_inputs = \u001b[38;5;28mself\u001b[39m.preprocess(inputs, **preprocess_params)\n\u001b[32m-> \u001b[39m\u001b[32m1281\u001b[39m     model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1282\u001b[39m     outputs = \u001b[38;5;28mself\u001b[39m.postprocess(model_outputs, **postprocess_params)\n\u001b[32m   1283\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/pipelines/base.py:1173\u001b[39m, in \u001b[36mPipeline.forward\u001b[39m\u001b[34m(self, model_inputs, **forward_params)\u001b[39m\n\u001b[32m   1171\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[32m   1172\u001b[39m         model_inputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_inputs, device=\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m-> \u001b[39m\u001b[32m1173\u001b[39m         model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1174\u001b[39m         model_outputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_outputs, device=torch.device(\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m   1175\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model_outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/pipelines/text_generation.py:397\u001b[39m, in \u001b[36mTextGenerationPipeline._forward\u001b[39m\u001b[34m(self, model_inputs, **generate_kwargs)\u001b[39m\n\u001b[32m    394\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mgeneration_config\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m generate_kwargs:\n\u001b[32m    395\u001b[39m     generate_kwargs[\u001b[33m\"\u001b[39m\u001b[33mgeneration_config\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.generation_config\n\u001b[32m--> \u001b[39m\u001b[32m397\u001b[39m output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ModelOutput):\n\u001b[32m    400\u001b[39m     generated_sequence = output.sequences\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py:2638\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2635\u001b[39m model_kwargs[\u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m] = generation_config.use_cache\n\u001b[32m   2637\u001b[39m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2638\u001b[39m result = \u001b[43mdecoding_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2639\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2640\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2641\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2642\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2643\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2644\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgeneration_mode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2645\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2646\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2648\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py:2843\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   2841\u001b[39m     model_inputs = \u001b[38;5;28mself\u001b[39m.prepare_inputs_for_generation(input_ids, **model_kwargs)\n\u001b[32m   2842\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._optimize_model_for_decode():\n\u001b[32m-> \u001b[39m\u001b[32m2843\u001b[39m         outputs = \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   2844\u001b[39m prefill_consumed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2845\u001b[39m model_kwargs = \u001b[38;5;28mself\u001b[39m._update_model_kwargs_for_generation(\n\u001b[32m   2846\u001b[39m     outputs,\n\u001b[32m   2847\u001b[39m     model_kwargs,\n\u001b[32m   2848\u001b[39m     is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2849\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/accelerate/hooks.py:175\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    173\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py:834\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    833\u001b[39m     return_dict = return_dict_passed\n\u001b[32m--> \u001b[39m\u001b[32m834\u001b[39m output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    836\u001b[39m     output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/models/llama/modeling_llama.py:500\u001b[39m, in \u001b[36mLlamaForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    498\u001b[39m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n\u001b[32m    499\u001b[39m slice_indices = \u001b[38;5;28mslice\u001b[39m(-logits_to_keep, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(logits_to_keep, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m logits_to_keep\n\u001b[32m--> \u001b[39m\u001b[32m500\u001b[39m logits = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlm_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mslice_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    502\u001b[39m loss = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    503\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/accelerate/hooks.py:170\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mnew_forward\u001b[39m(module, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m     args, kwargs = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_hf_hook\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpre_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    171\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m module._hf_hook.no_grad:\n\u001b[32m    172\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/accelerate/hooks.py:360\u001b[39m, in \u001b[36mAlignDevicesHook.pre_forward\u001b[39m\u001b[34m(self, module, *args, **kwargs)\u001b[39m\n\u001b[32m    352\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    353\u001b[39m             value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    354\u001b[39m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tied_params_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    355\u001b[39m             \u001b[38;5;129;01mand\u001b[39;00m value.data_ptr() \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tied_params_map\n\u001b[32m    356\u001b[39m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.execution_device \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tied_params_map[value.data_ptr()]\n\u001b[32m    357\u001b[39m         ):\n\u001b[32m    358\u001b[39m             \u001b[38;5;28mself\u001b[39m.tied_pointers_to_remove.add((value.data_ptr(), \u001b[38;5;28mself\u001b[39m.execution_device))\n\u001b[32m--> \u001b[39m\u001b[32m360\u001b[39m         \u001b[43mset_module_tensor_to_device\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m            \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    363\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexecution_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    364\u001b[39m \u001b[43m            \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfp16_statistics\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfp16_statistics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtied_params_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtied_params_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m send_to_device(args, \u001b[38;5;28mself\u001b[39m.execution_device), send_to_device(\n\u001b[32m    370\u001b[39m     kwargs, \u001b[38;5;28mself\u001b[39m.execution_device, skip_keys=\u001b[38;5;28mself\u001b[39m.skip_keys\n\u001b[32m    371\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/accelerate/utils/modeling.py:343\u001b[39m, in \u001b[36mset_module_tensor_to_device\u001b[39m\u001b[34m(module, tensor_name, device, value, dtype, fp16_statistics, tied_params_map, non_blocking, clear_cache)\u001b[39m\n\u001b[32m    341\u001b[39m             module._parameters[tensor_name] = param_cls(new_value, requires_grad=old_value.requires_grad)\n\u001b[32m    342\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, torch.Tensor):\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m     new_value = \u001b[43mvalue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    345\u001b[39m     new_value = torch.tensor(value, device=device)\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 31.36 GiB of which 996.88 MiB is free. Including non-PyTorch memory, this process has 30.38 GiB memory in use. Of the allocated memory 28.77 GiB is allocated by PyTorch, and 1.03 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# LoRA 파인튜닝 전(Base) vs 파인튜닝 후(LoRA) 모델 답변 비교\n",
    "from transformers import AutoModelForCausalLM, pipeline  # 모델 로드/파이프라인 생성\n",
    "\n",
    "base_model_id = \"NCSOFT/Llama-VARCO-8B-Instruct\"  # 베이스(파인튜닝 전) 모델 ID\n",
    "base_model = AutoModelForCausalLM.from_pretrained(  # 베이스 모델 로드\n",
    "    base_model_id,  # 모델 ID\n",
    "    dtype=torch.bfloat16,  # BF16 로드\n",
    "    device_map=\"auto\",  # CPU/GPU 자동 배치\n",
    ")\n",
    "base_pipe = pipeline(\"text-generation\", model=base_model, tokenizer=tokenizer)  # 베이스 모델 파이프라인 생성\n",
    "\n",
    "for idx, (prompt, label) in enumerate(zip(prompt_list[10:13], label_list[10:13])):  # 샘플 3개만 비교\n",
    "    print(f\"[샘플 {idx + 1}]\")  # 샘플 번호 출력\n",
    "    base_resp = test_inference(base_pipe, prompt)  # 베이스 모델 응답 생성\n",
    "    lora_resp = test_inference(pipe, prompt)  # LoRA 모델 응답 생성\n",
    "    print(f\"  [Base - 파인튜닝 전]\\n{base_resp}\")  # 베이스 모델 응답 출력\n",
    "    print(f\"  [LoRA - 파인튜닝 후]\\n{lora_resp}\")  # LoRA 모델 응답 출력\n",
    "    print(f\"  [Label]\\n{label}\")  # 정답(레이블) 출력\n",
    "    print(\"-\" * 50)  # 구분선 출력"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
