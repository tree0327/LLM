{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XY_a5oKjsahP"
   },
   "source": [
    "# LoRA\n",
    "Low-Rank Adaptation\n",
    "\n",
    "<img src='https://d.pr/i/kWZ843+' width=500/>\n",
    "\n",
    "대형 언어 모델(LLM)이나 Transformer 계열에서 **전체 파라미터를 직접 미세조정(fine-tuning)하지 않고**, 주요 가중치(주로 Linear 레이어)에 **저차원 행렬(저랭크 행렬)**로 변경만을 학습하는 방법이다.\n",
    "\n",
    "\n",
    "**LoRA 공식**\n",
    "\n",
    "1. 기존 Linear 연산: $Y = W X$\n",
    "2. LoRA 적용: $Y = (W + \\Delta W) X$\n",
    "  - $\\Delta W = B A$\n",
    "  - 여기서 $r \\ll d, k$\n",
    "3. 즉, $\\Delta W$는 **저랭크 행렬 곱**으로 근사화한다.\n",
    "\n",
    "\n",
    "\n",
    "**원리:**\n",
    "* 기존의 큰 가중치 행렬 $W$ 대신, $W + BA$ 형태로 쓴다.\n",
    "* 여기서 $B \\in \\mathbb{R}^{d \\times r}$, $A \\in \\mathbb{R}^{r \\times k}$는 **r이 작은 값(저랭크)** $B, A$만 학습, $W$는 동결(freeze)한다.\n",
    "\n",
    "**설명:**\n",
    "* $d$: 원래 가중치 $W\\in\\mathbb R^{d\\times k}$의 **행(row) 크기**, 곧 **입력(input) 차원**이다.\n",
    "* $k$: $W$의 **열(column) 크기**, 곧 **출력(output) 차원**이다.\n",
    "* $r$: 어댑터 $A\\in\\mathbb R^{r\\times k}, B\\in\\mathbb R^{d\\times r}$의 **내적(rank) 차원**으로, $r\\ll\\min(d,k)$인 작은 값이다.\n",
    "\n",
    "* 대문자 ℝ는 “Real numbers”, 곧 **실수 전체의 집합**을 뜻한다.\n",
    "* ℝᵈ는 d차원 실수 벡터 공간\n",
    "* ℝ^{d×k}는 d×k 크기의 실수 행렬들이 모인 공간을 의미한다.\n",
    "\n",
    "* 저차원 행렬 초기화\n",
    "  * A ∈ ℝ^{r×d}의 각 원소는 평균 0, 분산 σ²를 갖는 정규분포에서 샘플링해 초기화한다.\n",
    "  * B ∈ ℝ^{d×r}는 전부 0으로 초기화한다.\n",
    "  * 이렇게 하면 초반에는 B·A = 0 이므로 원래의 W만 동작하고, 학습이 진행되면서 B만 업데이트해 가벼운 파라미터 조정이 가능하다.\n",
    "\n",
    "\n",
    "**장점:**\n",
    "\n",
    "* 파라미터 수와 GPU 메모리 사용량 대폭 감소\n",
    "* 기존 Pretrained Model의 성능을 최대한 보존\n",
    "* 다양한 작업에 손쉽게 맞춤화(Prompt Tuning, Adapter 등과 유사한 \"파인튜닝 경량화\" 전략)\n",
    "\n",
    "\n",
    "\n",
    "* [LoRA 논문 - LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)\n",
    "* 실제 적용은 Hugging Face `peft` 라이브러리에서 훨씬 더 편하게 쓸 수 있다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1769153157169,
     "user": {
      "displayName": "sh qkel",
      "userId": "12967633263148061954"
     },
     "user_tz": -540
    },
    "id": "V4mgzudSsQli",
    "outputId": "4bcf1446-98eb-446a-c2ba-b3ceb3116c4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.7546, -2.1008,  0.5638, -0.9199],\n",
      "        [-1.2150,  1.0843, -1.9324,  2.0813],\n",
      "        [ 1.3128, -1.3836,  3.0943,  1.5675]])\n",
      "tensor([[ 0.9211, -1.9563,  4.1684, -1.1867],\n",
      "        [-5.7657,  3.1328,  5.6262,  0.8978],\n",
      "        [ 1.9991,  9.5809, 12.9361, -2.9406]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 입력데이터\n",
    "X = torch.randn(3, 8)  # (batch_size, in_feature)\n",
    "\n",
    "# 가중치\n",
    "W = torch.randn(4, 8) # (out_feature, in_feature)\n",
    "\n",
    "# r: 저차원 행렬 크기\n",
    "r = 2\n",
    "A = torch.randn(r, 8) # (r, in_feature)\n",
    "B = torch.randn(4, r) # (out_feature, r)\n",
    "\n",
    "# 순방향 연산\n",
    "# 1. 기존 출력\n",
    "Y_normal = X @ W.T # (3, 8) @ (8, 4) -> (3, 4)\n",
    "\n",
    "# 2. LoRA 적용된 출력\n",
    "delta_W = B @ A # (4, 2) @ (2, 8) -> (4, 8)\n",
    "Y_lora = X @ (W + delta_W).T # (3, 8) @ (8, 4) -> (3, 4)\n",
    "\n",
    "print(Y_normal)  # 기존 모델 선형층 출력 텐서\n",
    "print(Y_lora)    # LORA(Delta W) 반영 후 출력 텐서"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1769154455896,
     "user": {
      "displayName": "sh qkel",
      "userId": "12967633263148061954"
     },
     "user_tz": -540
    },
    "id": "U-i8PcMewPM2",
    "outputId": "eeca611a-3d04-49de-a0d3-a8e3da187ff3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1657, -0.2278, -0.2746, -0.2284],\n",
      "        [ 0.2578, -0.0518,  0.4027, -0.0450],\n",
      "        [ 1.6921,  0.3676,  1.4371, -2.2197]], grad_fn=<MmBackward0>)\n",
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]], grad_fn=<DivBackward0>)\n",
      "torch.Size([3, 4])\n",
      "tensor([[-0.1657, -0.2278, -0.2746, -0.2284],\n",
      "        [ 0.2578, -0.0518,  0.4027, -0.0450],\n",
      "        [ 1.6921,  0.3676,  1.4371, -2.2197]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# LoRA 선형층 간단 구현\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LoRALinear(nn.Module):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        r (rank): LoRA에서 추가되는 저랭크 행렬의 랭크(차원)이다.\n",
    "        - \"학습 효율성과 파라미터 효율성의 트레이드오프\" 조절 역할.\n",
    "        - 큰 r → 더 정교하게 원본 모델을 보정할 수 있지만, 파라미터 증가\n",
    "        - 작은 r → 파라미터와 연산량 적지만, 표현력이 제한됨\n",
    "        - r이 크면 더 많은 파라미터(학습가능한 자유도)를 갖고, r이 작으면 적은 파라미터(더 단순함).\n",
    "            - r = 4면, LoRA가 추가하는 변화량이 rank-4 선형 변환에 한정됨(압축).\n",
    "\n",
    "        alpha: LoRA가 추가하는 저랭크 행렬의 출력값을 얼마나 \"키울지\" 결정하는 스케일링 계수.\n",
    "        - alpha를 크게 하면 LoRA 변화량이 더 크게 모델에 반영됨\n",
    "        - alpha를 작게 하면 LoRA 변화량의 영향이 줄어듦\n",
    "        - 일반적으로 LoRA 논문에서는 $\\alpha = r$로 맞추기를 권장하나(=스케일 1배) 실무에서는 일반적으로 4배 사용\n",
    "        - $\\frac{\\alpha}{r}$로 나누어서 LoRA의 영향을 정규화함\n",
    "            - 예시: r=8, alpha=8이면, $\\frac{\\alpha}{r}=1$ (스케일 1배, 영향력 그대로)\n",
    "            - 만약 alpha=16, r=8이면 $\\frac{16}{8}=2$ (2배로 스케일 업)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, r=2, alpha=2):\n",
    "        super().__init__()\n",
    "        self.W = nn.Linear(in_features, out_features, bias=False)\n",
    "        self.r = r\n",
    "        self.alpha = alpha\n",
    "\n",
    "        # LoRA 저차원행렬\n",
    "        self.A = nn.Parameter(torch.zeros((r, in_features)))\n",
    "        self.B = nn.Parameter(torch.zeros((out_features, r)))\n",
    "        # LoRA 저차원행렬 초기화(논문)\n",
    "        nn.init.kaiming_uniform_(self.A, a=math.sqrt(5))  # 평균 0, 표준편차 a의 제곱\n",
    "        nn.init.zeros_(self.B)\n",
    "\n",
    "    def forward(self, x):\n",
    "        Wx = self.W(x) # (3, 8) -> (3, 4)\n",
    "        print(Wx)\n",
    "        deltaW = self.B @ self.A # (4, 2) @ (2, 8) -> (4, 8)\n",
    "        lora = deltaW @ x.T # (4, 8) @ (8, 3) -> (4, 3)\n",
    "        lora = lora.T * self.alpha / self.r # 적용비율 가중치 (3, 4)\n",
    "        print(lora)\n",
    "        return Wx + lora # (3, 4) + (3, 4)\n",
    "\n",
    "torch.manual_seed(42)         # 랜덤시드 고정 (재현성)\n",
    "lora_layer = LoRALinear(8, 4) # 입력 8차원, 출력 4차원 LoRA 선형층\n",
    "x = torch.randn(3, 8)         # 입력 더미 데이터 (batch=3, in_features=8)\n",
    "output = lora_layer(x)        # 순전파 적용(원본 출력 + LoRA 보정)\n",
    "\n",
    "print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1769154354313,
     "user": {
      "displayName": "sh qkel",
      "userId": "12967633263148061954"
     },
     "user_tz": -540
    },
    "id": "xFh2CD7GzuVh",
    "outputId": "7abd0b73-b13c-4eea-b13e-2ebe60e85264"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1657, -0.2278, -0.2746, -0.2284],\n",
      "        [ 0.2578, -0.0518,  0.4027, -0.0450],\n",
      "        [ 1.6921,  0.3676,  1.4371, -2.2197]], grad_fn=<MmBackward0>)\n",
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]], grad_fn=<DivBackward0>)\n",
      "torch.Size([3, 4])\n",
      "tensor([[-0.1657, -0.2278, -0.2746, -0.2284],\n",
      "        [ 0.2578, -0.0518,  0.4027, -0.0450],\n",
      "        [ 1.6921,  0.3676,  1.4371, -2.2197]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "lora_layer = LoRALinear(8, 4, alpha=4)  # alpha = 4 : LoRA 보정 역활 확대\n",
    "x = torch.randn(3, 8)\n",
    "output = lora_layer(x)\n",
    "\n",
    "print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNyYegX3XFoDnUmYuxSLuMn",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "nlp_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
