{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OnqR7cbqbvWk"
   },
   "source": [
    "# GPU VRAM 계산\n",
    "\n",
    "\n",
    "### 1. GPU VRAM 계산 추천 사이트\n",
    "\n",
    "다음 사이트들은 모델의 파라미터 수와 정밀도(Precision)를 기반으로 필요한 메모리를 자동으로 추산해 준다.\n",
    "\n",
    "- **Hugging Face Model Memory Usage (공식 Space)**\n",
    "    - https://huggingface.co/spaces/hf-accelerate/model-memory-usage\n",
    "- **LLM-VRAM-Calculator**\n",
    "    - https://llm-vram-calc.titanium-monkey.com/\n",
    "    - https://vram.asmirnov.xyz/\n",
    "    - 파라미터 수, 양자화(Quantization) 수준, 컨텍스트 길이(Context Window) 등을 상세하게 설정하여 계산할 수 있다. 학습 시 Optimizer(AdamW 등)에 따른 추가 메모리까지 계산해 주어 유용하다.\n",
    "\n",
    "- **간편 계산:** 파라미터 수(B)에 **2를 곱하면 FP16 용량**, **0.5를 곱하면 INT4 용량**이 나온다. (단위: GB)\n",
    "- **안전 마진:** 계산된 용량의 1.2배 이상 되는 GPU를 선택하는 것이 좋다.\n",
    "\n",
    "### 2. GPU VRAM 직접 계산 방법 (공식)\n",
    "\n",
    "sLLM 구동에 필요한 VRAM은 크게 **모델 가중치(Weights)**, **KV 캐시(KV Cache)**, **활성화(Activation)** 메모리로 구성된다. 가장 큰 비중을 차지하는 것은 모델 가중치이다.\n",
    "\n",
    "### 기본 공식 (모델 가중치)\n",
    "\n",
    "모델의 파라미터 수(Parameter Count)와 데이터 정밀도(Precision)를 곱하여 계산한다.\n",
    "\n",
    "$$\\text{VRAM (GB)} \\approx \\text{Parameters (Billion)} \\times \\text{Bytes per Parameter}$$\n",
    "\n",
    "**정밀도(Precision)별 바이트 수:**\n",
    "\n",
    "| 데이터 타입 | 비트 수 | 파라미터당 바이트 | 설명 |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **FP32** (Full Precision) | 32-bit | 4 Bytes | 일반적인 학습 시 사용 |\n",
    "| **FP16 / BF16** | 16-bit | 2 Bytes | 대부분의 추론 및 LoRA 학습 |\n",
    "| **INT8** | 8-bit | 1 Byte | 기본적인 양자화 |\n",
    "| **INT4** (GPTQ/AWQ) | 4-bit | 0.5 Bytes | 로컬 구동 시 가장 많이 사용 |\n",
    "\n",
    "### 실전 계산 예시: 8B (80억) 모델\n",
    "\n",
    "**1) FP16 (16-bit) 원본 모델 구동 시:**\n",
    "\n",
    "$$8 (\\text{Billion}) \\times 2 (\\text{Bytes}) = 16 \\text{GB}$$\n",
    "\n",
    "> 결과: 모델 로드에만 약 16GB가 필요하며, 컨텍스트 처리를 위해 24GB VRAM GPU(RTX 3090/4090 등)가 권장된다.\n",
    ">\n",
    "\n",
    "**2) INT4 (4-bit) 양자화 모델 구동 시:**\n",
    "\n",
    "$$8 (\\text{Billion}) \\times 0.5 (\\text{Bytes}) = 4 \\text{GB}$$\n",
    "\n",
    "> 결과: 모델 로드에 4GB가 필요하다. 여기에 KV 캐시 여유분을 더하면 6GB~8GB VRAM GPU에서도 구동이 가능하다.\n",
    ">\n",
    "\n",
    "\n",
    "### 3. 추가 고려 사항 (Overhead)\n",
    "\n",
    "단순 모델 크기 외에 실제 구동 시에는 다음과 같은 추가 메모리가 필요하다. 따라서 계산된 값에 약 **20%의 여유 공간**을 두는 것이 안전하다.\n",
    "\n",
    "1. **KV Cache (컨텍스트 길이):** 입력 프롬프트와 생성할 토큰의 길이가 길어질수록 메모리 사용량이 선형적으로 증가한다. 긴 문맥을 처리하려면 더 많은 VRAM이 필요하다.\n",
    "2. **Activation Overhead:** 연산 중간에 발생하는 임시 데이터들을 저장할 공간이 필요하다.\n",
    "3. **라이브러리 오버헤드:** PyTorch, CUDA 커널 등이 사용하는 기본 메모리가 있다 (약 0.5GB ~ 1GB).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21682,
     "status": "ok",
     "timestamp": 1770874675140,
     "user": {
      "displayName": "김종민",
      "userId": "00897423557836052337"
     },
     "user_tz": -540
    },
    "id": "uPIMbhg4bl9A",
    "outputId": "78bcf6c6-85ae-4dbb-b5c0-983082aab806"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/10.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/10.3 MB\u001b[0m \u001b[31m94.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m152.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m99.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/515.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m515.2/515.2 kB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m540.5/540.5 kB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m133.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.6/47.6 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "%pip install -Uq transformers datasets accelerate trl peft bitsandbytes hf_transfer wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 305,
     "status": "ok",
     "timestamp": 1770874675455,
     "user": {
      "displayName": "김종민",
      "userId": "00897423557836052337"
     },
     "user_tz": -540
    },
    "id": "0Otwtm7idu91",
    "outputId": "ff56672c-3ba3-4171-c2ca-c6cba55b9363"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Feb 12 05:37:55 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 580.82.07              Driver Version: 580.82.07      CUDA Version: 13.0     |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   63C    P8             11W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1770874747399,
     "user": {
      "displayName": "김종민",
      "userId": "00897423557836052337"
     },
     "user_tz": -540
    },
    "id": "BZEQSJ86W7Tt",
    "outputId": "b4876237-9155-463c-a405-c3223aa8ccb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# gpu 메모리 조회\n",
    "import torch\n",
    "\n",
    "print(torch.cuda.memory_allocated())  # 현재 사용중인 메모리\n",
    "print(torch.cuda.memory_reserved())   # 사용중 + 재사용을 위한 cache메모리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4438,
     "status": "ok",
     "timestamp": 1769484102384,
     "user": {
      "displayName": "sh qkel",
      "userId": "12967633263148061954"
     },
     "user_tz": -540
    },
    "id": "6abqD7vTdx1W",
    "outputId": "3931c16f-cdde-48e1-9139-6c6a44ca8c40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "할당 vram:  0.00GB\n",
      "캐싱 vram:  0.00GB\n"
     ]
    }
   ],
   "source": [
    "# gpu 메모리 조회\n",
    "import torch\n",
    "\n",
    "print(f'할당 vram: {torch.cuda.memory_allocated() / 1024 ** 3: .2f}GB')  # 현재 텐서가 실제로 사용중인 메모리(GB)\n",
    "print(f'캐싱 vram: {torch.cuda.memory_reserved() / 1024 ** 3: .2f}GB')   # Pytorch가 재사용을 위해 확보해둔 메모리(GB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1770875046271,
     "user": {
      "displayName": "김종민",
      "userId": "00897423557836052337"
     },
     "user_tz": -540
    },
    "id": "_OojuBNleO77",
    "outputId": "0c85509a-0138-4a59-deb8-f1ca782d38ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU VRAM 사용량:  2.38GB\n"
     ]
    }
   ],
   "source": [
    "# 현재 GPU VRAM 사용량을 출력하는 함수\n",
    "def print_gpu_utilization():\n",
    "    if torch.cuda.is_available():  # GPU 가용시\n",
    "        print(f'GPU VRAM 사용량: {torch.cuda.memory_allocated() / 1024 ** 3: .2f}GB')\n",
    "    else:\n",
    "        print('Runtime 유형을 GPU로 변경하세요.')\n",
    "\n",
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68,
     "referenced_widgets": [
      "53fb7e8bf388457fa62807ed340e7448",
      "6c858d16295844e292327a7da9146199",
      "1dc1da43bbdb40428950bf44201127c0",
      "3d2a056969164e4bbcccfe19621f0a1a",
      "7709b6154166413fb84aefbc8d789586",
      "693754bb9b564882ab24cfb3d40ee69d",
      "4e411e988053414ba90aeb5fe28797f2",
      "ac12270d011c4ffc869ee222e44cfa32",
      "cf3bf25b998f4768b0a14f75f8ee59a6",
      "594b658ff7d44786a4a877edc1d70987",
      "caf72f557fb4410e8c3e5853e10a142c"
     ]
    },
    "executionInfo": {
     "elapsed": 3291,
     "status": "ok",
     "timestamp": 1770875050009,
     "user": {
      "displayName": "김종민",
      "userId": "00897423557836052337"
     },
     "user_tz": -540
    },
    "id": "sw5Wp-FZenmd",
    "outputId": "a7d5ef9b-1a3d-4516-a19a-e82586eda59a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53fb7e8bf388457fa62807ed340e7448",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/332 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU VRAM 사용량:  4.77GB\n"
     ]
    }
   ],
   "source": [
    "# 모델 로드 및 gpu 확인\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "def load_model_and_tokenizer(model_id, peft=None):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, dtype='auto', device_map='auto')\n",
    "    print_gpu_utilization()\n",
    "    return model, tokenizer\n",
    "\n",
    "model_id = 'LGAI-EXAONE/EXAONE-4.0-1.2B'\n",
    "model, tokenizer = load_model_and_tokenizer(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7g__pcjpX5sA"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1dc1da43bbdb40428950bf44201127c0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ac12270d011c4ffc869ee222e44cfa32",
      "max": 332,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cf3bf25b998f4768b0a14f75f8ee59a6",
      "value": 332
     }
    },
    "3d2a056969164e4bbcccfe19621f0a1a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_594b658ff7d44786a4a877edc1d70987",
      "placeholder": "​",
      "style": "IPY_MODEL_caf72f557fb4410e8c3e5853e10a142c",
      "value": " 332/332 [00:01&lt;00:00, 266.52it/s, Materializing param=model.norm.weight]"
     }
    },
    "4e411e988053414ba90aeb5fe28797f2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "53fb7e8bf388457fa62807ed340e7448": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6c858d16295844e292327a7da9146199",
       "IPY_MODEL_1dc1da43bbdb40428950bf44201127c0",
       "IPY_MODEL_3d2a056969164e4bbcccfe19621f0a1a"
      ],
      "layout": "IPY_MODEL_7709b6154166413fb84aefbc8d789586"
     }
    },
    "594b658ff7d44786a4a877edc1d70987": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "693754bb9b564882ab24cfb3d40ee69d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6c858d16295844e292327a7da9146199": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_693754bb9b564882ab24cfb3d40ee69d",
      "placeholder": "​",
      "style": "IPY_MODEL_4e411e988053414ba90aeb5fe28797f2",
      "value": "Loading weights: 100%"
     }
    },
    "7709b6154166413fb84aefbc8d789586": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ac12270d011c4ffc869ee222e44cfa32": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "caf72f557fb4410e8c3e5853e10a142c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cf3bf25b998f4768b0a14f75f8ee59a6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
